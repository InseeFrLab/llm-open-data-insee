{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 12/07/2024 - Update Retrieval Evaluation process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal:\n",
    "The previous tests were conducted using a dataset comprising the 100 largest available documents from the raw database. As a result, this high-quality content may not accurately reflect the distribution of data in the entire vector database. While we can observe differences between the configurations, it is challenging to determine which combination is the best choice for our use case among the top configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several experiments will be done : \n",
    "- add random documents to the base corpus (uniformly distributed)\n",
    "- add other big documents to the base corpus (keeping the same extraction procedure of the top N largest content documents)\n",
    "\n",
    "what to observe : \n",
    "- evolution of the retrieval metrics facing this added noise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumption : run python src/db_building/insee_data_processing.py in the terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List \n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt \n",
    "from typing import List, Dict \n",
    "import math \n",
    "from tqdm import tqdm \n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "from db_building import extract_paragraphs\n",
    "from doc_building import compute_autokonenizer_chunk_size, build_documents_from_dataframe\n",
    "from evaluation import RetrievalConfiguration, hist_results, plot_results\n",
    "from config import MARKDOWN_SEPARATORS, DB_DIR_LOCAL, EMB_DEVICE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/home/onyxia/work/llm-open-data-insee/data_complete.csv\", low_memory=False) #we assume the textual information have already been extracted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = extract_paragraphs(data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pd.DataFrame.from_dict(results)\n",
    "ds.to_csv(\"insee_documents.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pd.read_csv(\"insee_documents.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#question and answer\n",
    "path_qa = \"../data/q_and_a_scored_filtered_Phi-3-mini-128k-instruct.csv\"\n",
    "test = pd.read_csv(path_qa)\n",
    "\n",
    "df_dict = {}\n",
    "df_dict[\"the_df_dataset\"] = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain_docs = [LangchainDocument(\n",
    "            page_content= doc[\"paragraphs\"],\n",
    "             metadata={\n",
    "                \"source\": doc[\"url_source\"], \n",
    "                \"title\": doc[\"title\"],\n",
    "                \"insee_id\": doc[\"id_origin\"], \n",
    "                \"categories\" : doc[\"categories\"],\n",
    "                \"date_diffusion\" : doc[\"dateDiffusion\"], \n",
    "                \"themes\" : doc[\"themes\"],\n",
    "                \"collections\" : doc[\"collections\"], \n",
    "                \"libelleAffichageGeo\" : doc[\"libelleAffichageGeo\"], \n",
    "                \"intertitres\" : doc[\"intertitres\"],\n",
    "                \"authors\" : doc[\"authors\"],\n",
    "                \"subtitle\" : doc[\"subtitle\"]\n",
    "             }) for _, doc in tqdm(ds.iterrows())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_documents_with_largest_content(documents : List[LangchainDocument], n=1000):\n",
    "    # Create a list of tuples (content_size, document)\n",
    "    document_sizes = [(len(doc.page_content.split()), doc) for doc in documents]\n",
    "\n",
    "    # Sort the list by content size in descending order\n",
    "    sorted_documents = sorted(document_sizes, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    # Get the top N documents\n",
    "    top_n_documents = [doc for (_ , doc) in sorted_documents[:n]]\n",
    "\n",
    "    return top_n_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_langchain_docs = get_top_n_documents_with_largest_content(langchain_docs, n=1000)\n",
    "\n",
    "counters_para = Counter()\n",
    "\n",
    "for doc in sample_langchain_docs:\n",
    "    counters_para[len(doc.page_content.split())] +=1\n",
    "\n",
    "lengths_sorted = sorted(counters_para.items())\n",
    "lengths , counts = zip(*lengths_sorted)\n",
    "# Create bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(lengths, counts, color='skyblue')\n",
    "plt.xlabel('Paragraph Length (number of words)')\n",
    "plt.ylabel('Number of Paragraphs')\n",
    "plt.title( f'Paragraph Length Distribution ({len(sample_langchain_docs)} docs)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "def from_docs_to_vector_database(langchain_docs: List[LangchainDocument], mode: str = \"random\", top_n = 100, config: RetrievalConfiguration = None): \n",
    "\n",
    "    if mode==\"random\":\n",
    "        np.random.seed(42) \n",
    "        top_documents = get_top_n_documents_with_largest_content(langchain_docs, n=100) \n",
    "        indices = np.random.randint(low=100, high=len(langchain_docs), size=top_n-100, dtype=int)\n",
    "        sample_langchain_docs = top_documents + [langchain_docs[i] for i in indices]\n",
    "    elif mode==\"top\":\n",
    "        sample_langchain_docs = get_top_n_documents_with_largest_content(langchain_docs, n=top_n) \n",
    "\n",
    "    autokenizer, chunk_size, chunk_overlap = compute_autokonenizer_chunk_size(config.get(\"embedding_model_name\"))\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "        autokenizer,\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=MARKDOWN_SEPARATORS,\n",
    "    )\n",
    "    docs_processed = text_splitter.split_documents(sample_langchain_docs)\n",
    "\n",
    "    # Remove duplicates\n",
    "    unique_texts = {}\n",
    "    docs_processed_unique = []\n",
    "    for doc in docs_processed:\n",
    "        if doc.page_content not in unique_texts:\n",
    "            unique_texts[doc.page_content] = True\n",
    "            docs_processed_unique.append(doc)\n",
    "\n",
    "    embedding_model = HuggingFaceEmbeddings(  # load from sentence transformers\n",
    "        model_name=config.get(\"embedding_model_name\"),\n",
    "        model_kwargs={\"device\": EMB_DEVICE, \"trust_remote_code\": True},\n",
    "        encode_kwargs={\"normalize_embeddings\": True},  # set True for cosine similarity\n",
    "        show_progress=False,\n",
    "    )\n",
    "    # Process documents in batches\n",
    "    chroma_client = chromadb.PersistentClient\n",
    "    max_batch_size = chroma_client._producer.max_batch_size\n",
    "\n",
    "    for i in range(0, len(docs_processed_unique), max_batch_size):\n",
    "        batch_docs = docs_processed_unique[i:i + max_batch_size]\n",
    "        db = Chroma.from_documents(\n",
    "            collection_name=config.get(\"collection\"),\n",
    "            documents=batch_docs,\n",
    "            persist_directory=DB_DIR_LOCAL,\n",
    "            embedding=embedding_model,\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_nb_docs = [10000, 15000, 20000, 30000, 35000, 40000]\n",
    "list_config = [ \n",
    "    RetrievalConfiguration(\n",
    "        name=f'test_docs_{nb_docs}',\n",
    "        database=\"chromadb\",\n",
    "        collection=f\"Solon-embeddings-large-0.1_docs_{nb_docs}\",\n",
    "        database_path=None,\n",
    "        embedding_model_name=\"OrdalieTech/Solon-embeddings-large-0.1\",\n",
    "        reranker_type=None,\n",
    "        reranker_name=None,\n",
    "        rerank_k=50,\n",
    "        k_values=[1, 2, 3, 5, 10, 15, 20, 25, 30, 35, 45, 50]\n",
    "        )\n",
    "        for nb_docs in list_nb_docs\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (config, nb_doc) in zip(list_config, list_nb_docs):\n",
    "    from_docs_to_vector_database(langchain_docs=langchain_docs, mode=\"random\", top_n=nb_doc, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import RetrievalEvaluator\n",
    "\n",
    "results = RetrievalEvaluator.run(\n",
    "    eval_configurations=list_config,\n",
    "    eval_dict=df_dict,  # Ensure 'df_dict' is a dictionary containing pandas DataFrames with the required structure\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment : Random Documents Injection (seed 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_results(\n",
    "    list_config, \n",
    "    results[\"the_df_dataset\"], \n",
    "    ir_metrics=['recall', 'mrr', 'ndcg','runtime'], \n",
    "    focus=\"name\", \n",
    "    title = \"Evaluating Retriever Performance on Multiple Dataset Sizes Using Solon Embedding Model and Random Document Selection (Seed 42)\",\n",
    "    k = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(\n",
    "    list_config, \n",
    "    results[\"the_df_dataset\"], \n",
    "    ir_metrics=['recall', 'mrr', 'ndcg', 'precision'], \n",
    "    focus=\"name\", \n",
    "    title = \"Retriever performances on multiple size dataset\",\n",
    "    k = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment : On Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_encoders = [\"BAAI/bge-reranker-v2-m3\", \"antoinelouis/crossencoder-camembert-large-mmarcoFR\", \"BAAI/bge-reranker-base\"]\n",
    "colberts = [\"bclavie/FraColBERTv2\", \"antoinelouis/colbertv2-camembert-L4-mmarcoFR\"]\n",
    "\n",
    "colbert_vs_cross_encoder_eval_config = [\n",
    "    RetrievalConfiguration(\n",
    "        name=f'cross_encoder_{i}',\n",
    "        database=\"chromadb\",\n",
    "        collection=\"insee_data\",\n",
    "        database_path=None,\n",
    "        embedding_model_name=\"OrdalieTech/Solon-embeddings-large-0.1\",\n",
    "        reranker_type=\"Cross-encoder\",\n",
    "        reranker_name=cross_encoder,\n",
    "        k_values=[1, 2, 3, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50],\n",
    "        rerank_k= 50\n",
    "    ) for i, cross_encoder in enumerate(cross_encoders)] + [\n",
    "    RetrievalConfiguration(\n",
    "        name=f'colbert_{i}',\n",
    "        database=\"chromadb\",\n",
    "        collection=\"insee_data\",\n",
    "        database_path=None,\n",
    "        embedding_model_name=\"OrdalieTech/Solon-embeddings-large-0.1\",\n",
    "        reranker_type=\"ColBERT\",\n",
    "        reranker_name=colbert,\n",
    "        k_values=[1, 2, 3, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50],\n",
    "        rerank_k= 50\n",
    "    ) for i, colbert in enumerate(colberts)] + [\n",
    "    RetrievalConfiguration(\n",
    "        name=f'baseline',\n",
    "        database=\"chromadb\",\n",
    "        collection=\"insee_data\",\n",
    "        database_path=None,\n",
    "        embedding_model_name=\"OrdalieTech/Solon-embeddings-large-0.1\",\n",
    "        reranker_type=None,\n",
    "        reranker_name=None,\n",
    "        k_values=[1, 2, 3, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50],\n",
    "        rerank_k= 50\n",
    "        )\n",
    "    ]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the evaluator\n",
    "results_colbert_vs_cross_encoder = RetrievalEvaluator.run(\n",
    "    eval_configurations = colbert_vs_cross_encoder_eval_config,\n",
    "    eval_dict=df_dict,  # Ensure 'df_dict' is a dictionary containing pandas DataFrames with the required structure\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the evaluator\n",
    "plot_results(\n",
    "    colbert_vs_cross_encoder_eval_config, \n",
    "    results_colbert_vs_cross_encoder[\"the_df_dataset\"], \n",
    "    ir_metrics=['recall', 'mrr', 'ndcg', 'precision'], \n",
    "    focus= \"reranker_name\", \n",
    "    title = \"Performance of Rerankers on Full INSEE Dataset (~40,000 Docs) using Solon-Embeddings-Large-0.1 model embedding model\",\n",
    "    k = 50,\n",
    "    cmap_name = \"tab10\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the evaluator\n",
    "hist_results(\n",
    "    colbert_vs_cross_encoder_eval_config, \n",
    "    results_colbert_vs_cross_encoder[\"the_df_dataset\"], \n",
    "    ir_metrics=['recall', 'mrr', 'ndcg',\"runtime\"], \n",
    "    focus = \"reranker_name\", \n",
    "    title = \"Performance of Rerankers on Full INSEE Dataset (~40,000 Docs) using Solon-Embeddings-Large-0.1 model embedding model\",\n",
    "    k = 5,\n",
    "    cmap_name = \"tab10\",\n",
    "    x_min=0.6\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def compute_performance(results):\n",
    "    config_names = [config_name for config_name in results.keys()]\n",
    "    \n",
    "    if \"baseline\" in config_names:\n",
    "        results_percentages = copy.deepcopy(results)  # Use deepcopy to avoid modifying the original\n",
    "        result_baseline = results[\"baseline\"]\n",
    "        \n",
    "        for config_name, config_res in results.items():\n",
    "            if config_name == \"baseline\":\n",
    "                continue  # Skip baseline itself\n",
    "            for metric in config_res.keys():\n",
    "                if isinstance(config_res[metric], dict):\n",
    "                    for i in config_res[metric]:\n",
    "                        base_res = result_baseline[metric][i]\n",
    "                        res = config_res[metric][i]\n",
    "                        res_over_baseline = ((res - base_res) / base_res) * 100\n",
    "                        results_percentages[config_name][metric][i] = res_over_baseline\n",
    "                elif isinstance(config_res[metric], float):\n",
    "                    base_res = result_baseline[metric]\n",
    "                    res = config_res[metric]\n",
    "                    res_over_baseline = ((res - base_res) / base_res) * 100\n",
    "                    results_percentages[config_name][metric] = res_over_baseline\n",
    "        \n",
    "        return results_percentages\n",
    "    else: \n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_percentages_over_baseline = compute_performance(results=results_colbert_vs_cross_encoder[\"the_df_dataset\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(\n",
    "    colbert_vs_cross_encoder_eval_config, \n",
    "    results_percentages_over_baseline, \n",
    "    ir_metrics=['recall', 'mrr', 'ndcg', 'precision'], \n",
    "    focus= \"reranker_name\", \n",
    "    title = \"Performance of Rerankers on Full INSEE Dataset (~40,000 Docs) using Solon-Embeddings-Large-0.1 model embedding model \\n(% over the baseline)\",\n",
    "    k = 50,\n",
    "    cmap_name = \"tab10\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment : Longest Documents Injection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_nb_docs = [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_config = [ \n",
    "    RetrievalConfiguration(\n",
    "        name=f'test_docs_{nb_docs}',\n",
    "        database=\"chromadb\",\n",
    "        collection=f\"Solon-embeddings-large-0.1_docs_{nb_docs}\",\n",
    "        database_path=None,\n",
    "        embedding_model_name=\"OrdalieTech/Solon-embeddings-large-0.1\",\n",
    "        reranker_type=None,\n",
    "        reranker_name=None,\n",
    "        rerank_k=50,\n",
    "        k_values=[1, 2, 3, 5, 10, 15, 20, 25, 30, 35, 45, 50]\n",
    "        )\n",
    "        for nb_docs in list_nb_docs\n",
    "]\n",
    "\n",
    "for (config, nb_doc) in zip(list_config, list_nb_docs):\n",
    "    from_docs_to_vector_database(langchain_docs=langchain_docs, mode=\"top\", top_n=nb_doc, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import RetrievalEvaluator\n",
    "\n",
    "results = RetrievalEvaluator.run(\n",
    "    eval_configurations=list_config,\n",
    "    eval_dict=df_dict,  # Ensure 'df_dict' is a dictionary containing pandas DataFrames with the required structure\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_results(\n",
    "    list_config, \n",
    "    results[\"the_df_dataset\"], \n",
    "    ir_metrics=['recall', 'mrr', 'ndcg','runtime'], \n",
    "    focus=\"name\", \n",
    "    title = \"Retriever performances on multiple size dataset (longest documents in Insee Data)\",\n",
    "    k = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(\n",
    "    list_config, \n",
    "    results[\"the_df_dataset\"], \n",
    "    ir_metrics=['recall', 'mrr', 'ndcg','precision'], \n",
    "    focus=\"name\", \n",
    "    title = \"Retriever performances on multiple size dataset \\n (longest documents in Insee Data)\",\n",
    "    k = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
