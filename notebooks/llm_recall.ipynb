{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Recall - Needle-in-a-haystack tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Needle-in-a-Haystack test is designed to evaluate the performance of LLM RAG systems across different sizes of context windows. The testing process is straightforward: it asks an LLM to answer a specific question while the correct information (\"the needle\") is embedded within a large, unrelated context (the haystack).\n",
    "\n",
    "- This test helps to better understand the retrieval abilities of models with very long context windows (some exceeding 200k tokens). Uniform retrieval performance is not guaranteed, as demonstrated in experiments showing that LLM in-context recall is prompt-dependent (see [LLM In-Context Recall is Prompt Dependent](https://arxiv.org/pdf/2404.08865v1))\n",
    "\n",
    "- To our knowledge, this experiment has never been conducted on French statistical data.\n",
    "\n",
    "We will also implement an improvement using multiple facts (needles) to better align with RAG expectations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will focus on opensource models (no API) and INSEE-related data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mc cp -q s3/projet-llm-insee-open-data/data/eval_data/eval_retrieval/insee_documents_sample_ref_retrieval_evaluation.csv ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"./insee_documents_sample_ref_retrieval_evaluation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from dateutil import parser\n",
    "\n",
    "# Convert date strings to datetime objects\n",
    "dates = [parser.isoparse(date_string) for date_string in data[\"date_diffusion\"]]\n",
    "\n",
    "# Sort the dates to ensure they are in chronological order\n",
    "dates.sort()\n",
    "\n",
    "# Plot the distribution of date differences\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(dates, bins=20, edgecolor=\"black\")\n",
    "plt.title(\"Distribution of Date Differences (Seconds)\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTRUCTION_TEMPLATE = \"\"\"\n",
    "Réponds à la question donnée en basant ta réponse uniquement sur les informations fournises en context. \n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Réponse:\n",
    "\"\"\"\n",
    "\n",
    "INSTRUCTION_EVAL = \"\"\"\n",
    "Tu es un assistant IA spécialisé en Statistique Publique.\n",
    "Veuillez agir en tant que juge impartial et évaluer la qualité de la réponse fournie par un assistant IA à la question de l'utilisateur affichée ci-dessous. \n",
    "Pour cette évaluation, vous devez principalement considérer le critère suivant:\n",
    "\n",
    "Précision:\n",
    "    Note 1 : La réponse n’a aucun rapport avec la référence.\n",
    "    Note 2 : La réponse a une pertinence mineure mais ne correspond pas à la référence.\n",
    "    Note 3 : La réponse est moyennement pertinente mais contient des inexactitudes.\n",
    "    Note 4 : La réponse correspond à la référence mais comporte des omissions mineures.\n",
    "    Note 5 : La réponse est tout à fait exacte et correspond parfaitement à la référence.\n",
    "\n",
    "\n",
    "Commencez votre évaluation en fournissant une brève explication. Soyez le plus objectif possible. \n",
    "Après avoir fourni votre explication, vous devez évaluer la réponse sur une échelle de 1 à 5.\n",
    "Vous répondrez en FRANCAIS en respectant OBLIGATOIREMENT le format de réponse suivant : {{\"explanation\" : explication, \"score\": score}}\n",
    "\n",
    "Par exemple vous pourrez renvoyer:\n",
    "\n",
    "Votre réponse : \n",
    "{{\"explanation\" : \"La réponse donnnée ne correspond absolument pas à la réponse de référence\", \"score\": 1}}\n",
    "\n",
    "--- \n",
    "\n",
    "Voic la réponse de référence : \n",
    "{ground_truth_answer}\n",
    "\n",
    "Voici la question posée : \n",
    "{question}\n",
    "\n",
    "Voici la réponse générée :\n",
    "{generated_answer}\n",
    "\n",
    "Votre réponse : \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Needle in Haystack class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task we can ask : \n",
    "- ask a figure : Quel est le taux de chômage de la population française au premier trimestre 2024 ? \n",
    "    Answer : 7.5\n",
    "- Give a definition : Quel est la définition du coût salariale ? \n",
    "    Answer : Le coût salarial est constitué par l'ensemble des dépenses qui incombent à l'employeur pour l'emploi d'un salarié. Il se répartit en :\n",
    "    un coût direct, composé principalement des salaires bruts et différents avantages salariaux ;\n",
    "    un coût indirect formé essentiellement des cotisations patronales légales et conventionnelles et de diverses charges.\n",
    "    Dans le coût direct, les avantages salariaux correspondent notamment aux avantages en nature, à l'intéressement et à la participation.\n",
    "    Dans le coût indirect les charges autres que les cotisations patronales correspondent notamment à la formation professionnelle, aux frais de transport et aux œuvres sociales.\n",
    "- ask to retrieve multiple figures ? \n",
    "    Donne moi le taux d'inflation, le taux de croissance et le taux de chômage au premier trimestre 2024.\n",
    "    Answer : \n",
    "        - taux d'inflation : +2.2%\n",
    "        - taux de croissance : 0.2%\n",
    "        - taux de chômage : 7.5%\n",
    "\n",
    "- A combien estime t on le nombre de logement supplémentaire par an entre 2024 et 2029 ? \n",
    "    Answer : \n",
    "    L’évaluation des besoins en logements dans la région des Pays de la Loire témoigne d’une dynamique soutenue, reflétant les enjeux démographiques et économiques de la région. Les besoins sont estimés à 23 700 logements par an entre 2024 et 2029.\n",
    "- Qu'est ce que l'inflation ? \n",
    "    answer : L'inflation est la perte du pouvoir d'achat de la monnaie qui se traduit par une augmentation générale et durable des prix. \n",
    "- Comment est calculé l'inflation ? \n",
    "    answer : L’indice des prix à la consommation (IPC) est utilisé pour évaluer l’inflation.\n",
    "- Comment s'appelle le jeu de données sur les échanges extérieurs de la France ? \n",
    "    Answer : DD_CNA_ECH_EXT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from matplotlib.colors import LinearSegmentedColormap, Normalize\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class Evaluator:\n",
    "    \"\"\"\n",
    "    Class to set up and run LLM-as-a-judge Needle-in-HayStack evaluation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer=None,\n",
    "        model=None,\n",
    "        question=\"\",\n",
    "        gt_answer=\"\",\n",
    "        prompt_eval=INSTRUCTION_EVAL,\n",
    "        kwargs={\"max_new_tokens\": 250, \"return_full_text\": False, \"do_sample\": False},\n",
    "        verbose=False,\n",
    "    ):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.question_asked = question\n",
    "        self.gt_answer = gt_answer\n",
    "        self.pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "        self.prompt_template_eval = prompt_eval\n",
    "        self.kwargs = kwargs\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def build_final_prompt(self, ground_truth_answer, generated_answer):\n",
    "        \"\"\"\n",
    "        Build evaluation prompt\n",
    "        \"\"\"\n",
    "        complete_prompt = self.prompt_template_eval.format(\n",
    "            question=self.question_asked,\n",
    "            ground_truth_answer=ground_truth_answer,\n",
    "            generated_answer=generated_answer,\n",
    "        )\n",
    "        instructions = [\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"Tu es un assistant spécialisé en statistique publique\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": complete_prompt},\n",
    "        ]\n",
    "        return self.tokenizer.apply_chat_template(instructions, tokenize=False)\n",
    "\n",
    "    def evaluation_parser(self, evaluation: str):\n",
    "        \"\"\"\n",
    "        parsing generated evaluation to extract score and explanation\n",
    "        \"\"\"\n",
    "\n",
    "        # Define a regular expression pattern to match a dictionary string\n",
    "        pattern = r\"\\{.*?\\}\"\n",
    "\n",
    "        # Find all matches in the string\n",
    "        matches = re.findall(pattern, evaluation)\n",
    "\n",
    "        dictionaries = []\n",
    "        for match in matches:\n",
    "            try:\n",
    "                # Safely evaluate the string to a dictionary\n",
    "                dictionaries.append(ast.literal_eval(match))\n",
    "            except (SyntaxError, ValueError):\n",
    "                if self.verbose:\n",
    "                    logger.error(\"Error at parsing Evaluation, return default value\")\n",
    "                    logger.info(\"evaluation reveived : \", evaluation)\n",
    "                return {\n",
    "                    \"explanation\": None,\n",
    "                    \"score\": None,\n",
    "                }  # If the match is not a valid dictionary, skip it\n",
    "\n",
    "        if len(dictionaries) > 0:\n",
    "            d = dictionaries[0]\n",
    "            return {\"explanation\": d[\"explanation\"], \"score\": int(d[\"score\"])}\n",
    "        else:\n",
    "            return {\"explanation\": None, \"score\": None}\n",
    "\n",
    "    def evaluate(self, generated_answers: List[str]) -> List[str]:\n",
    "        \"\"\"\n",
    "        run in batch evaluation\n",
    "        \"\"\"\n",
    "        final_prompt = [\n",
    "            self.build_final_prompt(ground_truth_answer=self.gt_answer, generated_answer=ans)\n",
    "            for ans in generated_answers\n",
    "        ]\n",
    "        evaluations = self.pipe(final_prompt, **self.kwargs)\n",
    "\n",
    "        return [self.evaluation_parser(eval_[0][\"generated_text\"]) for eval_ in evaluations]\n",
    "\n",
    "\n",
    "class LLMNeedleHaystackTester:\n",
    "    \"\"\"\n",
    "    Class to set up Needle in the haystack test ,\n",
    "    implementation inspired by https://github.com/Arize-ai/LLMTest_NeedleInAHaystack2 and https://github.com/gkamradt/LLMTest_NeedleInAHaystack\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        needle=\"\",\n",
    "        retrieval_question=\"\",\n",
    "        results_version=1,\n",
    "        context_lengths_min=50,\n",
    "        context_lengths_max=1000,\n",
    "        context_lengths_num_intervals=10,\n",
    "        context_lengths=None,\n",
    "        document_depth_percent_min=0,\n",
    "        document_depth_percent_max=100,\n",
    "        document_depth_percent_intervals=35,\n",
    "        document_depth_percents=None,\n",
    "        llm_name=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "        evaluation=False,\n",
    "        prompt_template=\"\",\n",
    "        corpus_file=\"./insee_documents_sample_ref_retrieval_evaluation.csv\",\n",
    "        save_results=False,\n",
    "        final_context_length_buffer=200,\n",
    "        kwargs={\n",
    "            \"max_new_tokens\": 50,\n",
    "            \"temperature\": 0.2,\n",
    "            \"return_full_text\": False,\n",
    "            \"do_sample\": True,\n",
    "        },\n",
    "        verbose=False,\n",
    "    ):\n",
    "        self.needle = needle\n",
    "        if not needle or not retrieval_question:\n",
    "            raise ValueError(\"Needle and retrieval_question must be provided.\")\n",
    "\n",
    "        self.context_lengths_num_intervals = context_lengths_num_intervals\n",
    "        self.document_depth_percent_intervals = document_depth_percent_intervals\n",
    "        self.retrieval_question = retrieval_question\n",
    "        self.results_version = results_version\n",
    "        self.save_results = save_results\n",
    "        self.corpus_file = corpus_file\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # load llm and its tokenizer for generation\n",
    "        if llm_name is not None:\n",
    "            self.llm_name = llm_name\n",
    "            self.model, self.tokenizer = self.load_llm_model(\n",
    "                llm_name, to_load=[\"tokenizer\", \"model\"], quantization=True\n",
    "            )\n",
    "            self.pipe = pipeline(task=\"text-generation\", model=self.model, tokenizer=self.tokenizer)\n",
    "        else:\n",
    "            raise \"Error No LLM model loaded\"\n",
    "\n",
    "        self.kwargs = kwargs\n",
    "        self.prompt_template = prompt_template\n",
    "        self.final_context_length_buffer = self.estimate_final_context_length_buffer()\n",
    "        self.evaluator = None\n",
    "\n",
    "        if evaluation:\n",
    "            model_name_eval = \"microsoft/Phi-3-mini-128k-instruct\"\n",
    "            eval_model, eval_tokenizer = self.load_llm_model(\n",
    "                model_name_eval, to_load=[\"tokenizer\", \"model\"], quantization=True\n",
    "            )\n",
    "            self.evaluator = Evaluator(\n",
    "                tokenizer=eval_tokenizer,\n",
    "                model=eval_model,\n",
    "                question=self.retrieval_question,\n",
    "                gt_answer=self.needle,\n",
    "                prompt_eval=INSTRUCTION_EVAL,\n",
    "                verbose=self.verbose,\n",
    "            )\n",
    "\n",
    "        if context_lengths is None:\n",
    "            if (\n",
    "                context_lengths_min is None\n",
    "                or context_lengths_max is None\n",
    "                or context_lengths_num_intervals is None\n",
    "            ):\n",
    "                raise ValueError(\n",
    "                    \"Either context_lengths_min, context_lengths_max, context_lengths_intervals need to be filled out OR the context_lengths_list needs to be supplied.\"\n",
    "                )\n",
    "            else:\n",
    "                context_lengths = np.round(\n",
    "                    np.linspace(\n",
    "                        context_lengths_min,\n",
    "                        context_lengths_max,\n",
    "                        num=context_lengths_num_intervals,\n",
    "                        endpoint=True,\n",
    "                    )\n",
    "                ).astype(int)\n",
    "                self.context_lengths = [int(x) for x in context_lengths]\n",
    "        else:\n",
    "            self.context_lengths = [int(x) for x in context_lengths]\n",
    "\n",
    "        if document_depth_percents is None:\n",
    "            if (\n",
    "                document_depth_percent_min is None\n",
    "                or document_depth_percent_max is None\n",
    "                or document_depth_percent_intervals is None\n",
    "            ):\n",
    "                raise ValueError(\n",
    "                    \"Either document_depth_percent_min, document_depth_percent_max, document_depth_percent_intervals need to be filled out OR the document_depth_percents needs to be supplied.\"\n",
    "                )\n",
    "            else:\n",
    "                document_depth_percents = np.round(\n",
    "                    np.linspace(\n",
    "                        document_depth_percent_min,\n",
    "                        document_depth_percent_max,\n",
    "                        num=document_depth_percent_intervals,\n",
    "                        endpoint=True,\n",
    "                    )\n",
    "                ).astype(int)\n",
    "                self.document_depth_percents = [int(x) for x in document_depth_percents]\n",
    "        else:\n",
    "            self.document_depth_percents = [int(x) for x in document_depth_percents]\n",
    "\n",
    "    def load_llm_model(\n",
    "        self, model_name, to_load=[\"tokenizer\", \"model\"], quantization=True\n",
    "    ) -> Tuple:\n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            pretrained_model_name_or_path=model_name, use_fast=True, device_map=\"auto\"\n",
    "        )\n",
    "\n",
    "        # Check if tokenizer has a pad_token; if not, set it to eos_token\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "        if \"model\" not in to_load:\n",
    "            return _, tokenizer\n",
    "\n",
    "        # Load LLM config\n",
    "        config = AutoConfig.from_pretrained(\n",
    "            pretrained_model_name_or_path=model_name, trust_remote_code=True\n",
    "        )\n",
    "\n",
    "        if quantization:\n",
    "            # Load quantization config\n",
    "            quantization_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_compute_dtype=\"float16\",\n",
    "                bnb_4bit_use_double_quant=False,\n",
    "            )\n",
    "\n",
    "        # Load LLM\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            pretrained_model_name_or_path=model_name,\n",
    "            config=config,\n",
    "            quantization_config=quantization_config if quantization else None,\n",
    "            device_map=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "            torch_dtype=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        return model, tokenizer\n",
    "\n",
    "    def load_corpus(self, nb_sample=2) -> str:\n",
    "        if os.path.exists(self.corpus_file):\n",
    "            ext = self.corpus_file.split(\".\")[-1]\n",
    "            if ext == \"json\":\n",
    "                df = pd.read_json(self.corpus_file)\n",
    "            elif ext == \"csv\":\n",
    "                df = pd.read_csv(self.corpus_file)\n",
    "            else:\n",
    "                raise ValueError(\"File extension is not recognized\")\n",
    "\n",
    "            return \" \".join(df.sample(nb_sample)[\"content\"].astype(str))\n",
    "        else:\n",
    "            raise FileNotFoundError(\"The provided file path does not exist\")\n",
    "\n",
    "    def build_final_prompt(self, question, context):\n",
    "        complete_prompt = self.prompt_template.format(context=context, question=question)\n",
    "        instructions = [{\"role\": \"user\", \"content\": complete_prompt}]\n",
    "        return self.tokenizer.apply_chat_template(instructions, tokenize=False)\n",
    "\n",
    "    def estimate_final_context_length_buffer(self) -> int:\n",
    "        length_token_question = len(self.encode_text_to_tokens(self.retrieval_question))\n",
    "        length_token_prompt = len(self.encode_text_to_tokens(self.prompt_template))\n",
    "        length_token_generation = self.kwargs.get(\"max_new_tokens\", 50)\n",
    "        return length_token_question + length_token_prompt + length_token_generation + 10\n",
    "\n",
    "    def result_exists(self, context_length: int, depth_percent: int) -> bool:\n",
    "        results_dir = \"results/\" + str(self.results_version) + \"/\"\n",
    "        if not os.path.exists(results_dir):\n",
    "            return False\n",
    "\n",
    "        for filename in os.listdir(results_dir):\n",
    "            if filename.endswith(\".json\"):\n",
    "                file_path = os.path.join(results_dir, filename)\n",
    "                try:\n",
    "                    with open(file_path) as f:\n",
    "                        try:\n",
    "                            result = json.load(f)\n",
    "                        except json.JSONDecodeError as e:\n",
    "                            print(f\"Error decoding JSON from file {file_path}: {e}\")\n",
    "                            continue\n",
    "\n",
    "                        context_length_met = result[\"context_length\"] == context_length\n",
    "                        depth_percent_met = result[\"depth_percent\"] == depth_percent\n",
    "                        version_met = result.get(\"version\", 1) == self.results_version\n",
    "                        model_met = result[\"model\"] == self.llm_name\n",
    "\n",
    "                        if context_length_met and depth_percent_met and version_met and model_met:\n",
    "                            return True\n",
    "\n",
    "                except OSError as e:\n",
    "                    logger.error(f\"Error reading file {file_path}: {e}\")\n",
    "                    continue\n",
    "        return False\n",
    "\n",
    "    def encode_text_to_tokens(self, text: str) -> List[int]:\n",
    "        return self.tokenizer.encode(text)\n",
    "\n",
    "    def decode_tokens(self, tokens: List[int], context_length: int = None) -> str:\n",
    "        return self.tokenizer.decode(tokens[:context_length])\n",
    "\n",
    "    def encode_and_trim(self, context: str, context_length: int) -> str:\n",
    "        tokens = self.encode_text_to_tokens(context)\n",
    "        if len(tokens) > context_length:\n",
    "            context = self.decode_tokens(tokens, context_length)\n",
    "        return context\n",
    "\n",
    "    def insert_needle(\n",
    "        self, needle: str, context: str, depth_percent: int, context_length: int\n",
    "    ) -> str:\n",
    "        tokens_needle = self.encode_text_to_tokens(needle)\n",
    "        tokens_context = self.encode_text_to_tokens(context)\n",
    "        context_length -= self.final_context_length_buffer\n",
    "\n",
    "        if len(tokens_context) + len(tokens_needle) > context_length:\n",
    "            tokens_context = tokens_context[: context_length - len(tokens_needle)]\n",
    "\n",
    "        tokens_new_context = [42]  # initialize\n",
    "\n",
    "        if depth_percent == 100:\n",
    "            tokens_new_context = tokens_context + tokens_needle  # add element at the end\n",
    "        else:\n",
    "            insertion_point = int(len(tokens_context) * (depth_percent / 100))\n",
    "            period_tokens = self.encode_text_to_tokens(\".\")  # encode point character\n",
    "            while tokens_new_context and (tokens_new_context[-1] not in period_tokens):\n",
    "                insertion_point -= 1\n",
    "                tokens_new_context = tokens_context[:insertion_point]\n",
    "\n",
    "            tokens_new_context += tokens_needle + tokens_context[insertion_point:]\n",
    "\n",
    "        new_context = self.decode_tokens(tokens_new_context)\n",
    "        return new_context\n",
    "\n",
    "    def generate_context(\n",
    "        self, needle: str, trim_context: str, context_length: int, depth_percent: int\n",
    "    ) -> str:\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            needle (str): _description_\n",
    "            trim_context (str): _description_\n",
    "            context_length (int): _description_\n",
    "            depth_percent (int): _description_\n",
    "\n",
    "        Returns:\n",
    "            str: _description_\n",
    "        \"\"\"\n",
    "        return self.insert_needle(\n",
    "            needle=needle,\n",
    "            context=trim_context,\n",
    "            depth_percent=depth_percent,\n",
    "            context_length=context_length,\n",
    "        )\n",
    "\n",
    "    def aggregate(self, results) -> Dict:\n",
    "        scores = [int(r[\"score\"]) for r in results]\n",
    "        return {\n",
    "            \"scores\": scores,\n",
    "            \"median\": np.median(scores),\n",
    "            \"mean\": np.mean(scores),\n",
    "            \"std_dev\": np.std(scores),\n",
    "            \"count\": len(scores),\n",
    "        }\n",
    "\n",
    "    def save(\n",
    "        self, result, context_length, depth_percent, gt_answer, corpus_sample, explanation=None\n",
    "    ) -> None:\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            result (_type_): _description_\n",
    "            context_length (_type_): _description_\n",
    "            depth_percent (_type_): _description_\n",
    "            gt_answer (_type_): _description_\n",
    "            corpus_sample (_type_): _description_\n",
    "            explanation (_type_, optional): _description_. Defaults to None.\n",
    "        \"\"\"\n",
    "        # timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "        results_dir = \"results/\" + str(self.results_version) + \"/\"\n",
    "        if not os.path.exists(results_dir):\n",
    "            os.makedirs(results_dir)\n",
    "\n",
    "        results_dict = {\n",
    "            \"model\": self.llm_name,\n",
    "            \"needle\": gt_answer,\n",
    "            \"context_length\": context_length,\n",
    "            \"depth_percent\": depth_percent,\n",
    "            \"result\": result,\n",
    "            \"prompt_template\": self.prompt_template,\n",
    "            \"corpus\": str(corpus_sample),\n",
    "            \"version\": self.results_version,\n",
    "        }\n",
    "        if explanation:\n",
    "            results_dict[\"explanations\"] = explanation\n",
    "\n",
    "        # build file name\n",
    "        filename = f'results_{self.llm_name.split(\"/\")[-1]}_{context_length}_{depth_percent}.json'\n",
    "        # turn a dictionary into a json object\n",
    "\n",
    "        with open(os.path.join(results_dir, filename), \"w\") as f:\n",
    "            json.dump(results_dict, f)\n",
    "\n",
    "    def run(self, nb_corpus_sample=2):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            nb_corpus_sample (int, optional): _description_. Defaults to 2.\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        corpus_sample = self.load_corpus(nb_corpus_sample)\n",
    "\n",
    "        print(\n",
    "            f\"{self.llm_name} tokenizes the corpus in {len(self.encode_text_to_tokens(corpus_sample))} tokens\"\n",
    "        )\n",
    "\n",
    "        all_results = {}\n",
    "\n",
    "        list_context_depth_tuples = []\n",
    "        list_final_prompts = []\n",
    "        generated_contexts = []\n",
    "        for context_length in self.context_lengths:\n",
    "            for depth_percent in self.document_depth_percents:\n",
    "                if not self.result_exists(context_length, depth_percent):\n",
    "                    # add only the non-already tested tuples\n",
    "                    list_context_depth_tuples.append((context_length, depth_percent))\n",
    "                    trimmed_context = self.encode_and_trim(corpus_sample, context_length)\n",
    "                    generated_context = self.generate_context(\n",
    "                        self.needle, trimmed_context, context_length, depth_percent\n",
    "                    )\n",
    "                    generated_contexts.append(generated_context)\n",
    "                    final_prompt = self.build_final_prompt(\n",
    "                        self.retrieval_question, generated_context\n",
    "                    )\n",
    "                    list_final_prompts.append(final_prompt)\n",
    "\n",
    "        if len(list_context_depth_tuples) != 0:\n",
    "            print(\n",
    "                f\"\\nThe following {len(list_context_depth_tuples)} context_lenght & depth percentages will be tested : \",\n",
    "                list_context_depth_tuples,\n",
    "            )\n",
    "            # run pipeline\n",
    "            answers = self.pipe(list_final_prompts, **self.kwargs)\n",
    "            generated_answers = [str(ans[0][\"generated_text\"]) for ans in answers]\n",
    "\n",
    "            # print(\"generated answer : \\n-\", \"\\n-\".join(generated_answers))\n",
    "\n",
    "            if self.evaluator:\n",
    "                evaluation_results = self.evaluator.evaluate(generated_answers)\n",
    "                # print(\"evaluation : \" , evaluation_results)\n",
    "                # extract explanations\n",
    "                for i, (context_length, depth_percent) in enumerate(list_context_depth_tuples):\n",
    "                    eval_res = evaluation_results[i]\n",
    "                    score = int(eval_res[\"score\"]) if eval_res[\"score\"] is not None else 0\n",
    "                    exp = (\n",
    "                        str(eval_res[\"explanation\"])\n",
    "                        if eval_res[\"explanation\"] is not None\n",
    "                        else \"No explanation available\"\n",
    "                    )\n",
    "                    all_results[(context_length, depth_percent)] = {\n",
    "                        \"score\": score,\n",
    "                        \"explanation\": exp,\n",
    "                    }  # keep the score\n",
    "\n",
    "                    if self.save_results:\n",
    "                        self.save(\n",
    "                            result=score,\n",
    "                            context_length=context_length,\n",
    "                            depth_percent=depth_percent,\n",
    "                            gt_answer=self.needle,\n",
    "                            corpus_sample=generated_contexts[\n",
    "                                i\n",
    "                            ],  # add the context where the needle have been added\n",
    "                            explanation=exp,\n",
    "                        )\n",
    "            else:\n",
    "                for i, (context_length, depth_percent) in enumerate(list_context_depth_tuples):\n",
    "                    all_results[(context_length, depth_percent)] = generated_answers[i]\n",
    "\n",
    "                    if self.save_results:\n",
    "                        self.save(\n",
    "                            result=generated_answers[i],\n",
    "                            context_length=context_length,\n",
    "                            depth_percent=depth_percent,\n",
    "                            gt_answer=self.needle,\n",
    "                            corpus_sample=generated_contexts[\n",
    "                                i\n",
    "                            ],  # add the context where the needle have been added\n",
    "                        )\n",
    "        else:\n",
    "            print(\"Skipping this configuration as result already exists\")\n",
    "\n",
    "        return all_results\n",
    "\n",
    "    def load_results(\n",
    "        self, results_dir=\"./results\", model_name=\"mistralai/Mistral-7B-Instruct-v0.2\", version=1\n",
    "    ):\n",
    "        \"\"\"\n",
    "        load the results from a previous experiment based on\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        results_dir += \"/\" + str(version)\n",
    "\n",
    "        if not os.path.exists(results_dir):\n",
    "            return None\n",
    "        else:\n",
    "            list_docs = os.listdir(results_dir)\n",
    "            print(f\"{len(list_docs)} documents have been found\")\n",
    "\n",
    "            for filename in tqdm(list_docs):\n",
    "                if filename.endswith(\".json\"):\n",
    "                    file_path = os.path.join(results_dir, filename)\n",
    "                    try:\n",
    "                        with open(file_path) as f:\n",
    "                            try:\n",
    "                                r = json.load(f)\n",
    "                            except json.JSONDecodeError as e:\n",
    "                                print(f\"Error decoding JSON from file {file_path}: {e}\")\n",
    "                                continue\n",
    "\n",
    "                            if (\n",
    "                                \"result\" in r\n",
    "                                and \"explanations\" in r\n",
    "                                and \"model\" in r\n",
    "                                and \"version\" in r\n",
    "                            ):\n",
    "                                if r[\"model\"] == model_name and r[\"version\"] == version:\n",
    "                                    results[(r[\"context_length\"], r[\"depth_percent\"])] = {\n",
    "                                        \"score\": r[\"result\"],\n",
    "                                        \"explanation\": r[\"explanations\"],\n",
    "                                    }\n",
    "\n",
    "                    except OSError as e:\n",
    "                        logger.error(f\"Error reading file {file_path}: {e}\")\n",
    "                        continue\n",
    "\n",
    "        return results\n",
    "\n",
    "    def plot(self, results: Dict, title=None, filename=None):\n",
    "        if title is None:\n",
    "            title = str(self.llm_name) + \" Recall Performance\"\n",
    "\n",
    "        if filename is None:\n",
    "            filename = str(self.llm_name).split(\"/\")[-1] + \"_recall_performances.png\"\n",
    "\n",
    "        depth_percents = sorted(set(k[1] for k in results))\n",
    "        context_lengths = sorted(set(k[0] for k in results))\n",
    "        scores = np.full((len(depth_percents), len(context_lengths)), np.nan)\n",
    "\n",
    "        for (context_length, depth_percent), dict_res in results.items():\n",
    "            i = depth_percents.index(depth_percent)\n",
    "            j = context_lengths.index(context_length)\n",
    "            scores[i, j] = int(dict_res[\"score\"])\n",
    "\n",
    "        # change nan value by 0\n",
    "        scores[np.isnan(scores)] = 0\n",
    "        recall_score = (\n",
    "            np.round(np.sum(scores) / (5 * len(depth_percents) * len(context_lengths)), 3) * 100\n",
    "        )\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 10))\n",
    "        cmap = LinearSegmentedColormap.from_list(\"custom_cmap\", [\"#F0496E\", \"#EBB839\", \"#0CD79F\"])\n",
    "        cax = ax.matshow(scores, cmap=cmap, norm=Normalize(vmin=0, vmax=5))\n",
    "\n",
    "        plt.colorbar(cax, fraction=0.046, pad=0.04)\n",
    "        plt.yticks(range(len(depth_percents)), depth_percents)\n",
    "        plt.xticks(range(len(context_lengths)), context_lengths, rotation=45)\n",
    "        plt.ylabel(\"Fact Placement Depth (%)\")\n",
    "        plt.xlabel(\"Haystack Size\")\n",
    "\n",
    "        plt.title(\n",
    "            f\"{title} (Recall Score : {str(recall_score)}%)\\n {self.retrieval_question}\",\n",
    "            fontsize=12,\n",
    "        )\n",
    "\n",
    "        plt.savefig(filename)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def generate_image(self, results: Dict, color_palette=\"viridis\", title=None):\n",
    "    # If title is None, set a default title\n",
    "    if title is None:\n",
    "        title = str(self.llm_name) + \" Recall Performance\"\n",
    "\n",
    "    # Extract and sort unique context lengths and depth percents\n",
    "    depth_percents = sorted(set(k[1] for k in results))\n",
    "    context_lengths = sorted(set(k[0] for k in results))\n",
    "\n",
    "    # Create a DataFrame to hold the results\n",
    "    data = pd.DataFrame(index=depth_percents, columns=context_lengths)\n",
    "\n",
    "    # Populate the DataFrame with results\n",
    "    for (context_length, depth_percent), res in results.items():\n",
    "        data.loc[depth_percent, context_length] = int(res[\"score\"])\n",
    "\n",
    "    # remove the nan values to get a complete heatmap\n",
    "    data.fillna(0, inplace=True)\n",
    "\n",
    "    recall_score = (\n",
    "        np.round(data.sum().sum() / (5 * len(depth_percents) * len(context_lengths)), 3) * 100\n",
    "    )\n",
    "\n",
    "    # Create the figure and axis for the heatmap\n",
    "    fig, ax = plt.subplots(figsize=(15, 10))\n",
    "\n",
    "    cmap = LinearSegmentedColormap.from_list(\"custom_cmap\", [\"#F0496E\", \"#EBB839\", \"#0CD79F\"])\n",
    "\n",
    "    # Generate the heatmap\n",
    "    sns.heatmap(\n",
    "        data,\n",
    "        # annot=True,\n",
    "        fmt=\"g\",  # \".2f\",\n",
    "        xticklabels=\"auto\",\n",
    "        yticklabels=\"auto\",\n",
    "        cmap=cmap,\n",
    "        cbar=True,\n",
    "        cbar_kws={\"label\": \"score\"},\n",
    "        vmin=0,\n",
    "        vmax=5,\n",
    "        linewidths=0.5,\n",
    "        linecolor=\"black\",\n",
    "        ax=ax,\n",
    "    )\n",
    "\n",
    "    # Set axis labels and title\n",
    "    ax.set_xlabel(\"Haystack Size\")\n",
    "    ax.set_ylabel(\"Fact Placement Depth (%)\")\n",
    "    ax.set_title(\n",
    "        f\"{title} (Recall Score : {recall_score}%)\\n {self.retrieval_question}\", fontsize=12\n",
    "    )\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # Save the figure to the specified file path\n",
    "    file_name = str(self.llm_name).split(\"/\")[-1] + \"_recall_performances.png\"\n",
    "\n",
    "    plt.savefig(file_name)\n",
    "\n",
    "    # Display the plot (optional)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# some experiments have already been evaluated for Mistral-7B-Instruct-v0.1 and Mistral-7B-Instruct-v0.2, meta-llama/Meta-Llama-3-8B-Instruct\n",
    "# Questions already asked :\n",
    "# question about unemployement\n",
    "# question about inflation defintion?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_name = \"mistralai/Mistral-7B-Instruct-v0.2\"  # context length : 32000\n",
    "# llm_name = \"meta-llama/Meta-Llama-3-8B-Instruct\" # context length : 8000\n",
    "\n",
    "question = \"Qu'est ce que l'inflation ?\"\n",
    "needle = \"L'inflation est la perte du pouvoir d'achat de la monnaie qui se traduit par une augmentation générale et durable des prix.\"\n",
    "\n",
    "llm_tester = LLMNeedleHaystackTester(\n",
    "    needle=needle,\n",
    "    retrieval_question=question,\n",
    "    results_version=3,\n",
    "    context_lengths_min=100,\n",
    "    context_lengths_max=32000,\n",
    "    context_lengths_num_intervals=10,\n",
    "    context_lengths=None,\n",
    "    document_depth_percent_min=0,\n",
    "    document_depth_percent_max=100,\n",
    "    document_depth_percent_intervals=10,\n",
    "    document_depth_percents=None,\n",
    "    llm_name=llm_name,\n",
    "    evaluation=True,\n",
    "    prompt_template=INSTRUCTION_TEMPLATE,\n",
    "    corpus_file=\"./insee_documents_sample_ref_retrieval_evaluation.csv\",\n",
    "    save_results=True,\n",
    "    final_context_length_buffer=200,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the experiment\n",
    "results = llm_tester.run(nb_corpus_sample=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = llm_tester.load_results(\n",
    "    lresults_dir=\"./results\", model_name=\"mistralai/Mistral-7B-Instruct-v0.2\", version=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_tester.plot(results=res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = llm_tester.load_results(\n",
    "    results_dir=\"./results\", model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\", version=2\n",
    ")\n",
    "llm_tester.plot(results=res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = llm_tester.load_results(\n",
    "    results_dir=\"./results\", model_name=\"mistralai/Mistral-7B-Instruct-v0.2\", version=2\n",
    ")\n",
    "llm_tester.plot(results=res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
