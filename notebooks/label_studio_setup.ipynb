{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Utilisation Label Studio pour création d'un jeu de données d'évaluation de retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "On souhaite utiliser Label Studio sur le SSP Cloud pour créer un jeu d'évaluation de la manière suivante:\n",
    "\n",
    "- On commence par sélectionner un échantillon de pages sur le site Insee;\n",
    "- On veut une tâche d'annotation par page;\n",
    "- Pour une tâche d'annotation, on affiche tout le texte de la page qui a été embeddé et stocké dans la vector database de RAG, ainsi qu'un lien vers la page du site Insee;\n",
    "- Une tâche d'annotation consiste à poser une ou plusieurs questions sur le contenu qui figure dans la page;\n",
    "- Cette tâche est plus facile à réaliser en lisant la page sur le site Insee, pour des raisons de mise en page. Ainsi, il faut avant de valider les questions valider que le contenu  correspondant figure bien dans le texte affiché dans Label Studio (ce texte est en général une sous-inclusion du texte figurant véritablement sur la page web)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "On utilise l'interface entre Label Studio et s3 - il faut sauvegarder les tâches d'annotation sous forme de JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import s3fs\n",
    "\n",
    "fs = s3fs.S3FileSystem(client_kwargs={\"endpoint_url\": \"https://\" + \"minio.lab.sspcloud.fr\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "with fs.open(\n",
    "    \"projet-llm-insee-open-data/data/eval_data/eval_retrieval/insee_documents_sample_ref_retrieval_evaluation.csv\",\n",
    "    \"r\",\n",
    ") as f:\n",
    "    df = pd.read_csv(f)\n",
    "df[\"text\"] = df[\"content\"]\n",
    "\n",
    "# Save each annotation task as a json file\n",
    "for i, row in enumerate(df[[\"text\", \"source\", \"title\"]].iterrows()):\n",
    "    json_data = {\"text\": row[1][\"text\"], \"source\": row[1][\"source\"], \"title\": row[1][\"title\"]}\n",
    "    with fs.open(\n",
    "        f\"projet-llm-insee-open-data/data/eval_data/eval_retrieval/test_tasks/{i}.json\",\n",
    "        \"w\",\n",
    "        encoding=\"utf8\",\n",
    "    ) as fp:\n",
    "        json.dump(json_data, fp, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "Un template d'annotation possible est le suivant, à renseigner sur Label Studio:\n",
    "\n",
    "```html\n",
    "<View>\n",
    "  <Header value=\"Titre du texte\"/>\n",
    "  <Text name=\"title\" value=\"$title\"/>\n",
    "  <Header value=\"Page source\"/>\n",
    "  <HyperText name=\"p1\" clickableLinks=\"true\" inline=\"true\" target=\"_blank\">\n",
    "    <a target=\"_blank\" href=\"$source\">$source</a>\n",
    "  </HyperText>\n",
    "  \n",
    "  <Header value=\"Posez une ou plusieurs questions sur le texte, en les séparant avec un pipe |. Vérifiez que le contenu correspondant à la question figure bien dans le texte suivant, qui est une extraction en général incomplète de la page.\"/>\n",
    "  <TextArea name=\"answer\" toName=\"text\" showSubmitButton=\"true\" maxSubmissions=\"1\" editable=\"true\" required=\"true\"/>\n",
    "\n",
    "  <Header value=\"Extraction\"/>\n",
    "  <Text name=\"text\" value=\"$text\"/>\n",
    "</View>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "L'interface avec s3 se configure dans Settings > Cloud Storage. Le code suivant permet de récupérer les annotations créées avec ce projet (https://projet-llm-insee-open-data-label-studio.user.lab.sspcloud.fr/projects/2/) et de les formatter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in fs.listdir(\n",
    "    \"projet-llm-insee-open-data/data/eval_data/eval_retrieval/test_annotations/\"\n",
    "):\n",
    "    with fs.open(path[\"Key\"], \"rb\") as f:\n",
    "        annotation_data = json.load(f)\n",
    "        source = annotation_data[\"task\"][\"data\"][\"source\"]\n",
    "        questions = annotation_data[\"result\"][0][\"value\"][\"text\"][0]\n",
    "        questions = questions.split(\"|\")\n",
    "        questions = [q.strip() for q in questions]\n",
    "        evaluation_data = evaluation_data + [{\"source\": source, \"questions\": questions}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
