{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate a RAG pipeline, we need : \n",
    "- a RAG pipeline (Retriever - LLM )\n",
    "- a Vectorial Databse (with associated embedding model)\n",
    "- a test dataset with Q&A. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load vectorial dataset \n",
    "!mc cp s3/projet-llm-insee-open-data/data/chroma_database/chroma_db  ./src/data --recursive\n",
    "#load test dataset \n",
    "!mc cp s3/projet-llm-insee-open-data/data/eval_data/eval_dataset.csv ./src/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel, RunnableLambda\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "from langchain.vectorstores import Chroma\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TextStreamer\n",
    "import torch\n",
    "\n",
    "#vector database\n",
    "DB_DIR = 'src/data/chroma_db'\n",
    "#embedding model \n",
    "EMB_DEVICE = \"cuda\"\n",
    "\n",
    "EMB_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "#LLM\n",
    "MODEL_DEVICE = {\"\": 0}\n",
    "#MODEL_NAME = \"tiiuae/falcon-7b\"  #use flash attention (faster Attention computation) and Quantization (smaller model memory usage)\n",
    "\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\" \n",
    "#MODEL_NAME = \"EleutherAI/gpt-neo-1.3B\"\n",
    "#MODEL_NAME = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "def build_llm_model():\n",
    "    \"\"\"\n",
    "    Create the llm model\n",
    "    \"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    #load LLM config \n",
    "    config = AutoConfig.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "    config.max_position_embeddings = 8096\n",
    "    #load quantization config \n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=\"float16\",\n",
    "            bnb_4bit_use_double_quant=False,\n",
    "        )\n",
    "    #load llm tokenizer \n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True, device_map='auto') \n",
    "\n",
    "    # Check if tokenizer has a pad_token; if not, set it to eos_token\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    #load llm \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            config=config,\n",
    "            quantization_config=quantization_config\n",
    "        )\n",
    "    #Create a pipeline with  tokenizer and model  \n",
    "    pipeline_HF = pipeline(task=\"text-generation\", # TextGenerationPipeline HF pipeline\n",
    "                model=model, \n",
    "                tokenizer=tokenizer,\n",
    "                max_new_tokens=2000,\n",
    "                temperature=0.2, \n",
    "                return_full_text=False, \n",
    "                device_map=\"auto\",\n",
    "                do_sample=True,\n",
    "            )\n",
    "    # Create a LangChain Runnable pipeline \n",
    "\n",
    "    langchain_llm = HuggingFacePipeline(pipeline=pipeline_HF)\n",
    "\n",
    "    return langchain_llm\n",
    "\n",
    "def format_docs(docs) -> str:\n",
    "    \"\"\"\n",
    "    Format the retrieved document before giving their content to complete the prompt \n",
    "    \"\"\"\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs) \n",
    "\n",
    "\n",
    "def build_chain(hf_embeddings, vectorstore, retriever, prompt, llm):\n",
    "    \"\"\" \n",
    "    Build a LLM chain based on Langchain package and INSEE data \n",
    "    \"\"\"\n",
    "    #Create a Langchain LLM Chain \n",
    "    rag_chain_from_docs = (\n",
    "        RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
    "        | prompt\n",
    "        | llm \n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    return rag_chain_from_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "<s>[INST]\n",
    "En utilisant les informations contenues dans le contexte,\n",
    "fournissez une réponse complète à la question.\n",
    "Répondez uniquement à la question posée, la réponse doit être concise et pertinente par rapport à la question.\n",
    "Fournissez le numéro du document source lorsque cela est pertinent.\n",
    "Si la réponse ne peut pas être déduite du contexte, ne donnez pas de réponse.</s>\n",
    "\n",
    "Contexte :\n",
    "{contexte}\n",
    "---\n",
    "Maintenant, voici la question à laquelle vous devez répondre.\n",
    "\n",
    "Question : {question}\n",
    "[/INST]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create prompt for chat template \n",
    "RAG_PROMPT_TEMPLATE = \"\"\"\n",
    "<s>[INST] \n",
    "Tu es un assistant spécialisé dans la statistique publique répondant aux questions d'agent de l'INSEE. \n",
    "Réponds en Français seulement.\n",
    "Utilise les informations obtenues dans le contexte, réponds de manière argumentée à la question posée.\n",
    "La réponse doit être développée et citer ses sources.\n",
    "\n",
    "Si tu ne peux pas induire ta réponse du contexte, ne réponds pas. \n",
    "Voici le contexte sur lequel tu dois baser ta réponse : \n",
    "Contexte: {context}\n",
    "        ---\n",
    "Voici la question à laquelle tu dois répondre : \n",
    "Question: {question}\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "#load Embedding model \n",
    "hf_embeddings = HuggingFaceEmbeddings(model_name=EMB_MODEL_NAME, model_kwargs={\"device\": EMB_DEVICE})\n",
    "#load vector database\n",
    "vectorstore = Chroma(collection_name=\"insee_data\", embedding_function=hf_embeddings, persist_directory=str(DB_DIR))\n",
    "#set up a retriever \n",
    "retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs = {\"k\":10})\n",
    "#generate prompt template\n",
    "prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=RAG_PROMPT_TEMPLATE)\n",
    "\n",
    "#create a pipeline with tokenizer and LLM\n",
    "llm = build_llm_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "langchain ChromaDB class support batch querying => ask multiple questions and recieved multiple documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragatouille import RAGPretrainedModel\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from langchain_core.documents import Document\n",
    "from typing import Optional, Tuple, List\n",
    "\n",
    "def answer_with_rag(\n",
    "    questions: list[str],\n",
    "    llm_model,\n",
    "    knowledge_index: VectorStore,\n",
    "    reranker: Optional[RAGPretrainedModel] = None,\n",
    "    num_retrieved_docs: int = 30,\n",
    "    num_docs_final: int = 7,\n",
    ") -> Tuple[str, List[Document]]:\n",
    "\n",
    "    \"\"\"Answer a batch of questions using RAG with the given knowledge index.\n",
    "    return a batch of answers and relevant documents. \n",
    "    \"\"\"\n",
    "    batch_final_prompt = []\n",
    "    batch_relevant_documents = []\n",
    "    for q in questions:\n",
    "        # Gather documents with retriever\n",
    "        relevant_docs = knowledge_index.similarity_search(query=q, k=num_retrieved_docs)\n",
    "        relevant_docs = [doc.page_content for doc in relevant_docs]  # keep only the text\n",
    "\n",
    "        # Optionally rerank results\n",
    "        if reranker:\n",
    "            relevant_docs = reranker.rerank(q, relevant_docs, k=num_docs_final)\n",
    "            relevant_docs = [doc[\"content\"] for doc in relevant_docs]\n",
    "\n",
    "        relevant_docs = relevant_docs[:num_docs_final]\n",
    "\n",
    "        # Build the final prompt\n",
    "        context = \"\\nExtracted documents:\\n\"\n",
    "        context += \"\".join([f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)])\n",
    "\n",
    "        final_prompt = RAG_PROMPT_TEMPLATE.format(question=q, context=context)\n",
    "\n",
    "        batch_final_prompt.append(final_prompt)\n",
    "        batch_relevant_documents.append(relevant_docs)\n",
    "\n",
    "    # Redact an answer\n",
    "    batch_answer = llm_model.batch(batch_final_prompt)\n",
    "    batch_answer = [out.replace(\"\\nA: \", \"\") for out in batch_answer] #clean up \n",
    "    return batch_answer, batch_relevant_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generating Answers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import time \n",
    "import numpy as np\n",
    "\n",
    "def run_rag_tests(\n",
    "    eval_dataset: datasets.Dataset,\n",
    "    llm,\n",
    "    knowledge_index: VectorStore,\n",
    "    output_file: str,\n",
    "    reranker: Optional[RAGPretrainedModel] = None,\n",
    "    verbose: Optional[bool] = True,\n",
    "    test_settings: Optional[str] = None,  # To document the test settings used\n",
    "    batch_size = 2\n",
    "):\n",
    "    \"\"\"Runs RAG tests on the given dataset and saves the results to the given output file.\"\"\"\n",
    "    try:  # load previous generations if they exist\n",
    "        with open(output_file, \"r\") as f:\n",
    "            outputs = json.load(f)\n",
    "    except:\n",
    "        outputs = []\n",
    "\n",
    "    num_rows = len(eval_dataset[\"train\"])\n",
    "\n",
    "    for batch_start in tqdm(range(0, num_rows, batch_size)):\n",
    "\n",
    "        # Adjust batch end to handle the last incomplete batch\n",
    "        batch_end = min(batch_start + batch_size, num_rows)\n",
    "        batch_examples = eval_dataset[\"train\"][batch_start:batch_end]\n",
    "        #eval_dataset is a HF dataset then a slice return a sub dataset (a dict {'context' : [...,...]}) \n",
    "        #no rows like in pandas dataframe. \n",
    "        batch_questions = batch_examples[\"question\"]\n",
    "\n",
    "        indices_to_remove = [] #the question have been already asked\n",
    "        already_answered = set([output[\"question\"] for output in outputs]) if len(outputs) > 0 else {}\n",
    "        for i, question in enumerate(batch_questions):\n",
    "            if question in already_answered: \n",
    "                indices_to_remove.append(i)\n",
    "                \n",
    "        batch_examples = {key: [value for i, value in enumerate(values) if i not in indices_to_remove] for key, values in batch_examples.items()}\n",
    "        #slice to only select the questions that have never been answered. \n",
    "        batch_questions = batch_examples[\"question\"]\n",
    "        \n",
    "        batch_answers, batch_relevant_docs = answer_with_rag(batch_questions, llm, knowledge_index, reranker=reranker)\n",
    "        for i , (question, answer , relevant_docs) in enumerate(zip(batch_questions, batch_answers, batch_relevant_docs)):\n",
    "            \n",
    "            if verbose:\n",
    "                print(\"=======================================================\")\n",
    "                print(f\"Question: {question}\")\n",
    "                print(f\"Answer: {answer}\")\n",
    "                print(f'True answer: {batch_examples[\"answer\"][i]}')\n",
    "            \n",
    "            result = {\n",
    "                \"question\": question,\n",
    "                \"true_answer\": batch_examples[\"answer\"][i],\n",
    "                \"source_doc\": batch_examples[\"source_doc\"][i],\n",
    "                \"generated_answer\": answer,\n",
    "                \"retrieved_docs\": [doc for doc in relevant_docs],\n",
    "            }\n",
    "            if test_settings:\n",
    "                result[\"test_settings\"] = test_settings\n",
    "            outputs.append(result)\n",
    "\n",
    "            with open(output_file, \"w\") as f:\n",
    "                json.dump(outputs, f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! mc cp s3/projet-llm-insee-open-data/data/eval_data/eval_dataset.csv ./src/data/\n",
    "eval_dataset = datasets.load_dataset('csv', data_files=\"src/data/eval_dataset.csv\",) #load eval dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_rag_tests(eval_dataset = eval_dataset,\n",
    "                llm = llm,\n",
    "                knowledge_index = vectorstore,\n",
    "                output_file = \"test_generated_ans.json\",\n",
    "                reranker = None,\n",
    "                verbose=True,\n",
    "                test_settings = MODEL_NAME,\n",
    "                batch_size = 5\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get access to test_generated_ans.json** : mc cp s3/projet-llm-insee-open-data/data/eval_data/test_generated_ans.json ./src/data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write an Evaluation prompt for a Critique LLM.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATION_PROMPT = \"\"\"\n",
    "<s><|user|>\n",
    "###Description de la tâche :\n",
    "Une instruction (pouvant inclure une Entrée à l'intérieur), une réponse à évaluer, une réponse de référence qui obtient un score de 5, et une grille de notation représentant un critère d'évaluation sont fournis.\n",
    "\n",
    "###L'instruction à évaluer :\n",
    "{instruction}\n",
    "\n",
    "###Réponse à évaluer :\n",
    "{response}\n",
    "\n",
    "###Réponse de référence (Score 5) :\n",
    "{reference_answer}\n",
    "\n",
    "###Grille de notation :\n",
    "[La réponse est-elle correcte, précise et factuelle par rapport à la réponse de référence ?]\n",
    "Score 1 : La réponse est complètement incorrecte, imprécise et/ou non factuelle.\n",
    "Score 2 : La réponse est principalement incorrecte, imprécise et/ou non factuelle.\n",
    "Score 3 : La réponse est quelque peu correcte, précise et/ou factuelle.\n",
    "Score 4 : La réponse est principalement correcte, précise et factuelle.\n",
    "Score 5 : La réponse est complètement correcte, précise et factuelle.\n",
    "\n",
    "1. Rédigez un feedback détaillé évaluant la qualité de la réponse strictement en fonction de la grille de notation donnée, sans évaluation générale.\n",
    "2. Après avoir rédigé un feedback, attribuez un score qui est un entier entre 1 et 5. Vous devez vous référer à la grille de notation.\n",
    "3. Le format de sortie devrait ressembler à ce qui suit : \"Feedback: {{écrire un feedback pour le critère}} [RESULTAT] {{un nombre entier entre 1 et 5}}\"\n",
    "4. Veuillez ne pas générer d'autres ouvertures, fermetures et explications. Assurez-vous d'inclure la balise [RESULTAT] dans votre sortie.\n",
    "<|end|>\n",
    "<|assistant|>\n",
    "Feedback: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import SystemMessage\n",
    "\n",
    "\n",
    "evaluation_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"Tu es un modèle de langue évaluateur juste\"),\n",
    "        HumanMessagePromptTemplate.from_template(EVALUATION_PROMPT),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluation_prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate the generated answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assess the generated answer by our RAG (using Mistral 8b), we use the open source alternative of GPT4 evaluation called Prometheus-13b-v1.0 part of the prometheus-eval LLM family. It has been fined tune on 100K feedback messages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, BitsAndBytesConfig\n",
    "\n",
    "#teacher_name = \"prometheus-eval/prometheus-7b-v2.0\"\n",
    "teacher_name = \"microsoft/Phi-3-mini-128k-instruct\" \n",
    "\n",
    "#load LLM config \n",
    "teacher_config = AutoConfig.from_pretrained(teacher_name, trust_remote_code=True)\n",
    "\n",
    "#load quantization config \n",
    "teacher_quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=\"float16\",\n",
    "        bnb_4bit_use_double_quant=False,\n",
    "    )\n",
    "#load llm tokenizer \n",
    "teacher_tokenizer = AutoTokenizer.from_pretrained(teacher_name, use_fast=True, device_map='auto')\n",
    "\n",
    "#load llm \n",
    "teacher_model = AutoModelForCausalLM.from_pretrained(teacher_name,\n",
    "        config=teacher_config,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config = teacher_quantization_config ,\n",
    "        trust_remote_code=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a pipeline for Evaluator Model \n",
    "from transformers import pipeline\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "pipeline_HF = pipeline(task=\"text-generation\", # TextGenerationPipeline HF pipeline\n",
    "                model=teacher_model, \n",
    "                tokenizer=teacher_tokenizer,\n",
    "                temperature=0.2, \n",
    "                return_full_text=False, \n",
    "                device_map=\"auto\",\n",
    "                do_sample=True,\n",
    "            )\n",
    "# Create a LangChain Runnable pipeline \n",
    "evaluator_model = HuggingFacePipeline(pipeline=pipeline_HF, model_kwargs={\"max_length\": 4000})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from tqdm import tqdm \n",
    "import json \n",
    "import random\n",
    "\n",
    "generation_args = {\n",
    "    \"return_full_text\": False,\n",
    "    \"do_sample\": True,\n",
    "    'max_length': 10000\n",
    "}\n",
    "\n",
    "def evaluate_answers(\n",
    "    answer_path: str,\n",
    "    eval_chat_model : pipeline,\n",
    "    evaluator_name: str,\n",
    "    evaluation_prompt_template: str, \n",
    "    pipeline_args : dict = generation_args\n",
    ") -> None:\n",
    "    \"\"\"Evaluates generated answers. Modifies the given answer file in place for better checkpointing.\"\"\"\n",
    "    answers = []\n",
    "    if os.path.isfile(answer_path):  # load previous generations if they exist\n",
    "        answers = json.load(open(answer_path, \"r\"))\n",
    "\n",
    "    i = random.randint(0, len(answers))\n",
    "    experiment = answers[i]\n",
    "\n",
    "    for experiment in tqdm(answers):\n",
    "        if f\"eval_score_{evaluator_name}\" in experiment:\n",
    "            continue\n",
    "\n",
    "        eval_prompt = evaluation_prompt_template.format(\n",
    "            instruction=experiment[\"question\"],\n",
    "            response=experiment[\"generated_answer\"],\n",
    "            reference_answer=experiment[\"true_answer\"],\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            eval_result = eval_chat_model(eval_prompt,**pipeline_args)\n",
    "            feedback, score = eval_result[0][\"generated_text\"].split(\"[RESULTAT]\")\n",
    "        except Exception as e:\n",
    "            print('Error:', e)\n",
    "            print(eval_result)\n",
    "            continue\n",
    "        \"\"\"\n",
    "        print(\"feedback : \", feedback)\n",
    "        print(\"score : \", score)\n",
    "        \"\"\"\n",
    "        experiment[f\"eval_score_{evaluator_name}\"] = score\n",
    "        experiment[f\"eval_feedback_{evaluator_name}\"] = feedback\n",
    "\n",
    "        with open(answer_path, \"w\") as f:\n",
    "            json.dump(answers, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_answers(\n",
    "        answer_path = \"/home/onyxia/work/llm-open-data-insee/src/data/test_generated_ans.json\", \n",
    "        eval_chat_model = pipeline_HF,\n",
    "        evaluator_name= teacher_name.replace(\"/\",\"-\"),\n",
    "        evaluation_prompt_template = EVALUATION_PROMPT, \n",
    "        pipeline_args = generation_args\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**inspect the results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "outputs = []\n",
    "for file in glob.glob(\"/home/onyxia/work/llm-open-data-insee/src/data/*.json\"):\n",
    "    output = pd.DataFrame(json.load(open(file, \"r\")))\n",
    "    output[\"settings\"] = file\n",
    "    outputs.append(output)\n",
    "result = pd.concat(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"eval_score_microsoft-Phi-3-mini-128k-instruct\"] = result[\"eval_score_microsoft-Phi-3-mini-128k-instruct\"] .apply(lambda x: int(x) if isinstance(x, str) else 1)\n",
    "result[\"eval_score_microsoft-Phi-3-mini-128k-instruct\"]  = (result[\"eval_score_microsoft-Phi-3-mini-128k-instruct\"] - 1 )/ 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_scores = result.groupby(\"settings\")[\"eval_score_microsoft-Phi-3-mini-128k-instruct\"].mean()\n",
    "average_scores.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
