{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate a RAG pipeline, we need : \n",
    "- a RAG pipeline (Retriever - LLM )\n",
    "- a Vectorial Databse (with associated embedding model)\n",
    "- a test dataset with Q&A. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b]11;?\u001b\\\u001b[6n\u001b[32;1mmc: \u001b[0m\u001b[32;1mConfiguration written to `/home/onyxia/.mc/config.json`. Please update your access credentials.\u001b[0;22m\n",
      "\u001b[32;1mmc: \u001b[0m\u001b[32;1mSuccessfully created `/home/onyxia/.mc/share`.\n",
      "\u001b[0m\u001b[32;1mmc: \u001b[0m\u001b[32;1mInitialized share uploads `/home/onyxia/.mc/share/uploads.json` file.\n",
      "\u001b[0m\u001b[32;1mmc: \u001b[0m\u001b[32;1mInitialized share downloads `/home/onyxia/.mc/share/downloads.json` file.\n",
      "...ataset.csv: 240.80 KiB / 240.80 KiB ┃▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓┃ 1.77 MiB/s 0s\u001b[0;22m\u001b[0m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b]11;?\u001b\\\u001b[6n\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m"
     ]
    }
   ],
   "source": [
    "#load vectorial dataset \n",
    "!mc cp s3/projet-llm-insee-open-data/data/chroma_database/chroma_db  ./src/data --recursive\n",
    "#load test dataset \n",
    "!mc cp s3/projet-llm-insee-open-data/data/eval_data/eval_dataset.csv ./src/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel, RunnableLambda\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "from langchain.vectorstores import Chroma\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TextStreamer\n",
    "import torch\n",
    "\n",
    "#vector database\n",
    "DB_DIR = 'src/data/chroma_db'\n",
    "#embedding model \n",
    "EMB_DEVICE = \"cuda\"\n",
    "\n",
    "EMB_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "#LLM\n",
    "MODEL_DEVICE = {\"\": 0}\n",
    "#MODEL_NAME = \"tiiuae/falcon-7b\"  #use flash attention (faster Attention computation) and Quantization (smaller model memory usage)\n",
    "\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\" \n",
    "#MODEL_NAME = \"EleutherAI/gpt-neo-1.3B\"\n",
    "#MODEL_NAME = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "def build_llm_model():\n",
    "    \"\"\"\n",
    "    Create the llm model\n",
    "    \"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    #load LLM config \n",
    "    config = AutoConfig.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "    config.max_position_embeddings = 8096\n",
    "    #load quantization config \n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=\"float16\",\n",
    "            bnb_4bit_use_double_quant=False,\n",
    "        )\n",
    "    #load llm tokenizer \n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True, device_map='auto') \n",
    "\n",
    "    # Check if tokenizer has a pad_token; if not, set it to eos_token\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    #load llm \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            config=config,\n",
    "            quantization_config=quantization_config\n",
    "        )\n",
    "    #Create a pipeline with  tokenizer and model  \n",
    "    pipeline_HF = pipeline(task=\"text-generation\", # TextGenerationPipeline HF pipeline\n",
    "                model=model, \n",
    "                tokenizer=tokenizer,\n",
    "                max_new_tokens=2000,\n",
    "                temperature=0.2, \n",
    "                return_full_text=False, \n",
    "                device_map=\"auto\",\n",
    "                do_sample=True,\n",
    "            )\n",
    "    # Create a LangChain Runnable pipeline \n",
    "\n",
    "    langchain_llm = HuggingFacePipeline(pipeline=pipeline_HF)\n",
    "\n",
    "    return langchain_llm\n",
    "\n",
    "def format_docs(docs) -> str:\n",
    "    \"\"\"\n",
    "    Format the retrieved document before giving their content to complete the prompt \n",
    "    \"\"\"\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs) \n",
    "\n",
    "\n",
    "def build_chain(hf_embeddings, vectorstore, retriever, prompt, llm):\n",
    "    \"\"\" \n",
    "    Build a LLM chain based on Langchain package and INSEE data \n",
    "    \"\"\"\n",
    "    #Create a Langchain LLM Chain \n",
    "    rag_chain_from_docs = (\n",
    "        RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
    "        | prompt\n",
    "        | llm \n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    return rag_chain_from_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "<s>[INST]\n",
    "En utilisant les informations contenues dans le contexte,\n",
    "fournissez une réponse complète à la question.\n",
    "Répondez uniquement à la question posée, la réponse doit être concise et pertinente par rapport à la question.\n",
    "Fournissez le numéro du document source lorsque cela est pertinent.\n",
    "Si la réponse ne peut pas être déduite du contexte, ne donnez pas de réponse.</s>\n",
    "\n",
    "Contexte :\n",
    "{contexte}\n",
    "---\n",
    "Maintenant, voici la question à laquelle vous devez répondre.\n",
    "\n",
    "Question : {question}\n",
    "[/INST]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create prompt for chat template \n",
    "RAG_PROMPT_TEMPLATE = \"\"\"\n",
    "<s>[INST] \n",
    "Tu es un assistant spécialisé dans la statistique publique répondant aux questions d'agent de l'INSEE. \n",
    "Réponds en Français seulement.\n",
    "Utilise les informations obtenues dans le contexte, réponds de manière argumentée à la question posée.\n",
    "La réponse doit être développée et citer ses sources.\n",
    "\n",
    "Si tu ne peux pas induire ta réponse du contexte, ne réponds pas. \n",
    "Voici le contexte sur lequel tu dois baser ta réponse : \n",
    "Contexte: {context}\n",
    "        ---\n",
    "Voici la question à laquelle tu dois répondre : \n",
    "Question: {question}\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "#load Embedding model \n",
    "hf_embeddings = HuggingFaceEmbeddings(model_name=EMB_MODEL_NAME, model_kwargs={\"device\": EMB_DEVICE})\n",
    "#load vector database\n",
    "vectorstore = Chroma(collection_name=\"insee_data\", embedding_function=hf_embeddings, persist_directory=str(DB_DIR))\n",
    "#set up a retriever \n",
    "retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs = {\"k\":10})\n",
    "#generate prompt template\n",
    "prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=RAG_PROMPT_TEMPLATE)\n",
    "\n",
    "#create a pipeline with tokenizer and LLM\n",
    "llm = build_llm_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "langchain ChromaDB class support batch querying => ask multiple questions and recieved multiple documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragatouille import RAGPretrainedModel\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from langchain_core.documents import Document\n",
    "from typing import Optional, Tuple, List\n",
    "\n",
    "def answer_with_rag(\n",
    "    questions: list[str],\n",
    "    llm_model,\n",
    "    knowledge_index: VectorStore,\n",
    "    reranker: Optional[RAGPretrainedModel] = None,\n",
    "    num_retrieved_docs: int = 30,\n",
    "    num_docs_final: int = 7,\n",
    ") -> Tuple[str, List[Document]]:\n",
    "\n",
    "    \"\"\"Answer a batch of questions using RAG with the given knowledge index.\n",
    "    return a batch of answers and relevant documents. \n",
    "    \"\"\"\n",
    "    batch_final_prompt = []\n",
    "    batch_relevant_documents = []\n",
    "    for q in questions:\n",
    "        # Gather documents with retriever\n",
    "        relevant_docs = knowledge_index.similarity_search(query=q, k=num_retrieved_docs)\n",
    "        relevant_docs = [doc.page_content for doc in relevant_docs]  # keep only the text\n",
    "\n",
    "        # Optionally rerank results\n",
    "        if reranker:\n",
    "            relevant_docs = reranker.rerank(q, relevant_docs, k=num_docs_final)\n",
    "            relevant_docs = [doc[\"content\"] for doc in relevant_docs]\n",
    "\n",
    "        relevant_docs = relevant_docs[:num_docs_final]\n",
    "\n",
    "        # Build the final prompt\n",
    "        context = \"\\nExtracted documents:\\n\"\n",
    "        context += \"\".join([f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)])\n",
    "\n",
    "        final_prompt = RAG_PROMPT_TEMPLATE.format(question=q, context=context)\n",
    "\n",
    "        batch_final_prompt.append(final_prompt)\n",
    "        batch_relevant_documents.append(relevant_docs)\n",
    "\n",
    "    # Redact an answer\n",
    "    batch_answer = llm_model.batch(batch_final_prompt)\n",
    "    batch_answer = [out.replace(\"\\nA: \", \"\") for out in batch_answer] #clean up \n",
    "    return batch_answer, batch_relevant_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generating Answers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import time \n",
    "import numpy as np\n",
    "\n",
    "def run_rag_tests(\n",
    "    eval_dataset: datasets.Dataset,\n",
    "    llm,\n",
    "    knowledge_index: VectorStore,\n",
    "    output_file: str,\n",
    "    reranker: Optional[RAGPretrainedModel] = None,\n",
    "    verbose: Optional[bool] = True,\n",
    "    test_settings: Optional[str] = None,  # To document the test settings used\n",
    "    batch_size = 2\n",
    "):\n",
    "    \"\"\"Runs RAG tests on the given dataset and saves the results to the given output file.\"\"\"\n",
    "    try:  # load previous generations if they exist\n",
    "        with open(output_file, \"r\") as f:\n",
    "            outputs = json.load(f)\n",
    "    except:\n",
    "        outputs = []\n",
    "\n",
    "    num_rows = len(eval_dataset[\"train\"])\n",
    "\n",
    "    for batch_start in tqdm(range(0, num_rows, batch_size)):\n",
    "\n",
    "        # Adjust batch end to handle the last incomplete batch\n",
    "        batch_end = min(batch_start + batch_size, num_rows)\n",
    "        batch_examples = eval_dataset[\"train\"][batch_start:batch_end]\n",
    "        #eval_dataset is a HF dataset then a slice return a sub dataset (a dict {'context' : [...,...]}) \n",
    "        #no rows like in pandas dataframe. \n",
    "        batch_questions = batch_examples[\"question\"]\n",
    "\n",
    "        indices_to_remove = [] #the question have been already asked\n",
    "        already_answered = set([output[\"question\"] for output in outputs]) if len(outputs) > 0 else {}\n",
    "        for i, question in enumerate(batch_questions):\n",
    "            if question in already_answered: \n",
    "                indices_to_remove.append(i)\n",
    "                \n",
    "        batch_examples = {key: [value for i, value in enumerate(values) if i not in indices_to_remove] for key, values in batch_examples.items()}\n",
    "        #slice to only select the questions that have never been answered. \n",
    "        batch_questions = batch_examples[\"question\"]\n",
    "        \n",
    "        batch_answers, batch_relevant_docs = answer_with_rag(batch_questions, llm, knowledge_index, reranker=reranker)\n",
    "        for i , (question, answer , relevant_docs) in enumerate(zip(batch_questions, batch_answers, batch_relevant_docs)):\n",
    "            \n",
    "            if verbose:\n",
    "                print(\"=======================================================\")\n",
    "                print(f\"Question: {question}\")\n",
    "                print(f\"Answer: {answer}\")\n",
    "                print(f'True answer: {batch_examples[\"answer\"][i]}')\n",
    "            \n",
    "            result = {\n",
    "                \"question\": question,\n",
    "                \"true_answer\": batch_examples[\"answer\"][i],\n",
    "                \"source_doc\": batch_examples[\"source_doc\"][i],\n",
    "                \"generated_answer\": answer,\n",
    "                \"retrieved_docs\": [doc for doc in relevant_docs],\n",
    "            }\n",
    "            if test_settings:\n",
    "                result[\"test_settings\"] = test_settings\n",
    "            outputs.append(result)\n",
    "\n",
    "            with open(output_file, \"w\") as f:\n",
    "                json.dump(outputs, f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! mc cp s3/projet-llm-insee-open-data/data/eval_data/eval_dataset.csv ./src/data/\n",
    "eval_dataset = datasets.load_dataset('csv', data_files=\"src/data/eval_dataset.csv\",) #load eval dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_rag_tests(eval_dataset = eval_dataset,\n",
    "                llm = llm,\n",
    "                knowledge_index = vectorstore,\n",
    "                output_file = \"test_generated_ans.json\",\n",
    "                reranker = None,\n",
    "                verbose=True,\n",
    "                test_settings = MODEL_NAME,\n",
    "                batch_size = 5\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get access to test_generated_ans.json** : mc cp s3/projet-llm-insee-open-data/data/eval_data/test_generated_ans.json ./src/data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write an Evaluation prompt for a Critique LLM.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATION_PROMPT = \"\"\"\n",
    "<s><|user|>\n",
    "###Description de la tâche :\n",
    "Une instruction (pouvant inclure une Entrée à l'intérieur), une réponse à évaluer, une réponse de référence qui obtient un score de 5, et une grille de notation représentant un critère d'évaluation sont fournis.\n",
    "\n",
    "###L'instruction à évaluer :\n",
    "{instruction}\n",
    "\n",
    "###Réponse à évaluer :\n",
    "{response}\n",
    "\n",
    "###Réponse de référence (Score 5) :\n",
    "{reference_answer}\n",
    "\n",
    "###Grille de notation :\n",
    "[La réponse est-elle correcte, précise et factuelle par rapport à la réponse de référence ?]\n",
    "Score 1 : La réponse est complètement incorrecte, imprécise et/ou non factuelle.\n",
    "Score 2 : La réponse est principalement incorrecte, imprécise et/ou non factuelle.\n",
    "Score 3 : La réponse est quelque peu correcte, précise et/ou factuelle.\n",
    "Score 4 : La réponse est principalement correcte, précise et factuelle.\n",
    "Score 5 : La réponse est complètement correcte, précise et factuelle.\n",
    "\n",
    "1. Rédigez un feedback détaillé évaluant la qualité de la réponse strictement en fonction de la grille de notation donnée, sans évaluation générale.\n",
    "2. Après avoir rédigé un feedback, attribuez un score qui est un entier entre 1 et 5. Vous devez vous référer à la grille de notation.\n",
    "3. Le format de sortie devrait ressembler à ce qui suit : \"Feedback: {{écrire un feedback pour le critère}} [RESULTAT] {{un nombre entier entre 1 et 5}}\"\n",
    "4. Veuillez ne pas générer d'autres ouvertures, fermetures et explications. Assurez-vous d'inclure la balise [RESULTAT] dans votre sortie.\n",
    "<|end|>\n",
    "<|assistant|>\n",
    "Feedback: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import SystemMessage\n",
    "\n",
    "\n",
    "evaluation_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"Tu es un modèle de langue évaluateur juste\"),\n",
    "        HumanMessagePromptTemplate.from_template(EVALUATION_PROMPT),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['instruction', 'reference_answer', 'response'] messages=[SystemMessage(content='Tu es un modèle de langage évaluateur juste'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['instruction', 'reference_answer', 'response'], template='\\n<s><|user|>\\n###Description de la tâche :\\nUne instruction (pouvant inclure une Entrée à l\\'intérieur), une réponse à évaluer, une réponse de référence qui obtient un score de 5, et une grille de notation représentant un critère d\\'évaluation sont fournis.\\n1. Rédigez un feedback détaillé évaluant la qualité de la réponse strictement en fonction de la grille de notation donnée, sans évaluation générale.\\n2. Après avoir rédigé un feedback, attribuez un score qui est un entier entre 1 et 5. Vous devez vous référer à la grille de notation.\\n3. Le format de sortie devrait ressembler à ce qui suit : \"Feedback: {{écrire un feedback pour le critère}} [RESULTAT] {{un nombre entier entre 1 et 5}}\"\\n4. Veuillez ne pas générer d\\'autres ouvertures, fermetures et explications. Assurez-vous d\\'inclure [RESULTAT] dans votre sortie.\\n\\n###L\\'instruction à évaluer :\\n{instruction}\\n\\n###Réponse à évaluer :\\n{response}\\n\\n###Réponse de référence (Score 5) :\\n{reference_answer}\\n\\n###Grille de notation :\\n[La réponse est-elle correcte, précise et factuelle par rapport à la réponse de référence ?]\\nScore 1 : La réponse est complètement incorrecte, imprécise et/ou non factuelle.\\nScore 2 : La réponse est principalement incorrecte, imprécise et/ou non factuelle.\\nScore 3 : La réponse est quelque peu correcte, précise et/ou factuelle.\\nScore 4 : La réponse est principalement correcte, précise et factuelle.\\nScore 5 : La réponse est complètement correcte, précise et factuelle.\\n<|end|>\\n<|assistant|>\\n###Feedback :'))]\n"
     ]
    }
   ],
   "source": [
    "print(evaluation_prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate the generated answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assess the generated answer by our RAG (using Mistral 8b), we use the open source alternative of GPT4 evaluation called Prometheus-13b-v1.0 part of the prometheus-eval LLM family. It has been fined tune on 100K feedback messages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5dcfc9c0aa24849ab2cb723e35f6252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a988067da8d04429a7074b28765630e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1de06d20db049c5a1d2b6177aebe9a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c5ec86b9245424a9db3c825ba55e69e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a39146fee4f4e5288aca5b9b079f114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/568 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd6ef3fc7b574fe4ad903c43a63ab396",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_phi3.py:   0%|          | 0.00/73.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-128k-instruct:\n",
      "- modeling_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attenton` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb883e8fbedb4e038b2455662b1ecde5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/16.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d469ff54d063416c90b9723ad7d6b3b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3603eefc4dca406a9efc16e7026c8837",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50cdbe180f77406b95cfe795576c6131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58bb70e014ed475b82cccd66c988ab7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "911f818dfb3f4a45a336fd5bd9733214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/172 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, BitsAndBytesConfig\n",
    "\n",
    "#teacher_name = \"prometheus-eval/prometheus-7b-v2.0\"\n",
    "teacher_name = \"microsoft/Phi-3-mini-128k-instruct\" \n",
    "\n",
    "#load LLM config \n",
    "teacher_config = AutoConfig.from_pretrained(teacher_name, trust_remote_code=True)\n",
    "\n",
    "#load quantization config \n",
    "teacher_quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=\"float16\",\n",
    "        bnb_4bit_use_double_quant=False,\n",
    "    )\n",
    "#load llm tokenizer \n",
    "teacher_tokenizer = AutoTokenizer.from_pretrained(teacher_name, use_fast=True, device_map='auto')\n",
    "\n",
    "#load llm \n",
    "teacher_model = AutoModelForCausalLM.from_pretrained(teacher_name,\n",
    "        config=teacher_config,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config = teacher_quantization_config ,\n",
    "        trust_remote_code=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a pipeline for Evaluator Model \n",
    "from transformers import pipeline\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "pipeline_HF = pipeline(task=\"text-generation\", # TextGenerationPipeline HF pipeline\n",
    "                model=teacher_model, \n",
    "                tokenizer=teacher_tokenizer,\n",
    "                temperature=0.2, \n",
    "                return_full_text=False, \n",
    "                device_map=\"auto\",\n",
    "                do_sample=True,\n",
    "            )\n",
    "# Create a LangChain Runnable pipeline \n",
    "evaluator_model = HuggingFacePipeline(pipeline=pipeline_HF, model_kwargs={\"max_length\": 4000})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from tqdm import tqdm \n",
    "import json \n",
    "import random\n",
    "\n",
    "generation_args = {\n",
    "    \"return_full_text\": False,\n",
    "    \"do_sample\": True,\n",
    "    'max_length': 10000\n",
    "}\n",
    "\n",
    "def evaluate_answers(\n",
    "    answer_path: str,\n",
    "    eval_chat_model : pipeline,\n",
    "    evaluator_name: str,\n",
    "    evaluation_prompt_template: str, \n",
    "    pipeline_args : dict = generation_args\n",
    ") -> None:\n",
    "    \"\"\"Evaluates generated answers. Modifies the given answer file in place for better checkpointing.\"\"\"\n",
    "    answers = []\n",
    "    if os.path.isfile(answer_path):  # load previous generations if they exist\n",
    "        answers = json.load(open(answer_path, \"r\"))\n",
    "\n",
    "    i = random.randint(0, len(answers))\n",
    "    experiment = answers[i]\n",
    "\n",
    "    for experiment in tqdm(answers):\n",
    "        if f\"eval_score_{evaluator_name}\" in experiment:\n",
    "            continue\n",
    "\n",
    "        eval_prompt = evaluation_prompt_template.format(\n",
    "            instruction=experiment[\"question\"],\n",
    "            response=experiment[\"generated_answer\"],\n",
    "            reference_answer=experiment[\"true_answer\"],\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            eval_result = eval_chat_model(eval_prompt,**pipeline_args)\n",
    "            feedback, score = eval_result[0][\"generated_text\"].split(\"[RESULTAT]\")\n",
    "        except Exception as e:\n",
    "            print('Error:', e)\n",
    "            print(eval_result)\n",
    "            continue\n",
    "        \"\"\"\n",
    "        print(\"feedback : \", feedback)\n",
    "        print(\"score : \", score)\n",
    "        \"\"\"\n",
    "        experiment[f\"eval_score_{evaluator_name}\"] = score\n",
    "        experiment[f\"eval_feedback_{evaluator_name}\"] = feedback\n",
    "\n",
    "        with open(answer_path, \"w\") as f:\n",
    "            json.dump(answers, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 57/77 [00:00<00:00, 92.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: not enough values to unpack (expected 2, got 1)\n",
      "[{'generated_text': ''}]\n",
      "Error: Input length of input_ids is 2048, but `max_length` is set to 2048. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.\n",
      "[{'generated_text': ''}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [03:04<00:00,  2.40s/it]\n"
     ]
    }
   ],
   "source": [
    "evaluate_answers(\n",
    "        answer_path = \"/home/onyxia/work/llm-open-data-insee/src/data/test_generated_ans.json\", \n",
    "        eval_chat_model = pipeline_HF,\n",
    "        evaluator_name= teacher_name.replace(\"/\",\"-\"),\n",
    "        evaluation_prompt_template = EVALUATION_PROMPT, \n",
    "        pipeline_args = generation_args\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**inspect the results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "outputs = []\n",
    "for file in glob.glob(\"/home/onyxia/work/llm-open-data-insee/src/data/*.json\"):\n",
    "    output = pd.DataFrame(json.load(open(file, \"r\")))\n",
    "    output[\"settings\"] = file\n",
    "    outputs.append(output)\n",
    "result = pd.concat(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"eval_score_microsoft-Phi-3-mini-128k-instruct\"] = result[\"eval_score_microsoft-Phi-3-mini-128k-instruct\"] .apply(lambda x: int(x) if isinstance(x, str) else 1)\n",
    "result[\"eval_score_microsoft-Phi-3-mini-128k-instruct\"]  = (result[\"eval_score_microsoft-Phi-3-mini-128k-instruct\"] - 1 )/ 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "settings\n",
       "/home/onyxia/work/llm-open-data-insee/src/data/test_generated_ans.json    0.282468\n",
       "Name: eval_score_microsoft-Phi-3-mini-128k-instruct, dtype: float64"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_scores = result.groupby(\"settings\")[\"eval_score_microsoft-Phi-3-mini-128k-instruct\"].mean()\n",
    "average_scores.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>true_answer</th>\n",
       "      <th>source_doc</th>\n",
       "      <th>generated_answer</th>\n",
       "      <th>retrieved_docs</th>\n",
       "      <th>test_settings</th>\n",
       "      <th>eval_score_microsoft-Phi-3-mini-128k-instruct</th>\n",
       "      <th>eval_feedback_microsoft-Phi-3-mini-128k-instruct</th>\n",
       "      <th>settings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Quelle était la population du Territoire de la...</td>\n",
       "      <td>La population du Territoire de la Côte Ouest é...</td>\n",
       "      <td>https://www.insee.fr/fr/statistiques/1293858</td>\n",
       "      <td>La population du Territoire de la Côte Ouest n...</td>\n",
       "      <td>[. Entre 2008 et 2013, le rythme annuel de pro...</td>\n",
       "      <td>mistralai/Mistral-7B-Instruct-v0.2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>\\n\\nLa réponse fournie n'est pas correcte, pré...</td>\n",
       "      <td>/home/onyxia/work/llm-open-data-insee/src/data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quelles mesures ont été mises en place pour pr...</td>\n",
       "      <td>Des mesures telles que la loi Génisson (2001),...</td>\n",
       "      <td>https://www.insee.fr/fr/statistiques/1285800</td>\n",
       "      <td>Dans le contexte fourni, plusieurs mesures ont...</td>\n",
       "      <td>[. Dans ce cadre, en juillet 2014, un premier ...</td>\n",
       "      <td>mistralai/Mistral-7B-Instruct-v0.2</td>\n",
       "      <td>0.50</td>\n",
       "      <td>\\n\\nLa réponse fournie présente une structure ...</td>\n",
       "      <td>/home/onyxia/work/llm-open-data-insee/src/data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Quand Mayotte a-t-elle commencé à utiliser la ...</td>\n",
       "      <td>Mayotte a commencé à utiliser la méthode commu...</td>\n",
       "      <td>https://www.insee.fr/fr/statistiques/7728787</td>\n",
       "      <td>Mayotte a commencé à utiliser la méthode commu...</td>\n",
       "      <td>[Mayotte intègre désormais la méthode commune ...</td>\n",
       "      <td>mistralai/Mistral-7B-Instruct-v0.2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>\\n\\nLa réponse fournie est directe et correspo...</td>\n",
       "      <td>/home/onyxia/work/llm-open-data-insee/src/data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Combien d'emplois sont considérés comme liés a...</td>\n",
       "      <td>Pour les secteurs d'activités partiellement to...</td>\n",
       "      <td>https://www.insee.fr/fr/statistiques/1283777</td>\n",
       "      <td>Dans le contexte fourni, les emplois liés au t...</td>\n",
       "      <td>[. L’emploi touristique englobe les activités,...</td>\n",
       "      <td>mistralai/Mistral-7B-Instruct-v0.2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>\\n\\nLa réponse fournie ne correspond pas à la ...</td>\n",
       "      <td>/home/onyxia/work/llm-open-data-insee/src/data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Quelle est la différence de nombre de décès en...</td>\n",
       "      <td>La différence de nombre de décès entre le 1er ...</td>\n",
       "      <td>https://www.insee.fr/fr/statistiques/5055298</td>\n",
       "      <td>La région de Bourgogne-Franche-Comté a enregis...</td>\n",
       "      <td>[. C’est particulièrement le cas pour avril 20...</td>\n",
       "      <td>mistralai/Mistral-7B-Instruct-v0.2</td>\n",
       "      <td>0.50</td>\n",
       "      <td>\\n\\nLa réponse donnée mentionne correctement l...</td>\n",
       "      <td>/home/onyxia/work/llm-open-data-insee/src/data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Quelle est la différence de rémunération moyen...</td>\n",
       "      <td>La différence de rémunération moyenne entre le...</td>\n",
       "      <td>https://www.insee.fr/fr/statistiques/1372799</td>\n",
       "      <td>Dans le contexte fourni, les documents indique...</td>\n",
       "      <td>[En France, les accords collectifs, négociés a...</td>\n",
       "      <td>mistralai/Mistral-7B-Instruct-v0.2</td>\n",
       "      <td>0.50</td>\n",
       "      <td>\\n\\nLa réponse fournie ne mentionne pas la dif...</td>\n",
       "      <td>/home/onyxia/work/llm-open-data-insee/src/data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Quelle est la progression de l'emploi salarié ...</td>\n",
       "      <td>La progression de l'emploi salarié dans le sec...</td>\n",
       "      <td>https://www.insee.fr/fr/statistiques/4277983</td>\n",
       "      <td>Dans le contexte fourni, il n'y a pas de donné...</td>\n",
       "      <td>[. Ce changement de source a pu entraîner une ...</td>\n",
       "      <td>mistralai/Mistral-7B-Instruct-v0.2</td>\n",
       "      <td>0.25</td>\n",
       "      <td>La réponse fournie ne correspond pas à la rép...</td>\n",
       "      <td>/home/onyxia/work/llm-open-data-insee/src/data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Quelle est la tendance démographique de la pop...</td>\n",
       "      <td>La population régionale de Bourgogne-Franche-C...</td>\n",
       "      <td>https://www.insee.fr/fr/statistiques/5369489</td>\n",
       "      <td>La tendance démographique de la population rég...</td>\n",
       "      <td>[Si les tendances démographiques récentes se m...</td>\n",
       "      <td>mistralai/Mistral-7B-Instruct-v0.2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>\\n\\nLa réponse fournie ne correspond pas à la ...</td>\n",
       "      <td>/home/onyxia/work/llm-open-data-insee/src/data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Quels sont les sujets principaux traités dans ...</td>\n",
       "      <td>Les sujets principaux traités dans le rapport ...</td>\n",
       "      <td>https://www.insee.fr/fr/statistiques/2894036</td>\n",
       "      <td>Le rapport annuel \"L'Économie française - Comp...</td>\n",
       "      <td>[L'économie française - Comptes et dossiers Éd...</td>\n",
       "      <td>mistralai/Mistral-7B-Instruct-v0.2</td>\n",
       "      <td>0.25</td>\n",
       "      <td>\\n\\nLa réponse fournie présente une synthèse d...</td>\n",
       "      <td>/home/onyxia/work/llm-open-data-insee/src/data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Quelle industrie a connu une augmentation de +...</td>\n",
       "      <td>L'industrie pharmaceutique a connu une augment...</td>\n",
       "      <td>https://www.insee.fr/fr/information/3357117</td>\n",
       "      <td>Based on the provided context, I cannot direct...</td>\n",
       "      <td>[. Les prix des produits de santé augmentent é...</td>\n",
       "      <td>mistralai/Mistral-7B-Instruct-v0.2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>\\n\\nLa réponse donnée ne correspond pas à la r...</td>\n",
       "      <td>/home/onyxia/work/llm-open-data-insee/src/data...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>77 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question  \\\n",
       "0   Quelle était la population du Territoire de la...   \n",
       "1   Quelles mesures ont été mises en place pour pr...   \n",
       "2   Quand Mayotte a-t-elle commencé à utiliser la ...   \n",
       "3   Combien d'emplois sont considérés comme liés a...   \n",
       "4   Quelle est la différence de nombre de décès en...   \n",
       "..                                                ...   \n",
       "72  Quelle est la différence de rémunération moyen...   \n",
       "73  Quelle est la progression de l'emploi salarié ...   \n",
       "74  Quelle est la tendance démographique de la pop...   \n",
       "75  Quels sont les sujets principaux traités dans ...   \n",
       "76  Quelle industrie a connu une augmentation de +...   \n",
       "\n",
       "                                          true_answer  \\\n",
       "0   La population du Territoire de la Côte Ouest é...   \n",
       "1   Des mesures telles que la loi Génisson (2001),...   \n",
       "2   Mayotte a commencé à utiliser la méthode commu...   \n",
       "3   Pour les secteurs d'activités partiellement to...   \n",
       "4   La différence de nombre de décès entre le 1er ...   \n",
       "..                                                ...   \n",
       "72  La différence de rémunération moyenne entre le...   \n",
       "73  La progression de l'emploi salarié dans le sec...   \n",
       "74  La population régionale de Bourgogne-Franche-C...   \n",
       "75  Les sujets principaux traités dans le rapport ...   \n",
       "76  L'industrie pharmaceutique a connu une augment...   \n",
       "\n",
       "                                      source_doc  \\\n",
       "0   https://www.insee.fr/fr/statistiques/1293858   \n",
       "1   https://www.insee.fr/fr/statistiques/1285800   \n",
       "2   https://www.insee.fr/fr/statistiques/7728787   \n",
       "3   https://www.insee.fr/fr/statistiques/1283777   \n",
       "4   https://www.insee.fr/fr/statistiques/5055298   \n",
       "..                                           ...   \n",
       "72  https://www.insee.fr/fr/statistiques/1372799   \n",
       "73  https://www.insee.fr/fr/statistiques/4277983   \n",
       "74  https://www.insee.fr/fr/statistiques/5369489   \n",
       "75  https://www.insee.fr/fr/statistiques/2894036   \n",
       "76   https://www.insee.fr/fr/information/3357117   \n",
       "\n",
       "                                     generated_answer  \\\n",
       "0   La population du Territoire de la Côte Ouest n...   \n",
       "1   Dans le contexte fourni, plusieurs mesures ont...   \n",
       "2   Mayotte a commencé à utiliser la méthode commu...   \n",
       "3   Dans le contexte fourni, les emplois liés au t...   \n",
       "4   La région de Bourgogne-Franche-Comté a enregis...   \n",
       "..                                                ...   \n",
       "72  Dans le contexte fourni, les documents indique...   \n",
       "73  Dans le contexte fourni, il n'y a pas de donné...   \n",
       "74  La tendance démographique de la population rég...   \n",
       "75  Le rapport annuel \"L'Économie française - Comp...   \n",
       "76  Based on the provided context, I cannot direct...   \n",
       "\n",
       "                                       retrieved_docs  \\\n",
       "0   [. Entre 2008 et 2013, le rythme annuel de pro...   \n",
       "1   [. Dans ce cadre, en juillet 2014, un premier ...   \n",
       "2   [Mayotte intègre désormais la méthode commune ...   \n",
       "3   [. L’emploi touristique englobe les activités,...   \n",
       "4   [. C’est particulièrement le cas pour avril 20...   \n",
       "..                                                ...   \n",
       "72  [En France, les accords collectifs, négociés a...   \n",
       "73  [. Ce changement de source a pu entraîner une ...   \n",
       "74  [Si les tendances démographiques récentes se m...   \n",
       "75  [L'économie française - Comptes et dossiers Éd...   \n",
       "76  [. Les prix des produits de santé augmentent é...   \n",
       "\n",
       "                         test_settings  \\\n",
       "0   mistralai/Mistral-7B-Instruct-v0.2   \n",
       "1   mistralai/Mistral-7B-Instruct-v0.2   \n",
       "2   mistralai/Mistral-7B-Instruct-v0.2   \n",
       "3   mistralai/Mistral-7B-Instruct-v0.2   \n",
       "4   mistralai/Mistral-7B-Instruct-v0.2   \n",
       "..                                 ...   \n",
       "72  mistralai/Mistral-7B-Instruct-v0.2   \n",
       "73  mistralai/Mistral-7B-Instruct-v0.2   \n",
       "74  mistralai/Mistral-7B-Instruct-v0.2   \n",
       "75  mistralai/Mistral-7B-Instruct-v0.2   \n",
       "76  mistralai/Mistral-7B-Instruct-v0.2   \n",
       "\n",
       "    eval_score_microsoft-Phi-3-mini-128k-instruct  \\\n",
       "0                                            0.00   \n",
       "1                                            0.50   \n",
       "2                                            1.00   \n",
       "3                                            0.00   \n",
       "4                                            0.50   \n",
       "..                                            ...   \n",
       "72                                           0.50   \n",
       "73                                           0.25   \n",
       "74                                           0.00   \n",
       "75                                           0.25   \n",
       "76                                           0.00   \n",
       "\n",
       "     eval_feedback_microsoft-Phi-3-mini-128k-instruct  \\\n",
       "0   \\n\\nLa réponse fournie n'est pas correcte, pré...   \n",
       "1   \\n\\nLa réponse fournie présente une structure ...   \n",
       "2   \\n\\nLa réponse fournie est directe et correspo...   \n",
       "3   \\n\\nLa réponse fournie ne correspond pas à la ...   \n",
       "4   \\n\\nLa réponse donnée mentionne correctement l...   \n",
       "..                                                ...   \n",
       "72  \\n\\nLa réponse fournie ne mentionne pas la dif...   \n",
       "73   La réponse fournie ne correspond pas à la rép...   \n",
       "74  \\n\\nLa réponse fournie ne correspond pas à la ...   \n",
       "75  \\n\\nLa réponse fournie présente une synthèse d...   \n",
       "76  \\n\\nLa réponse donnée ne correspond pas à la r...   \n",
       "\n",
       "                                             settings  \n",
       "0   /home/onyxia/work/llm-open-data-insee/src/data...  \n",
       "1   /home/onyxia/work/llm-open-data-insee/src/data...  \n",
       "2   /home/onyxia/work/llm-open-data-insee/src/data...  \n",
       "3   /home/onyxia/work/llm-open-data-insee/src/data...  \n",
       "4   /home/onyxia/work/llm-open-data-insee/src/data...  \n",
       "..                                                ...  \n",
       "72  /home/onyxia/work/llm-open-data-insee/src/data...  \n",
       "73  /home/onyxia/work/llm-open-data-insee/src/data...  \n",
       "74  /home/onyxia/work/llm-open-data-insee/src/data...  \n",
       "75  /home/onyxia/work/llm-open-data-insee/src/data...  \n",
       "76  /home/onyxia/work/llm-open-data-insee/src/data...  \n",
       "\n",
       "[77 rows x 9 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
