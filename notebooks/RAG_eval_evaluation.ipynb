{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate a RAG pipeline, we need : \n",
    "- a RAG pipeline (Retriever - LLM )\n",
    "- a Vectorial Databse (with associated embedding model)\n",
    "- a test dataset with Q&A. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b]11;?\u001b\\\u001b[6n\u001b[32;1mmc: \u001b[0m\u001b[32;1mConfiguration written to `/home/onyxia/.mc/config.json`. Please update your access credentials.\u001b[0;22m\n",
      "\u001b[32;1mmc: \u001b[0m\u001b[32;1mSuccessfully created `/home/onyxia/.mc/share`.\n",
      "\u001b[0m\u001b[32;1mmc: \u001b[0m\u001b[32;1mInitialized share uploads `/home/onyxia/.mc/share/uploads.json` file.\n",
      "\u001b[0m\u001b[32;1mmc: \u001b[0m\u001b[32;1mInitialized share downloads `/home/onyxia/.mc/share/downloads.json` file.\n",
      "...ataset.csv: 240.80 KiB / 240.80 KiB ┃▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓┃ 1.77 MiB/s 0s\u001b[0;22m\u001b[0m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b]11;?\u001b\\\u001b[6n\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m"
     ]
    }
   ],
   "source": [
    "#load vectorial dataset \n",
    "!mc cp s3/projet-llm-insee-open-data/data/chroma_database/chroma_db  ./src/data --recursive\n",
    "#load test dataset \n",
    "!mc cp s3/projet-llm-insee-open-data/data/eval_data/eval_dataset.csv ./src/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create prompt for chat template \n",
    "RAG_PROMPT_TEMPLATE = \"\"\"\n",
    "<s>[INST] \n",
    "Tu es un assistant spécialisé dans la statistique publique répondant aux questions d'agent de l'INSEE. \n",
    "Réponds en Français seulement.\n",
    "Utilise les informations obtenues dans le contexte, réponds de manière argumentée à la question posée.\n",
    "La réponse doit être développée et citer ses sources.\n",
    "\n",
    "Si tu ne peux pas induire ta réponse du contexte, ne réponds pas. \n",
    "Voici le contexte sur lequel tu dois baser ta réponse : \n",
    "Contexte: {context}\n",
    "        ---\n",
    "Voici la question à laquelle tu dois répondre : \n",
    "Question: {question}\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "#load Embedding model \n",
    "hf_embeddings = HuggingFaceEmbeddings(model_name=EMB_MODEL_NAME, model_kwargs={\"device\": EMB_DEVICE})\n",
    "#load vector database\n",
    "vectorstore = Chroma(collection_name=\"insee_data\", embedding_function=hf_embeddings, persist_directory=str(DB_DIR))\n",
    "#set up a retriever \n",
    "retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs = {\"k\":10})\n",
    "#generate prompt template\n",
    "prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=RAG_PROMPT_TEMPLATE)\n",
    "\n",
    "#create a pipeline with tokenizer and LLM\n",
    "llm = build_llm_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "langchain ChromaDB class support batch querying => ask multiple questions and recieved multiple documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragatouille import RAGPretrainedModel\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from langchain_core.documents import Document\n",
    "from typing import Optional, Tuple, List\n",
    "\n",
    "def answer_with_rag(\n",
    "    questions: list[str],\n",
    "    llm_model,\n",
    "    knowledge_index: VectorStore,\n",
    "    reranker: Optional[RAGPretrainedModel] = None,\n",
    "    num_retrieved_docs: int = 30,\n",
    "    num_docs_final: int = 7,\n",
    ") -> Tuple[str, List[Document]]:\n",
    "\n",
    "    \"\"\"Answer a batch of questions using RAG with the given knowledge index.\n",
    "    return a batch of answers and relevant documents. \n",
    "    \"\"\"\n",
    "    batch_final_prompt = []\n",
    "    batch_relevant_documents = []\n",
    "    for q in questions:\n",
    "        # Gather documents with retriever\n",
    "        relevant_docs = knowledge_index.similarity_search(query=q, k=num_retrieved_docs)\n",
    "        relevant_docs = [doc.page_content for doc in relevant_docs]  # keep only the text\n",
    "\n",
    "        # Optionally rerank results\n",
    "        if reranker:\n",
    "            relevant_docs = reranker.rerank(q, relevant_docs, k=num_docs_final)\n",
    "            relevant_docs = [doc[\"content\"] for doc in relevant_docs]\n",
    "\n",
    "        relevant_docs = relevant_docs[:num_docs_final]\n",
    "\n",
    "        # Build the final prompt\n",
    "        context = \"\\nExtracted documents:\\n\"\n",
    "        context += \"\".join([f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)])\n",
    "\n",
    "        final_prompt = RAG_PROMPT_TEMPLATE.format(question=q, context=context)\n",
    "\n",
    "        batch_final_prompt.append(final_prompt)\n",
    "        batch_relevant_documents.append(relevant_docs)\n",
    "\n",
    "    # Redact an answer\n",
    "    batch_answer = llm_model.batch(batch_final_prompt)\n",
    "    batch_answer = [out.replace(\"\\nA: \", \"\") for out in batch_answer] #clean up \n",
    "    return batch_answer, batch_relevant_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import time \n",
    "import numpy as np\n",
    "\n",
    "def run_rag_tests(\n",
    "    eval_dataset: datasets.Dataset,\n",
    "    llm,\n",
    "    knowledge_index: VectorStore,\n",
    "    output_file: str,\n",
    "    reranker: Optional[RAGPretrainedModel] = None,\n",
    "    verbose: Optional[bool] = True,\n",
    "    test_settings: Optional[str] = None,  # To document the test settings used\n",
    "    batch_size = 2\n",
    "):\n",
    "    \"\"\"Runs RAG tests on the given dataset and saves the results to the given output file.\"\"\"\n",
    "    try:  # load previous generations if they exist\n",
    "        with open(output_file, \"r\") as f:\n",
    "            outputs = json.load(f)\n",
    "    except:\n",
    "        outputs = []\n",
    "\n",
    "    num_rows = len(eval_dataset[\"train\"])\n",
    "\n",
    "    for batch_start in tqdm(range(0, num_rows, batch_size)):\n",
    "\n",
    "        # Adjust batch end to handle the last incomplete batch\n",
    "        batch_end = min(batch_start + batch_size, num_rows)\n",
    "        batch_examples = eval_dataset[\"train\"][batch_start:batch_end]\n",
    "        #eval_dataset is a HF dataset then a slice return a sub dataset (a dict {'context' : [...,...]}) \n",
    "        #no rows like in pandas dataframe. \n",
    "        batch_questions = batch_examples[\"question\"]\n",
    "\n",
    "        indices_to_remove = [] #the question have been already asked\n",
    "        already_answered = set([output[\"question\"] for output in outputs]) if len(outputs) > 0 else {}\n",
    "        for i, question in enumerate(batch_questions):\n",
    "            if question in already_answered: \n",
    "                indices_to_remove.append(i)\n",
    "                \n",
    "        batch_examples = {key: [value for i, value in enumerate(values) if i not in indices_to_remove] for key, values in batch_examples.items()}\n",
    "        #slice to only select the questions that have never been answered. \n",
    "        batch_questions = batch_examples[\"question\"]\n",
    "        \n",
    "        batch_answers, batch_relevant_docs = answer_with_rag(batch_questions, llm, knowledge_index, reranker=reranker)\n",
    "        for i , (question, answer , relevant_docs) in enumerate(zip(batch_questions, batch_answers, batch_relevant_docs)):\n",
    "            \n",
    "            if verbose:\n",
    "                print(\"=======================================================\")\n",
    "                print(f\"Question: {question}\")\n",
    "                print(f\"Answer: {answer}\")\n",
    "                print(f'True answer: {batch_examples[\"answer\"][i]}')\n",
    "            \n",
    "            result = {\n",
    "                \"question\": question,\n",
    "                \"true_answer\": batch_examples[\"answer\"][i],\n",
    "                \"source_doc\": batch_examples[\"source_doc\"][i],\n",
    "                \"generated_answer\": answer,\n",
    "                \"retrieved_docs\": [doc for doc in relevant_docs],\n",
    "            }\n",
    "            if test_settings:\n",
    "                result[\"test_settings\"] = test_settings\n",
    "            outputs.append(result)\n",
    "\n",
    "            with open(output_file, \"w\") as f:\n",
    "                json.dump(outputs, f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! mc cp s3/projet-llm-insee-open-data/data/eval_data/eval_dataset.csv ./src/data/\n",
    "eval_dataset = datasets.load_dataset('csv', data_files=\"src/data/eval_dataset.csv\",) #load eval dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_rag_tests(eval_dataset = eval_dataset,\n",
    "                llm = llm,\n",
    "                knowledge_index = vectorstore,\n",
    "                output_file = \"test_generated_ans.json\",\n",
    "                reranker = None,\n",
    "                verbose=True,\n",
    "                test_settings = MODEL_NAME,\n",
    "                batch_size = 5\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get access to test_generated_ans.json** : mc cp s3/projet-llm-insee-open-data/data/eval_data/test_generated_ans.json ./src/data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write an Evaluation prompt for a Critique LLM.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATION_PROMPT = \"\"\"\n",
    "<s><|user|>\n",
    "###Description de la tâche :\n",
    "Une instruction (pouvant inclure une Entrée à l'intérieur), une réponse à évaluer, une réponse de référence qui obtient un score de 5, et une grille de notation représentant un critère d'évaluation sont fournis.\n",
    "\n",
    "###L'instruction à évaluer :\n",
    "{instruction}\n",
    "\n",
    "###Réponse à évaluer :\n",
    "{response}\n",
    "\n",
    "###Réponse de référence (Score 5) :\n",
    "{reference_answer}\n",
    "\n",
    "###Grille de notation :\n",
    "[La réponse est-elle correcte, précise et factuelle par rapport à la réponse de référence ?]\n",
    "Score 1 : La réponse est complètement incorrecte, imprécise et/ou non factuelle.\n",
    "Score 2 : La réponse est principalement incorrecte, imprécise et/ou non factuelle.\n",
    "Score 3 : La réponse est quelque peu correcte, précise et/ou factuelle.\n",
    "Score 4 : La réponse est principalement correcte, précise et factuelle.\n",
    "Score 5 : La réponse est complètement correcte, précise et factuelle.\n",
    "\n",
    "1. Rédigez un feedback détaillé évaluant la qualité de la réponse strictement en fonction de la grille de notation donnée, sans évaluation générale.\n",
    "2. Après avoir rédigé un feedback, attribuez un score qui est un entier entre 1 et 5. Vous devez vous référer à la grille de notation.\n",
    "3. Le format de sortie devrait ressembler à ce qui suit : \"Feedback: {{écrire un feedback pour le critère}} [RESULTAT] {{un nombre entier entre 1 et 5}}\"\n",
    "4. Veuillez ne pas générer d'autres ouvertures, fermetures et explications. Assurez-vous d'inclure la balise [RESULTAT] dans votre sortie.\n",
    "<|end|>\n",
    "<|assistant|>\n",
    "Feedback: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import SystemMessage\n",
    "\n",
    "\n",
    "evaluation_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"Tu es un modèle de langue évaluateur juste\"),\n",
    "        HumanMessagePromptTemplate.from_template(EVALUATION_PROMPT),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['instruction', 'reference_answer', 'response'] messages=[SystemMessage(content='Tu es un modèle de langage évaluateur juste'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['instruction', 'reference_answer', 'response'], template='\\n<s><|user|>\\n###Description de la tâche :\\nUne instruction (pouvant inclure une Entrée à l\\'intérieur), une réponse à évaluer, une réponse de référence qui obtient un score de 5, et une grille de notation représentant un critère d\\'évaluation sont fournis.\\n1. Rédigez un feedback détaillé évaluant la qualité de la réponse strictement en fonction de la grille de notation donnée, sans évaluation générale.\\n2. Après avoir rédigé un feedback, attribuez un score qui est un entier entre 1 et 5. Vous devez vous référer à la grille de notation.\\n3. Le format de sortie devrait ressembler à ce qui suit : \"Feedback: {{écrire un feedback pour le critère}} [RESULTAT] {{un nombre entier entre 1 et 5}}\"\\n4. Veuillez ne pas générer d\\'autres ouvertures, fermetures et explications. Assurez-vous d\\'inclure [RESULTAT] dans votre sortie.\\n\\n###L\\'instruction à évaluer :\\n{instruction}\\n\\n###Réponse à évaluer :\\n{response}\\n\\n###Réponse de référence (Score 5) :\\n{reference_answer}\\n\\n###Grille de notation :\\n[La réponse est-elle correcte, précise et factuelle par rapport à la réponse de référence ?]\\nScore 1 : La réponse est complètement incorrecte, imprécise et/ou non factuelle.\\nScore 2 : La réponse est principalement incorrecte, imprécise et/ou non factuelle.\\nScore 3 : La réponse est quelque peu correcte, précise et/ou factuelle.\\nScore 4 : La réponse est principalement correcte, précise et factuelle.\\nScore 5 : La réponse est complètement correcte, précise et factuelle.\\n<|end|>\\n<|assistant|>\\n###Feedback :'))]\n"
     ]
    }
   ],
   "source": [
    "print(evaluation_prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate the generated answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assess the generated answer by our RAG (using Mistral 8b), we use the open source alternative of GPT4 evaluation called Prometheus-13b-v1.0 part of the prometheus-eval LLM family. It has been fined tune on 100K feedback messages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5dcfc9c0aa24849ab2cb723e35f6252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a988067da8d04429a7074b28765630e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1de06d20db049c5a1d2b6177aebe9a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c5ec86b9245424a9db3c825ba55e69e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a39146fee4f4e5288aca5b9b079f114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/568 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd6ef3fc7b574fe4ad903c43a63ab396",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_phi3.py:   0%|          | 0.00/73.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-128k-instruct:\n",
      "- modeling_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attenton` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb883e8fbedb4e038b2455662b1ecde5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/16.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d469ff54d063416c90b9723ad7d6b3b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3603eefc4dca406a9efc16e7026c8837",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50cdbe180f77406b95cfe795576c6131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58bb70e014ed475b82cccd66c988ab7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "911f818dfb3f4a45a336fd5bd9733214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/172 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, BitsAndBytesConfig\n",
    "\n",
    "#teacher_name = \"prometheus-eval/prometheus-7b-v2.0\"\n",
    "teacher_name = \"microsoft/Phi-3-mini-128k-instruct\" \n",
    "\n",
    "#load LLM config \n",
    "teacher_config = AutoConfig.from_pretrained(teacher_name, trust_remote_code=True)\n",
    "\n",
    "#load quantization config \n",
    "teacher_quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=\"float16\",\n",
    "        bnb_4bit_use_double_quant=False,\n",
    "    )\n",
    "#load llm tokenizer \n",
    "teacher_tokenizer = AutoTokenizer.from_pretrained(teacher_name, use_fast=True, device_map='auto')\n",
    "\n",
    "#load llm \n",
    "teacher_model = AutoModelForCausalLM.from_pretrained(teacher_name,\n",
    "        config=teacher_config,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config = teacher_quantization_config ,\n",
    "        trust_remote_code=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a pipeline for Evaluator Model \n",
    "from transformers import pipeline\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "pipeline_HF = pipeline(task=\"text-generation\", # TextGenerationPipeline HF pipeline\n",
    "                model=teacher_model, \n",
    "                tokenizer=teacher_tokenizer,\n",
    "                temperature=0.2, \n",
    "                return_full_text=False, \n",
    "                device_map=\"auto\",\n",
    "                do_sample=True,\n",
    "            )\n",
    "# Create a LangChain Runnable pipeline \n",
    "evaluator_model = HuggingFacePipeline(pipeline=pipeline_HF, model_kwargs={\"max_length\": 4000})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from tqdm import tqdm \n",
    "import json \n",
    "import random\n",
    "\n",
    "generation_args = {\n",
    "    \"return_full_text\": False,\n",
    "    \"do_sample\": True,\n",
    "    'max_length': 10000\n",
    "}\n",
    "\n",
    "def evaluate_answers(\n",
    "    answer_path: str,\n",
    "    eval_chat_model : pipeline,\n",
    "    evaluator_name: str,\n",
    "    evaluation_prompt_template: str, \n",
    "    pipeline_args : dict = generation_args\n",
    ") -> None:\n",
    "    \"\"\"Evaluates generated answers. Modifies the given answer file in place for better checkpointing.\"\"\"\n",
    "    answers = []\n",
    "    if os.path.isfile(answer_path):  # load previous generations if they exist\n",
    "        answers = json.load(open(answer_path, \"r\"))\n",
    "\n",
    "    i = random.randint(0, len(answers))\n",
    "    experiment = answers[i]\n",
    "\n",
    "    for experiment in tqdm(answers):\n",
    "        if f\"eval_score_{evaluator_name}\" in experiment:\n",
    "            continue\n",
    "\n",
    "        eval_prompt = evaluation_prompt_template.format(\n",
    "            instruction=experiment[\"question\"],\n",
    "            response=experiment[\"generated_answer\"],\n",
    "            reference_answer=experiment[\"true_answer\"],\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            eval_result = eval_chat_model(eval_prompt,**pipeline_args)\n",
    "            feedback, score = eval_result[0][\"generated_text\"].split(\"[RESULTAT]\")\n",
    "        except Exception as e:\n",
    "            print('Error:', e)\n",
    "            print(eval_result)\n",
    "            continue\n",
    "        \"\"\"\n",
    "        print(\"feedback : \", feedback)\n",
    "        print(\"score : \", score)\n",
    "        \"\"\"\n",
    "        experiment[f\"eval_score_{evaluator_name}\"] = score\n",
    "        experiment[f\"eval_feedback_{evaluator_name}\"] = feedback\n",
    "\n",
    "        with open(answer_path, \"w\") as f:\n",
    "            json.dump(answers, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 57/77 [00:00<00:00, 92.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: not enough values to unpack (expected 2, got 1)\n",
      "[{'generated_text': ''}]\n",
      "Error: Input length of input_ids is 2048, but `max_length` is set to 2048. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.\n",
      "[{'generated_text': ''}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [03:04<00:00,  2.40s/it]\n"
     ]
    }
   ],
   "source": [
    "evaluate_answers(\n",
    "        answer_path = \"/home/onyxia/work/llm-open-data-insee/src/data/test_generated_ans.json\", \n",
    "        eval_chat_model = pipeline_HF,\n",
    "        evaluator_name= teacher_name.replace(\"/\",\"-\"),\n",
    "        evaluation_prompt_template = EVALUATION_PROMPT, \n",
    "        pipeline_args = generation_args\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**inspect the results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "outputs = []\n",
    "for file in glob.glob(\"/home/onyxia/work/llm-open-data-insee/src/data/*.json\"):\n",
    "    output = pd.DataFrame(json.load(open(file, \"r\")))\n",
    "    output[\"settings\"] = file\n",
    "    outputs.append(output)\n",
    "result = pd.concat(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"eval_score_microsoft-Phi-3-mini-128k-instruct\"] = result[\"eval_score_microsoft-Phi-3-mini-128k-instruct\"] .apply(lambda x: int(x) if isinstance(x, str) else 1)\n",
    "result[\"eval_score_microsoft-Phi-3-mini-128k-instruct\"]  = (result[\"eval_score_microsoft-Phi-3-mini-128k-instruct\"] - 1 )/ 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "settings\n",
       "/home/onyxia/work/llm-open-data-insee/src/data/test_generated_ans.json    0.282468\n",
       "Name: eval_score_microsoft-Phi-3-mini-128k-instruct, dtype: float64"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_scores = result.groupby(\"settings\")[\"eval_score_microsoft-Phi-3-mini-128k-instruct\"].mean()\n",
    "average_scores.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>true_answer</th>\n",
       "      <th>source_doc</th>\n",
       "      <th>generated_answer</th>\n",
       "      <th>retrieved_docs</th>\n",
       "      <th>test_settings</th>\n",
       "      <th>eval_score_microsoft-Phi-3-mini-128k-instruct</th>\n",
       "      <th>eval_feedback_microsoft-Phi-3-mini-128k-instruct</th>\n",
       "      <th>settings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Quelle était la population du Territoire de la...</td>\n",
       "      <td>La population du Territoire de la Côte Ouest é...</td>\n",
       "      <td>https://www.insee.fr/fr/statistiques/1293858</td>\n",
       "      <td>La population du Territoire de la Côte Ouest n...</td>\n",
       "      <td>[. Entre 2008 et 2013, le rythme annuel de pro...</td>\n",
       "      <td>mistralai/Mistral-7B-Instruct-v0.2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>\\n\\nLa réponse fournie n'est pas correcte, pré...</td>\n",
       "      <td>/home/onyxia/work/llm-open-data-insee/src/data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quelles mesures ont été mises en place pour pr...</td>\n",
       "      <td>Des mesures telles que la loi Génisson (2001),...</td>\n",
       "      <td>https://www.insee.fr/fr/statistiques/1285800</td>\n",
       "      <td>Dans le contexte fourni, plusieurs mesures ont...</td>\n",
       "      <td>[. Dans ce cadre, en juillet 2014, un premier ...</td>\n",
       "      <td>mistralai/Mistral-7B-Instruct-v0.2</td>\n",
       "      <td>0.50</td>\n",
       "      <td>\\n\\nLa réponse fournie présente une structure ...</td>\n",
       "      <td>/home/onyxia/work/llm-open-data-insee/src/data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Quand Mayotte a-t-elle commencé à utiliser la ...</td>\n",
       "      <td>Mayotte a commencé à utiliser la méthode commu...</td>\n",
       "      <td>https://www.insee.fr/fr/statistiques/7728787</td>\n",
       "      <td>Mayotte a commencé à utiliser la méthode commu...</td>\n",
       "      <td>[Mayotte intègre désormais la méthode commune ...</td>\n",
       "      <td>mistralai/Mistral-7B-Instruct-v0.2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>\\n\\nLa réponse fournie est directe et correspo...</td>\n",
       "      <td>/home/onyxia/work/llm-open-data-insee/src/data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Combien d'emplois sont considérés comme liés a...</td>\n",
       "      <td>Pour les secteurs d'activités partiellement to...</td>\n",
       "      <td>https://www.insee.fr/fr/statistiques/1283777</td>\n",
       "      <td>Dans le contexte fourni, les emplois liés au t...</td>\n",
       "      <td>[. L’emploi touristique englobe les activités,...</td>\n",
       "      <td>mistralai/Mistral-7B-Instruct-v0.2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>\\n\\nLa réponse fournie ne correspond pas à la ...</td>\n",
       "      <td>/home/onyxia/work/llm-open-data-insee/src/data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Quelle est la différence de nombre de décès en...</td>\n",
       "      <td>La différence de nombre de décès entre le 1er ...</td>\n",
       "      <td>https://www.insee.fr/fr/statistiques/5055298</td>\n",
       "      <td>La région de Bourgogne-Franche-Comté a enregis...</td>\n",
       "      <td>[. C’est particulièrement le cas pour avril 20...</td>\n",
       "      <td>mistralai/Mistral-7B-Instruct-v0.2</td>\n",
       "      <td>0.50</td>\n",
       "      <td>\\n\\nLa réponse donnée mentionne correctement l...</td>\n",
       "      <td>/home/onyxia/work/llm-open-data-insee/src/data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Quelle est la différence de rémunération moyen...</td>\n",
       "      <td>La différence de rémunération moyenne entre le...</td>\n",
       "      <td>https://www.insee.fr/fr/statistiques/1372799</td>\n",
       "      <td>Dans le contexte fourni, les documents indique...</td>\n",
       "      <td>[En France, les accords collectifs, négociés a...</td>\n",
       "      <td>mistralai/Mistral-7B-Instruct-v0.2</td>\n",
       "      <td>0.50</td>\n",
       "      <td>\\n\\nLa réponse fournie ne mentionne pas la dif...</td>\n",
       "      <td>/home/onyxia/work/llm-open-data-insee/src/data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Quelle est la progression de l'emploi salarié ...</td>\n",
       "      <td>La progression de l'emploi salarié dans le sec...</td>\n",
       "      <td>https://www.insee.fr/fr/statistiques/4277983</td>\n",
       "      <td>Dans le contexte fourni, il n'y a pas de donné...</td>\n",
       "      <td>[. Ce changement de source a pu entraîner une ...</td>\n",
       "      <td>mistralai/Mistral-7B-Instruct-v0.2</td>\n",
       "      <td>0.25</td>\n",
       "      <td>La réponse fournie ne correspond pas à la rép...</td>\n",
       "      <td>/home/onyxia/work/llm-open-data-insee/src/data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Quelle est la tendance démographique de la pop...</td>\n",
       "      <td>La population régionale de Bourgogne-Franche-C...</td>\n",
       "      <td>https://www.insee.fr/fr/statistiques/5369489</td>\n",
       "      <td>La tendance démographique de la population rég...</td>\n",
       "      <td>[Si les tendances démographiques récentes se m...</td>\n",
       "      <td>mistralai/Mistral-7B-Instruct-v0.2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>\\n\\nLa réponse fournie ne correspond pas à la ...</td>\n",
       "      <td>/home/onyxia/work/llm-open-data-insee/src/data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Quels sont les sujets principaux traités dans ...</td>\n",
       "      <td>Les sujets principaux traités dans le rapport ...</td>\n",
       "      <td>https://www.insee.fr/fr/statistiques/2894036</td>\n",
       "      <td>Le rapport annuel \"L'Économie française - Comp...</td>\n",
       "      <td>[L'économie française - Comptes et dossiers Éd...</td>\n",
       "      <td>mistralai/Mistral-7B-Instruct-v0.2</td>\n",
       "      <td>0.25</td>\n",
       "      <td>\\n\\nLa réponse fournie présente une synthèse d...</td>\n",
       "      <td>/home/onyxia/work/llm-open-data-insee/src/data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Quelle industrie a connu une augmentation de +...</td>\n",
       "      <td>L'industrie pharmaceutique a connu une augment...</td>\n",
       "      <td>https://www.insee.fr/fr/information/3357117</td>\n",
       "      <td>Based on the provided context, I cannot direct...</td>\n",
       "      <td>[. Les prix des produits de santé augmentent é...</td>\n",
       "      <td>mistralai/Mistral-7B-Instruct-v0.2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>\\n\\nLa réponse donnée ne correspond pas à la r...</td>\n",
       "      <td>/home/onyxia/work/llm-open-data-insee/src/data...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>77 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question  \\\n",
       "0   Quelle était la population du Territoire de la...   \n",
       "1   Quelles mesures ont été mises en place pour pr...   \n",
       "2   Quand Mayotte a-t-elle commencé à utiliser la ...   \n",
       "3   Combien d'emplois sont considérés comme liés a...   \n",
       "4   Quelle est la différence de nombre de décès en...   \n",
       "..                                                ...   \n",
       "72  Quelle est la différence de rémunération moyen...   \n",
       "73  Quelle est la progression de l'emploi salarié ...   \n",
       "74  Quelle est la tendance démographique de la pop...   \n",
       "75  Quels sont les sujets principaux traités dans ...   \n",
       "76  Quelle industrie a connu une augmentation de +...   \n",
       "\n",
       "                                          true_answer  \\\n",
       "0   La population du Territoire de la Côte Ouest é...   \n",
       "1   Des mesures telles que la loi Génisson (2001),...   \n",
       "2   Mayotte a commencé à utiliser la méthode commu...   \n",
       "3   Pour les secteurs d'activités partiellement to...   \n",
       "4   La différence de nombre de décès entre le 1er ...   \n",
       "..                                                ...   \n",
       "72  La différence de rémunération moyenne entre le...   \n",
       "73  La progression de l'emploi salarié dans le sec...   \n",
       "74  La population régionale de Bourgogne-Franche-C...   \n",
       "75  Les sujets principaux traités dans le rapport ...   \n",
       "76  L'industrie pharmaceutique a connu une augment...   \n",
       "\n",
       "                                      source_doc  \\\n",
       "0   https://www.insee.fr/fr/statistiques/1293858   \n",
       "1   https://www.insee.fr/fr/statistiques/1285800   \n",
       "2   https://www.insee.fr/fr/statistiques/7728787   \n",
       "3   https://www.insee.fr/fr/statistiques/1283777   \n",
       "4   https://www.insee.fr/fr/statistiques/5055298   \n",
       "..                                           ...   \n",
       "72  https://www.insee.fr/fr/statistiques/1372799   \n",
       "73  https://www.insee.fr/fr/statistiques/4277983   \n",
       "74  https://www.insee.fr/fr/statistiques/5369489   \n",
       "75  https://www.insee.fr/fr/statistiques/2894036   \n",
       "76   https://www.insee.fr/fr/information/3357117   \n",
       "\n",
       "                                     generated_answer  \\\n",
       "0   La population du Territoire de la Côte Ouest n...   \n",
       "1   Dans le contexte fourni, plusieurs mesures ont...   \n",
       "2   Mayotte a commencé à utiliser la méthode commu...   \n",
       "3   Dans le contexte fourni, les emplois liés au t...   \n",
       "4   La région de Bourgogne-Franche-Comté a enregis...   \n",
       "..                                                ...   \n",
       "72  Dans le contexte fourni, les documents indique...   \n",
       "73  Dans le contexte fourni, il n'y a pas de donné...   \n",
       "74  La tendance démographique de la population rég...   \n",
       "75  Le rapport annuel \"L'Économie française - Comp...   \n",
       "76  Based on the provided context, I cannot direct...   \n",
       "\n",
       "                                       retrieved_docs  \\\n",
       "0   [. Entre 2008 et 2013, le rythme annuel de pro...   \n",
       "1   [. Dans ce cadre, en juillet 2014, un premier ...   \n",
       "2   [Mayotte intègre désormais la méthode commune ...   \n",
       "3   [. L’emploi touristique englobe les activités,...   \n",
       "4   [. C’est particulièrement le cas pour avril 20...   \n",
       "..                                                ...   \n",
       "72  [En France, les accords collectifs, négociés a...   \n",
       "73  [. Ce changement de source a pu entraîner une ...   \n",
       "74  [Si les tendances démographiques récentes se m...   \n",
       "75  [L'économie française - Comptes et dossiers Éd...   \n",
       "76  [. Les prix des produits de santé augmentent é...   \n",
       "\n",
       "                         test_settings  \\\n",
       "0   mistralai/Mistral-7B-Instruct-v0.2   \n",
       "1   mistralai/Mistral-7B-Instruct-v0.2   \n",
       "2   mistralai/Mistral-7B-Instruct-v0.2   \n",
       "3   mistralai/Mistral-7B-Instruct-v0.2   \n",
       "4   mistralai/Mistral-7B-Instruct-v0.2   \n",
       "..                                 ...   \n",
       "72  mistralai/Mistral-7B-Instruct-v0.2   \n",
       "73  mistralai/Mistral-7B-Instruct-v0.2   \n",
       "74  mistralai/Mistral-7B-Instruct-v0.2   \n",
       "75  mistralai/Mistral-7B-Instruct-v0.2   \n",
       "76  mistralai/Mistral-7B-Instruct-v0.2   \n",
       "\n",
       "    eval_score_microsoft-Phi-3-mini-128k-instruct  \\\n",
       "0                                            0.00   \n",
       "1                                            0.50   \n",
       "2                                            1.00   \n",
       "3                                            0.00   \n",
       "4                                            0.50   \n",
       "..                                            ...   \n",
       "72                                           0.50   \n",
       "73                                           0.25   \n",
       "74                                           0.00   \n",
       "75                                           0.25   \n",
       "76                                           0.00   \n",
       "\n",
       "     eval_feedback_microsoft-Phi-3-mini-128k-instruct  \\\n",
       "0   \\n\\nLa réponse fournie n'est pas correcte, pré...   \n",
       "1   \\n\\nLa réponse fournie présente une structure ...   \n",
       "2   \\n\\nLa réponse fournie est directe et correspo...   \n",
       "3   \\n\\nLa réponse fournie ne correspond pas à la ...   \n",
       "4   \\n\\nLa réponse donnée mentionne correctement l...   \n",
       "..                                                ...   \n",
       "72  \\n\\nLa réponse fournie ne mentionne pas la dif...   \n",
       "73   La réponse fournie ne correspond pas à la rép...   \n",
       "74  \\n\\nLa réponse fournie ne correspond pas à la ...   \n",
       "75  \\n\\nLa réponse fournie présente une synthèse d...   \n",
       "76  \\n\\nLa réponse donnée ne correspond pas à la r...   \n",
       "\n",
       "                                             settings  \n",
       "0   /home/onyxia/work/llm-open-data-insee/src/data...  \n",
       "1   /home/onyxia/work/llm-open-data-insee/src/data...  \n",
       "2   /home/onyxia/work/llm-open-data-insee/src/data...  \n",
       "3   /home/onyxia/work/llm-open-data-insee/src/data...  \n",
       "4   /home/onyxia/work/llm-open-data-insee/src/data...  \n",
       "..                                                ...  \n",
       "72  /home/onyxia/work/llm-open-data-insee/src/data...  \n",
       "73  /home/onyxia/work/llm-open-data-insee/src/data...  \n",
       "74  /home/onyxia/work/llm-open-data-insee/src/data...  \n",
       "75  /home/onyxia/work/llm-open-data-insee/src/data...  \n",
       "76  /home/onyxia/work/llm-open-data-insee/src/data...  \n",
       "\n",
       "[77 rows x 9 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal : This notebook aims to provide a code allowing to evaluate the generated answer of the RAG pipeline based on a reference dataset. It is inspired by RAGAS implementation of evaluation framework and relies on recent strong instruction fine tuned LLM with a long context size in order to evaluate the results. \n",
    "\n",
    "The following code exploit the projet function to load models, build chains and generate answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Corpus data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mc cp -r s3/projet-llm-insee-open-data/data/chroma_database/chroma_db/  ../data/chroma_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Q&A test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mc cp s3/projet-llm-insee-open-data/data/eval_data/eval_retrieval/q_and_a_scored_filtered_Phi-3-mini-128k-instruct.csv ../data/q_and_a_ref_retrieval_evaluation_Phi-3-mini-128k-instruct.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import (\n",
    "    build_vector_database\n",
    ")\n",
    "from db_building import build_database_from_dataframe, reload_database_from_local_dir\n",
    "from evaluation.utils import choosing_reranker_test, RetrievalConfiguration, build_chain_reranker_test\n",
    "from model_building import build_llm_model\n",
    "import os \n",
    "from transformers import pipeline\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils function \n",
    "def format_docs(docs: list):\n",
    "    return \"\\n\\n\".join([f\"Doc {i + 1}:\\nTitle: {doc.metadata[\"title\"]}\\nContent:\\n{doc.page_content}\" for i, doc in enumerate(docs)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define an evaluator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-30 16:06:38,165 - INFO - Model meta-llama/Meta-Llama-3.1-8B-Instruct found in local cache. \n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0108a1c5efd14ed096cba6723e6ee175",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set Validator chain in chainlit session\n",
    "EVALUATION_PROMPT = \"\"\"\n",
    "Description de la tâche :\n",
    "Vous devez évaluer la pertinence de la réponse générée par une pipeline de RAG par rapport à la question posée, en tenant compte de la réponse de référence fournie.\n",
    "\n",
    "Instruction à évaluer :\n",
    "La réponse est-elle pertinente, précise et factuelle par rapport à la question posée et la réponse de référence ?\n",
    "\n",
    "Réponse à évaluer :\n",
    "{response}\n",
    "\n",
    "Question posée :\n",
    "{question}\n",
    "\n",
    "Réponse de référence (Score 5) :\n",
    "{reference_answer}\n",
    "\n",
    "Grille de notation :\n",
    "Score 1 : La réponse est hors sujet, non pertinente, et/ou factuellement incorrecte.\n",
    "Score 2 : La réponse est principalement hors sujet, peu pertinente, et/ou présente des erreurs factuelles importantes.\n",
    "Score 3 : La réponse est quelque peu pertinente, avec des éléments corrects mais manquant de précision ou de complétude.\n",
    "Score 4 : La réponse est principalement pertinente, précise et factuelle, mais pourrait manquer de détails mineurs.\n",
    "Score 5 : La réponse est totalement pertinente, précise, et factuelle par rapport à la question et à la réponse de référence.\n",
    "\n",
    "Instructions :\n",
    "\n",
    "    Rédigez un feedback détaillé évaluant la pertinence de la réponse en fonction de la grille de notation donnée, sans évaluation générale.\n",
    "    Après avoir rédigé un feedback, attribuez un score qui est un entier entre 1 et 5, en vous référant à la grille de notation.\n",
    "    Le format de sortie devrait ressembler à ce qui suit : \"Feedback : {{écrire un feedback pour le critère}} [RESULTAT] {{un nombre entier entre 1 et 5}}\"\n",
    "    Veuillez ne pas générer d'autres ouvertures, fermetures et explications. Assurez-vous d'inclure la balise [RESULTAT] dans votre sortie.\n",
    "\n",
    "Feedback :\"\"\"\n",
    "\n",
    "EVAL_TEMPLATE = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": EVALUATION_PROMPT ,\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "eval_llm, eval_tokenizer = build_llm_model(\n",
    "        model_name=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "        quantization_config=True,\n",
    "        config=True,\n",
    "        token=os.getenv(\"HF_TOKEN\"),\n",
    "        streaming=False,\n",
    "        generation_args={\"max_new_tokens\": 1000, \"return_full_text\": False, \"do_sample\": False, \"temperature\": None, \"top_p\" : None},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-30 13:38:47,569 - INFO - Model meta-llama/Meta-Llama-3.1-8B-Instruct found in local cache. \n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "316ebcfc55a54341bca044a231254270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "retrieval_config =  RetrievalConfiguration(\n",
    "        name=f'test',\n",
    "        database=\"chromadb\",\n",
    "        collection=\"insee_data\",\n",
    "        database_path=\"\",\n",
    "        embedding_model_name=\"OrdalieTech/Solon-embeddings-large-0.1\",\n",
    "        reranker_type=\"Cross-encoder\",\n",
    "        reranker_name=\"BAAI/bge-reranker-v2-m3\",\n",
    "        rerank_k=50,\n",
    "        k_values=[1, 2, 3, 5, 10, 15, 20, 25, 30, 35, 45, 50]\n",
    "    )\n",
    "\n",
    "nb_input_docs = 10\n",
    "\n",
    "# Set Validator chain in chainlit session\n",
    "llm, tokenizer = build_llm_model(\n",
    "        model_name=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "        quantization_config=True,\n",
    "        config=True,\n",
    "        token=os.getenv(\"HF_TOKEN\"),\n",
    "        streaming=False,\n",
    "        generation_args={\"max_new_tokens\": 2000, \"return_full_text\": False, \"do_sample\": True, \"temperature\": 0.2},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chatbot configuration\n",
    "CHATBOT_INSTRUCTION = \"\"\"\n",
    "En utilisant UNIQUEMENT les informations présentes dans le contexte, réponds de manière argumentée à la question posée.\n",
    "La réponse doit être développée et citer ses sources.\n",
    "\n",
    "Si tu ne peux pas induire ta réponse du contexte, ne réponds pas.\n",
    "\"\"\"\n",
    "\n",
    "USER_INSTRUCTION = \"\"\"Voici le contexte sur lequel tu dois baser ta réponse :\n",
    "Contexte:\n",
    "{context}\n",
    "---\n",
    "Voici la question à laquelle tu dois répondre :\n",
    "Question: {question}\"\"\"\n",
    "\n",
    "CHATBOT_TEMPLATE = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\"Tu es un assistant spécialisé dans la statistique publique.\n",
    "    Tu réponds à des questions concernant les données de l'Insee, l'institut national statistique Français.\n",
    "    Réponds en FRANCAIS UNIQUEMENT.\"\"\",\n",
    "    },\n",
    "    {\"role\": \"assistant\", \"content\": CHATBOT_INSTRUCTION},\n",
    "    {\"role\": \"user\", \"content\": USER_INSTRUCTION},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feedback_score(eval_response):\n",
    "    try:\n",
    "        feedback , score = eval_response.split(\"[RESULTAT]\")\n",
    "        feedback = feedback.split(\"Feedback :\")[-1].strip()\n",
    "        score = int(score.strip()) \n",
    "    except Exception as e:\n",
    "        feedback = None\n",
    "        score = None\n",
    "    return feedback, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Initialize the vector database and reranker chain\n",
    "vector_db = reload_database_from_local_dir(\n",
    "            embed_model_name=retrieval_config.embedding_model_name,\n",
    "            collection_name=\"insee_data\",\n",
    "            persist_directory=\"../data/chroma_db\",\n",
    "            embed_device=\"cuda\",\n",
    "            config=None,\n",
    "        )\n",
    "\n",
    "# Load evaluation data\n",
    "df_test = pd.read_csv(\"../data/q_and_a_ref_retrieval_evaluation_Phi-3-mini-128k-instruct.csv\")\n",
    "\n",
    "# Extract queries\n",
    "queries = df_test[\"question\"]\n",
    "\n",
    "# Build reranker chain\n",
    "reranker = build_chain_reranker_test(retrieval_config)\n",
    "\n",
    "# Embed queries\n",
    "embedded_queries = vector_db.embeddings.embed_documents(queries)\n",
    "\n",
    "# Initialize results dictionary\n",
    "results = {\n",
    "    \"question\": [],\n",
    "    \"ground_truth_answer\": [],\n",
    "    \"generated_answer\": [],\n",
    "    \"evaluation_score\": [],\n",
    "    \"evaluation_explanation\": []\n",
    "}\n",
    "\n",
    "# Loop through the test data\n",
    "for i, row in tqdm(df_test.iterrows(), total=len(df_test)):\n",
    "\n",
    "    # Base retriever\n",
    "    retrieved_docs = vector_db.similarity_search_by_vector(embedding=embedded_queries[i], k=max(retrieval_config.k_values))\n",
    "    \n",
    "    # Reranker\n",
    "    retrieved_docs = reranker.invoke({\"documents\": retrieved_docs, \"query\": row[\"question\"]})\n",
    "    \n",
    "    # Keep top nb_input_docs\n",
    "    retrieved_docs = retrieved_docs[:nb_input_docs]\n",
    "\n",
    "    # Format documents for input\n",
    "    input_documents = format_docs(retrieved_docs)\n",
    "\n",
    "    # Prepare RAG prompt template\n",
    "    RAG_PROMPT_TEMPLATE = tokenizer.apply_chat_template(CHATBOT_TEMPLATE, tokenize=False, add_generation_prompt=True)\n",
    "    prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=RAG_PROMPT_TEMPLATE)\n",
    "\n",
    "    # Create the final prompt for generation\n",
    "    final_prompt = prompt.format(context=input_documents, question=row[\"question\"])\n",
    "\n",
    "    # Generate the response using the LLM\n",
    "    response = llm.invoke(final_prompt)\n",
    "\n",
    "    # Store results\n",
    "    results[\"question\"].append(row[\"question\"])\n",
    "    results[\"ground_truth_answer\"].append(row[\"answer\"])\n",
    "    results[\"generated_answer\"].append(response)\n",
    "\n",
    "    #### Evaluation ####\n",
    "    \n",
    "    # Prepare evaluation prompt template\n",
    "    EVAL_PROMPT_TEMPLATE = eval_tokenizer.apply_chat_template(EVAL_TEMPLATE, tokenize=False, add_generation_prompt=True)\n",
    "    eval_prompt = PromptTemplate(input_variables=[\"response\", \"question\", \"reference_answer\"], template=EVAL_PROMPT_TEMPLATE)\n",
    "\n",
    "    # Create the final prompt for evaluation\n",
    "    eval_final_prompt = eval_prompt.format(response=response, question=row[\"question\"], reference_answer=row[\"answer\"])\n",
    "\n",
    "    # Invoke the evaluation LLM\n",
    "    eval_response = eval_llm.invoke(eval_final_prompt)\n",
    "    # Extract evaluation score and explanation\n",
    "\n",
    "    evaluation_explanation, evaluation_score = extract_feedback_score(eval_response)\n",
    "\n",
    "    # Store evaluation results\n",
    "    results[\"evaluation_score\"].append(evaluation_score)\n",
    "    results[\"evaluation_explanation\"].append(evaluation_explanation)\n",
    "\n",
    "# Optionally save the results\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results.to_csv(\"../data/evaluation_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "1it [00:03,  3.76s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "2it [00:09,  4.71s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "3it [00:12,  4.02s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "4it [00:15,  3.78s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "5it [00:18,  3.43s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "6it [00:21,  3.10s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "7it [00:24,  3.20s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "8it [00:30,  4.08s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "9it [00:34,  4.23s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "10it [00:35,  3.21s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "11it [00:40,  3.59s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "12it [00:41,  2.77s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "13it [00:44,  3.05s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "14it [00:47,  3.03s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "15it [00:52,  3.38s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "16it [00:52,  2.64s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "17it [00:56,  2.91s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "18it [00:59,  2.98s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "19it [01:02,  2.82s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "20it [01:05,  2.92s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "21it [01:09,  3.44s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "22it [01:13,  3.38s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "23it [01:17,  3.80s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "24it [01:21,  3.61s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "25it [01:23,  3.27s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "26it [01:28,  3.81s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "27it [01:31,  3.64s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "28it [01:34,  3.21s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "29it [01:36,  3.00s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "30it [01:39,  2.96s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "31it [01:41,  2.59s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "32it [01:46,  3.50s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "33it [01:50,  3.47s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "34it [01:55,  3.86s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "35it [02:00,  4.20s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "36it [02:05,  4.59s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "37it [02:09,  4.40s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "38it [02:13,  4.27s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "39it [02:16,  3.97s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "40it [02:20,  3.89s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "41it [02:23,  3.54s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "42it [02:24,  2.74s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "43it [02:27,  3.05s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "44it [02:32,  3.48s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "45it [02:37,  3.93s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "46it [02:40,  3.73s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "47it [02:44,  3.82s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "48it [02:47,  3.68s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "49it [02:51,  3.60s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "50it [02:54,  3.58s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "51it [02:55,  2.78s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "52it [02:58,  2.68s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "53it [03:01,  2.87s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "54it [03:06,  3.37s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "55it [03:09,  3.37s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "56it [03:12,  3.24s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "57it [03:16,  3.53s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "58it [03:20,  3.62s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "59it [03:23,  3.39s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "60it [03:32,  5.08s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "61it [03:36,  4.78s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "62it [03:39,  4.28s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "63it [03:43,  4.08s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "64it [03:48,  4.38s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "65it [03:51,  4.13s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "66it [03:55,  3.89s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "67it [03:57,  3.54s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "68it [04:01,  3.67s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "69it [04:02,  2.83s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "70it [04:06,  3.11s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "71it [04:09,  3.06s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "72it [04:12,  3.01s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "73it [04:14,  2.91s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "74it [04:18,  3.22s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "75it [04:21,  3.10s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "76it [04:23,  2.67s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "77it [04:25,  2.64s/it]/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "78it [04:28,  3.44s/it]\n"
     ]
    }
   ],
   "source": [
    "for i, row in tqdm(df_results.iterrows()):\n",
    "    question, ref_answer, generated_answer = row[\"question\"],  row[\"ground_truth_answer\"], row[\"generated_answer\"]\n",
    "    \n",
    "    # Prepare evaluation prompt template\n",
    "    EVAL_PROMPT_TEMPLATE = eval_tokenizer.apply_chat_template(EVAL_TEMPLATE, tokenize=False, add_generation_prompt=True)\n",
    "    eval_prompt = PromptTemplate(input_variables=[\"response\", \"question\", \"reference_answer\"], template=EVAL_PROMPT_TEMPLATE)\n",
    "\n",
    "    # Create the final prompt for evaluation\n",
    "    eval_final_prompt = eval_prompt.format(response=generated_answer, question=question, reference_answer=ref_answer)\n",
    "\n",
    "    # Invoke the evaluation LLM\n",
    "    eval_response = eval_llm.invoke(eval_final_prompt)\n",
    "    # Extract evaluation score and explanation\n",
    "    evaluation_explanation, evaluation_score = extract_feedback_score(eval_response)\n",
    "\n",
    "    df_results.at[i, 'evaluation_score'] = evaluation_score\n",
    "    df_results.at[i, 'evaluation_explanation'] = evaluation_explanation        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score (over Test Dataset) : 3.62\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(f\"Average score (over Test Dataset) : {np.round(df_results['evaluation_score'].mean(),2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming df_results is your DataFrame and it contains an \"evaluation_score\" column.\n",
    "# You can customize the bins, colors, etc., as needed.\n",
    "\n",
    "plt.figure(figsize=(10, 6))  # Set the figure size\n",
    "df_results[\"evaluation_score\"].hist(bins=20, color='skyblue', edgecolor='black')\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title(\"Score Distribution\", fontsize=16)\n",
    "plt.xlabel(\"Evaluation Score\", fontsize=14)\n",
    "plt.ylabel(\"Frequency\", fontsize=14)\n",
    "\n",
    "# Customize the grid\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
