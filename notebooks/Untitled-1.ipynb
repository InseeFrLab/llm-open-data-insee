{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bac à sable RAG\n",
    "\n",
    "Chargement de l'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "import s3fs\n",
    "from langchain.schema.runnable.config import RunnableConfig\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "from src.chain_building.build_chain import build_chain\n",
    "from src.chain_building.build_chain_validator import build_chain_validator\n",
    "from src.config import CHATBOT_TEMPLATE, EMB_MODEL_NAME\n",
    "from src.db_building import load_retriever, load_vector_database\n",
    "from src.model_building import build_llm_model\n",
    "\n",
    "# Logging configuration\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %I:%M:%S %p\",\n",
    "    level=logging.DEBUG,\n",
    ")\n",
    "\n",
    "# Remote file configuration\n",
    "os.environ[\"MLFLOW_TRACKING_URI\"] = \"https://projet-llm-insee-open-data-mlflow.user.lab.sspcloud.fr/\"\n",
    "fs = s3fs.S3FileSystem(client_kwargs={\"endpoint_url\": f\"\"\"https://{os.environ[\"AWS_S3_ENDPOINT\"]}\"\"\"})\n",
    "\n",
    "# PARAMETERS --------------------------------------\n",
    "\n",
    "os.environ[\"UVICORN_TIMEOUT_KEEP_ALIVE\"] = \"0\"\n",
    "\n",
    "model = os.getenv(\"LLM_MODEL_NAME\")\n",
    "CHROMA_DB_LOCAL_DIRECTORY = \"./data/chroma_db\"\n",
    "CLI_MESSAGE_SEPARATOR = f\"{80*'-'} \\n\"\n",
    "quantization = True\n",
    "DEFAULT_MAX_NEW_TOKENS = 10\n",
    "DEFAULT_MODEL_TEMPERATURE = 1\n",
    "embedding = os.getenv(\"EMB_MODEL_NAME\", EMB_MODEL_NAME)\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "LLM_MODEL = os.getenv(\"LLM_MODEL_NAME\", \"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "LLM_MODEL = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "QUANTIZATION = os.getenv(\"QUANTIZATION\", True)\n",
    "MAX_NEW_TOKENS = int(os.getenv(\"MAX_NEW_TOKENS\", DEFAULT_MAX_NEW_TOKENS))\n",
    "MODEL_TEMPERATURE = int(os.getenv(\"MODEL_TEMPERATURE\", DEFAULT_MODEL_TEMPERATURE))\n",
    "RETURN_FULL_TEXT = os.getenv(\"RETURN_FULL_TEXT\", True)\n",
    "DO_SAMPLE = os.getenv(\"DO_SAMPLE\", True)\n",
    "DATABASE_RUN_ID = \"32d4150a14fa40d49b9512e1f3ff9e8c\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ajoute un prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model_building.fetch_llm_model import cache_model_from_hf_hub\n",
    "\n",
    "cache_model_from_hf_hub(\n",
    "    model_id,\n",
    "    s3_bucket=os.environ[\"S3_BUCKET\"],\n",
    "    s3_cache_dir=\"models/hf_hub\",\n",
    "    s3_endpoint=f'https://{os.environ[\"AWS_S3_ENDPOINT\"]}',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    # token=os.getenv(\"HF_TOKEN\"),\n",
    "    use_fast=True,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm, tokenizer = build_llm_model(\n",
    "#             model_name=model_id,\n",
    "#             quantization_config=QUANTIZATION,\n",
    "#             config=True,\n",
    "#             token=os.getenv(\"HF_TOKEN\"),\n",
    "#             streaming=False,\n",
    "#             generation_args={\n",
    "#                 \"max_new_tokens\": 512,\n",
    "#                 \"return_full_text\": RETURN_FULL_TEXT,\n",
    "#                 \"do_sample\": DO_SAMPLE,\n",
    "#                 \"temperature\": MODEL_TEMPERATURE\n",
    "#             },\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = load_vector_database(\n",
    "    filesystem=fs,\n",
    "    database_run_id=DATABASE_RUN_ID,\n",
    "    # hard coded pour le moment\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever, vectorstore = load_retriever(\n",
    "    emb_model_name=embedding,\n",
    "    persist_directory=CHROMA_DB_LOCAL_DIRECTORY,\n",
    "    vectorstore=db,\n",
    "    retriever_params={\"search_type\": \"similarity\", \"search_kwargs\": {\"k\": 30}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(db.get()[\"documents\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On repart de zéro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs: list):\n",
    "    return \"\\n\\n\".join(\n",
    "        [\n",
    "            f\"\"\"\n",
    "            Doc {i + 1}:\\nTitle: {doc.metadata.get(\"Header 1\")}\\n\n",
    "            Source: {doc.metadata.get(\"url\")}\\n\n",
    "            Content:\\n{doc.page_content}\n",
    "            \"\"\"\n",
    "            for i, doc in enumerate(docs)\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_instructions = \"\"\"\n",
    "Tu es un assistant spécialisé dans la statistique publique.\n",
    "Tu réponds à des questions concernant les données de l'Insee, l'institut national statistique Français.\n",
    "\n",
    "Réponds en FRANCAIS UNIQUEMENT. Utilise une mise en forme au format markdown.\n",
    "\n",
    "En utilisant UNIQUEMENT les informations présentes dans le contexte, réponds de manière argumentée à la question posée.\n",
    "\n",
    "La réponse doit être développée et citer ses sources (titre et url de la publication) qui sont référencées à la fin.\n",
    "Cite notamment l'url d'origine de la publication, dans un format markdown.\n",
    "\n",
    "Cite 5 sources maximum.\n",
    "\n",
    "Tu n'es pas obligé d'utiliser les sources les moins pertinentes.\n",
    "\n",
    "Si tu ne peux pas induire ta réponse du contexte, ne réponds pas.\n",
    "\"\"\"\n",
    "\n",
    "question_instructions = \"\"\"\n",
    "Voici le contexte sur lequel tu dois baser ta réponse :\n",
    "Contexte: {context}\n",
    "---\n",
    "\n",
    "Voici la question à laquelle tu dois répondre :\n",
    "Question: {question}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "RAG_PROMPT_TEMPLATE = tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_instructions,\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": question_instructions},\n",
    "    ],\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=RAG_PROMPT_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question1 = \"Quelle est la définition du PIB ?\"\n",
    "question2 = \"Où trouver les taux de chômage?\"\n",
    "\n",
    "documents_retriever_list1 = retriever.invoke(question1)\n",
    "retrieved_docs_as_string1 = format_docs(documents_retriever_list1)\n",
    "documents_retriever_list2 = retriever.invoke(question2)\n",
    "retrieved_docs_as_string2 = format_docs(documents_retriever_list2)\n",
    "\n",
    "prompt_injected1 = prompt.format(context=retrieved_docs_as_string1, question=question1)\n",
    "prompt_injected2 = prompt.format(context=retrieved_docs_as_string2, question=question2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"prompt1.txt\", \"w\") as file:\n",
    "    file.write(prompt_injected1)\n",
    "\n",
    "with open(\"prompt2.txt\", \"w\") as file:\n",
    "    file.write(prompt_injected2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt_injected1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"prompt1.txt\") as file:\n",
    "    prompt_injected1 = file.read()\n",
    "\n",
    "with open(\"prompt2.txt\") as file:\n",
    "    prompt_injected2 = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "LLM_MODEL = model_id\n",
    "MAX_NEW_TOKEN = 8192\n",
    "TEMPERATURE = 0.2\n",
    "REP_PENALTY = 1.1\n",
    "TOP_P = 0.8\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    max_tokens=MAX_NEW_TOKEN,\n",
    "    temperature=TEMPERATURE,\n",
    "    top_p=TOP_P,\n",
    "    repetition_penalty=REP_PENALTY,\n",
    ")\n",
    "\n",
    "llm = LLM(model=LLM_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = llm.generate([prompt_injected1, prompt_injected2], sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = [outputs[i].outputs[0].text for i in range(len(outputs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "\n",
    "\n",
    "printmd(responses[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printmd(responses[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intégration à langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"prompt1.txt\") as file:\n",
    "    prompt_injected1 = file.read()\n",
    "\n",
    "with open(\"prompt2.txt\") as file:\n",
    "    prompt_injected2 = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import VLLM\n",
    "\n",
    "LLM_MODEL = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "MAX_NEW_TOKEN = 8192\n",
    "TEMPERATURE = 0.2\n",
    "REP_PENALTY = 1.1\n",
    "TOP_P = 0.8\n",
    "\n",
    "llm = VLLM(model=LLM_MODEL, max_new_tokens=MAX_NEW_TOKEN, top_p=TOP_P, temperature=TEMPERATURE, rep_penalty=REP_PENALTY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke(prompt_injected1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chaines chainlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_instructions = \"\"\"\n",
    "Tu es un assistant spécialisé dans la statistique publique.\n",
    "Tu réponds à des questions concernant les données de l'Insee, l'institut national statistique Français.\n",
    "\n",
    "Réponds en FRANCAIS UNIQUEMENT. Utilise une mise en forme au format markdown.\n",
    "\n",
    "En utilisant UNIQUEMENT les informations présentes dans le contexte, réponds de manière argumentée à la question posée.\n",
    "\n",
    "La réponse doit être développée et citer ses sources (titre et url de la publication) qui sont référencées à la fin.\n",
    "Cite notamment l'url d'origine de la publication, dans un format markdown.\n",
    "\n",
    "Cite 5 sources maximum.\n",
    "\n",
    "Tu n'es pas obligé d'utiliser les sources les moins pertinentes. \n",
    "\n",
    "Si tu ne peux pas induire ta réponse du contexte, ne réponds pas.\n",
    "\n",
    "Voici le contexte sur lequel tu dois baser ta réponse :\n",
    "Contexte: {context}\n",
    "\"\"\"\n",
    "\n",
    "question_instructions = \"\"\"\n",
    "Voici la question à laquelle tu dois répondre :\n",
    "Question: {question}\n",
    "\n",
    "Réponse:\n",
    "\"\"\"\n",
    "\n",
    "template = f\"\"\"\n",
    "{system_instructions}\n",
    "\n",
    "{question_instructions}\n",
    "\"\"\"\n",
    "\n",
    "custom_rag_prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | custom_rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "answer = rag_chain.invoke(\"Quelle est la définition du PIB ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "\n",
    "\n",
    "printmd(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "rag_chain = {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()} | prompt | llm | StrOutputParser()\n",
    "\n",
    "for chunk in rag_chain.stream(\"Quelle est la définition du PIB?\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    max_tokens=MAX_NEW_TOKEN,\n",
    "    temperature=TEMPERATURE,\n",
    "    top_p=TOP_P,\n",
    "    repetition_penalty=REP_PENALTY,\n",
    ")\n",
    "\n",
    "llm = LLM(model=LLM_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "from vllm import SamplingParams\n",
    "\n",
    "sampling_params = SamplingParams(temperature=0.2, top_p=0.95)\n",
    "outputs = llm.generate([prompt_injected], sampling_params)\n",
    "\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    # !r calls repr(), which prints a string inside quotes.\n",
    "    print()\n",
    "    print(f\"Question: {question!r}\")\n",
    "    pprint.pprint(f\"Generated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import VLLM\n",
    "\n",
    "llm = VLLM(\n",
    "    model=model_id,\n",
    "    trust_remote_code=True,  # mandatory for hf models\n",
    "    max_new_tokens=128,\n",
    "    top_k=10,\n",
    "    top_p=0.95,\n",
    "    temperature=0.8,\n",
    "    # tensor_parallel_size=... # for distributed inference\n",
    ")\n",
    "\n",
    "print(llm(\"What is the capital of France ?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilisation en direct de LLama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "llama_direct = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, device_map=\"auto\", pad_token_id=tokenizer.eos_token_id\n",
    ").to(device)\n",
    "\n",
    "\n",
    "CHATBOT_TEMPLATE_bis = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system_instructions,\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": question_instructions.format(context=retrieved_docs_as_string, question=question)},\n",
    "]\n",
    "\n",
    "tokenized_chat = tokenizer.apply_chat_template(\n",
    "    CHATBOT_TEMPLATE_bis, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\"\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(tokenized_chat[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = llama_direct.generate(tokenized_chat, max_new_tokens=128000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = tokenizer.decode(outputs[0]).split(\"tion du PIB?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "\n",
    "\n",
    "printmd(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output  # .outputs[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHATBOT_INSTRUCTION = \"\"\"\n",
    "En utilisant UNIQUEMENT les informations présentes dans le contexte, réponds de manière argumentée à la question posée.\n",
    "La réponse doit être développée et citer ses sources.\n",
    "\n",
    "Si tu ne peux pas induire ta réponse du contexte, ne réponds pas.\n",
    "\"\"\"\n",
    "\n",
    "USER_INSTRUCTION = \"\"\"\n",
    "Voici le contexte sur lequel tu dois baser ta réponse :\n",
    "Contexte:\n",
    "{context}\n",
    "---\n",
    "Voici la question à laquelle tu dois répondre :\n",
    "Question: {question}\"\"\"\n",
    "\n",
    "CHATBOT_TEMPLATE = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\"Tu es un assistant spécialisé dans la statistique publique.\n",
    "    Tu réponds à des questions concernant les données de l'Insee, l'institut national statistique Français.\n",
    "    Réponds en FRANCAIS UNIQUEMENT.\"\"\",\n",
    "    },\n",
    "    {\"role\": \"assistant\", \"content\": CHATBOT_INSTRUCTION},\n",
    "    {\"role\": \"user\", \"content\": USER_INSTRUCTION},\n",
    "]\n",
    "\n",
    "RAG_PROMPT_TEMPLATE = tokenizer.apply_chat_template(CHATBOT_TEMPLATE, tokenize=False, add_generation_prompt=True)\n",
    "prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=RAG_PROMPT_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.generate(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain_from_docs = RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"]))) | prompt\n",
    "\n",
    "rag_chain_with_source = RunnableParallel({\"context\": retriever, \"question\": RunnablePassthrough()}).assign(\n",
    "    answer=rag_chain_from_docs\n",
    ")\n",
    "\n",
    "question = rag_chain_with_source.invoke(\"Quelle est la définition du PIB ?\")[\"answer\"].to_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On teste la question en standalone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    return_full_text=True,\n",
    "    device_map=\"auto\",\n",
    "    do_sample=True,\n",
    "    temperature=0.2,\n",
    ")\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You assist French opendata specialists. You answer in French\"},\n",
    "    {\"role\": \"user\", \"content\": question},\n",
    "]\n",
    "outputs = pipe(\n",
    "    messages,\n",
    "    max_new_tokens=2000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_outside_chain = outputs[0][\"generated_text\"][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHATBOT_TEMPLATE = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"Tu es un assistant spécialisé dans la statistique publique.\n",
    "    Tu réponds à des questions concernant les données de l'Insee, l'institut national statistique Français.\n",
    "    Réponds en FRANCAIS UNIQUEMENT.\n",
    "    \n",
    "    En utilisant UNIQUEMENT les informations présentes dans le contexte, réponds de manière argumentée à la question posée.\n",
    "    La réponse doit être développée et citer ses sources.\n",
    "\n",
    "    Si tu ne peux pas induire ta réponse du contexte, ne réponds pas.\n",
    "    \"\"\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": question},\n",
    "]\n",
    "\n",
    "outputs2 = pipe(\n",
    "    messages,\n",
    "    max_new_tokens=2000,\n",
    ")\n",
    "answer_outside_chain2 = outputs2[0][\"generated_text\"][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs2[0][-1][\"generated_text\"][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intégration dans un prompt\n",
    "\n",
    "conclusion: c'est la galère, il vaut mieux utiliser vllm, cf. projet thomas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.config import CHATBOT_TEMPLATE as chatbottemplate\n",
    "\n",
    "RAG_PROMPT_TEMPLATE = tokenizer.apply_chat_template(chatbottemplate, tokenize=False, add_generation_prompt=True)\n",
    "prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=RAG_PROMPT_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_documents = format_docs(retriever.invoke(\"Quelle est la définition du chômage ?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_messages = prompt.invoke(\n",
    "    {\"context\": retrieved_documents, \"question\": \"Quelle est la définition du PIB ?\"}\n",
    ").to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()} | prompt\n",
    "\n",
    "retrieved_in_prompt = rag_chain.invoke(\"Quelle est la définition du PIB ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"Tu es un assistant spécialisé dans la statistique publique.\n",
    "    Tu réponds à des questions concernant les données de l'Insee, l'institut national statistique Français.\n",
    "    Réponds en FRANCAIS UNIQUEMENT.\n",
    "    En utilisant UNIQUEMENT les informations présentes dans le contexte, réponds de manière argumentée à la question posée.\n",
    "    La réponse doit être développée et citer ses sources.\n",
    "\n",
    "    Si tu ne peux pas induire ta réponse du contexte, ne réponds pas.    \n",
    "    \"\"\",\n",
    "    },\n",
    "    {\"role\": \"system\", \"content\": \"You assist French opendata specialists. You answer in French\"},\n",
    "    {\"role\": \"user\", \"content\": question},\n",
    "]\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    return_full_text=True,\n",
    "    device_map=\"auto\",\n",
    "    do_sample=True,\n",
    "    temperature=0.2,\n",
    ")\n",
    "\n",
    "outputs = pipe(\n",
    "    messages,\n",
    "    max_new_tokens=2000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[0][\"generated_text\"][-1][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_PROMPT_TEMPLATE = tokenizer.apply_chat_template(chatbottemplate, tokenize=False, add_generation_prompt=True)\n",
    "prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=RAG_PROMPT_TEMPLATE)\n",
    "\n",
    "template = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"Tu es un assistant spécialisé dans la statistique publique.\n",
    "    Tu réponds à des questions concernant les données de l'Insee, l'institut national statistique Français.\n",
    "    Réponds en FRANCAIS UNIQUEMENT.\n",
    "    En utilisant UNIQUEMENT les informations présentes dans le contexte, réponds de manière argumentée à la question posée.\n",
    "    La réponse doit être développée et citer ses sources.\n",
    "\n",
    "    Si tu ne peux pas induire ta réponse du contexte, ne réponds pas.    \n",
    "    \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\"\n",
    "    Voici la question à laquelle tu dois répondre\n",
    "    Question: {question}\n",
    "    Context: {context}\n",
    "    \"\"\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt.invoke({\"context\": \"filler context\", \"question\": \"filler question\"}).to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    return_full_text=True,\n",
    "    device_map=\"auto\",\n",
    "    do_sample=True,\n",
    "    temperature=0.2,\n",
    ")\n",
    "\n",
    "outputs = pipe(\n",
    "    messages,\n",
    "    max_new_tokens=2000,\n",
    ")\n",
    "\n",
    "hf = HuggingFacePipeline(pipeline=pipe, pad_token_id=pipe.model.config.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_pipeline = hf.invoke(retrieved_in_prompt.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avec une chaine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()} | prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in rag_chain.stream(\"Quelle est la définition du PIB ?\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "rag_chain = {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()} | prompt | llm | StrOutputParser()\n",
    "\n",
    "rag_chain_from_docs = (\n",
    "    RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"]))) | prompt | llm | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain_with_source = RunnableParallel({\"context\": retriever, \"question\": RunnablePassthrough()}).assign(\n",
    "    answer=rag_chain_from_docs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pib = rag_chain_with_source.invoke(\"Quelle est la définition du PIB ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You assist French opendata specialists. You answer in French\"},\n",
    "    {\"role\": \"user\", \"content\": question},\n",
    "]\n",
    "outputs = pipe(\n",
    "    messages,\n",
    "    max_new_tokens=2000,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFacePipeline\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are an assistant for question-answering tasks.\n",
    "    Use the following documents to answer the question.\n",
    "    If you don't know the answer, just say that you don't know.\n",
    "    Use three sentences maximum and keep the answer concise:\n",
    "    Question: {question}\n",
    "    Documents: {documents}\n",
    "    Answer:\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"documents\"],\n",
    ")\n",
    "\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You assist French opendata specialists. You answer in French\"},\n",
    "    {\"role\": \"user\", \"content\": \"Quelle est la définition du PIB ?\"},\n",
    "]\n",
    "outputs = pipe(\n",
    "    messages,\n",
    "    max_new_tokens=2000,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][-1])\n",
    "\n",
    "\n",
    "# llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PromptTemplate(\n",
    "    template=\"\"\"You are an assistant for question-answering tasks.\n",
    "    Use the following documents to answer the question.\n",
    "    If you don't know the answer, just say that you don't know.\n",
    "    Use three sentences maximum and keep the answer concise:\n",
    "    Question: {question}\n",
    "    Answer:\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"documents\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = build_chain(\n",
    "    retriever=retriever,\n",
    "    prompt=prompt,\n",
    "    llm=llm,\n",
    "    reranker=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_pipeline = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_pipeline.invoke(\"Donne moi les chiffres du chômage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    return_full_text=True,\n",
    "    device_map=\"auto\",\n",
    "    do_sample=True,\n",
    "    temperature=0.2,\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "x = llm.invoke(\"je veux les chiffres du chômage en France\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHATBOT_INSTRUCTION = \"\"\"\n",
    "En utilisant UNIQUEMENT les informations présentes dans le contexte, réponds de manière argumentée à la question posée.\n",
    "La réponse doit être développée et citer ses sources.\n",
    "\n",
    "Si tu ne peux pas induire ta réponse du contexte, ne réponds pas.\n",
    "\"\"\"\n",
    "\n",
    "USER_INSTRUCTION = \"\"\"Voici le contexte sur lequel tu dois baser ta réponse :\n",
    "Contexte:\n",
    "{context}\n",
    "---\n",
    "Voici la question à laquelle tu dois répondre :\n",
    "Question: {question}\"\"\"\n",
    "\n",
    "CHATBOT_TEMPLATE = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\"Tu es un assistant spécialisé dans la statistique publique.\n",
    "    Tu réponds à des questions concernant les données de l'Insee, l'institut national statistique Français.\n",
    "    Réponds en FRANCAIS UNIQUEMENT.\"\"\",\n",
    "    },\n",
    "    {\"role\": \"assistant\", \"content\": CHATBOT_INSTRUCTION},\n",
    "    {\"role\": \"user\", \"content\": USER_INSTRUCTION},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_PROMPT_TEMPLATE = tokenizer.apply_chat_template(CHATBOT_TEMPLATE, tokenize=False, add_generation_prompt=True)\n",
    "prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=RAG_PROMPT_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"]))) | prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm, tokenizer = build_llm_model(\n",
    "    model_name=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    quantization_config=QUANTIZATION,\n",
    "    config=True,\n",
    "    token=os.getenv(\"HF_TOKEN\"),\n",
    "    streaming=False,\n",
    "    generation_args={\n",
    "        \"max_new_tokens\": 100000,\n",
    "        \"max_length\": 100000,\n",
    "        \"return_full_text\": RETURN_FULL_TEXT,\n",
    "        \"do_sample\": DO_SAMPLE,\n",
    "        \"temperature\": MODEL_TEMPERATURE,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llm.invoke(\"quels sont les chiffres du chômage\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.invoke(\"je veux les chiffres du chomage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_docs = db.get()[\"documents\"]\n",
    "ndocs = f\"Ma base de connaissance du site Insee comporte {len(db_docs)} documents\"\n",
    "ndocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_PROMPT_TEMPLATE = tokenizer.apply_chat_template(CHATBOT_TEMPLATE, tokenize=False, add_generation_prompt=True)\n",
    "prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=RAG_PROMPT_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator = build_chain_validator(evaluator_llm=llm, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.chain_building.build_chain import build_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = build_chain(\n",
    "    retriever=retriever,\n",
    "    prompt=prompt,\n",
    "    llm=llm,\n",
    "    reranker=\"BM25\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in chain.stream(\"Je veux les chiffres du chômage\"):\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il y a un problème en ce moment sur la chaine, au niveau de la génération après retrieval. Je vais tester sur un exemple plus minime, à partir de \n",
    "\n",
    "* https://python.langchain.com/docs/integrations/llms/huggingface_pipelines/#gpu-inference\n",
    "* https://python.langchain.com/v0.1/docs/expression_language/interface/#async-stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "retrieval_chain = (\n",
    "    {\n",
    "        \"context\": retriever.with_config(run_name=\"Docs\"),\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriever.invoke(\"je veux les chiffres du chômage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# await retrieval_chain.ainvoke(\"je veux les chiffres du chômage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_chain.batch([\"je veux les chiffres du chômage\", \"quelle est la définition de l'inflation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = retrieval_chain.invoke(\"je veux les chiffres du chômage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain.invoke(\"je veux les chiffres du chômage\")\n",
    "# await chain.ainvoke(\"je veux les chiffres du chômage\")\n",
    "for s in chain.stream(\"je veux les chiffres du chômage\"):\n",
    "    print(s)\n",
    "# async for s in chain.astream(\"je veux les chiffres du chômage\"):\n",
    "#    print(s, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs: list):\n",
    "    return \"\\n\\n\".join(\n",
    "        [\n",
    "            f\"\"\"\n",
    "            Doc {i + 1}:\\nTitle: {doc.metadata.get(\"Header 1\")}\\n\n",
    "            Source: {doc.metadata.get(\"url\")}\\n\n",
    "            Content:\\n{doc.page_content}\n",
    "            \"\"\"\n",
    "            for i, doc in enumerate(docs)\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stream = retrieval_chain.invoke(\"donne chiffres chomage\")\n",
    "\n",
    "llm.invoke(\"donne chiffres chomage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async for event in retrieval_chain.astream_events(\n",
    "    \"je veux les chiffres du chômage\", version=\"v1\", include_names=[\"Docs\", \"my_llm\"]\n",
    "):\n",
    "    kind = event[\"event\"]\n",
    "    if kind == \"on_chat_model_stream\":\n",
    "        print(event[\"data\"][\"chunk\"].content, end=\"|\")\n",
    "    elif kind in {\"on_chat_model_start\"}:\n",
    "        print()\n",
    "        print(\"Streaming LLM:\")\n",
    "    elif kind in {\"on_chat_model_end\"}:\n",
    "        print()\n",
    "        print(\"Done streaming LLM.\")\n",
    "    elif kind == \"on_retriever_end\":\n",
    "        print(\"--\")\n",
    "        print(\"Retrieved the following documents:\")\n",
    "        print(event[\"data\"][\"output\"][\"documents\"])\n",
    "    elif kind == \"on_tool_end\":\n",
    "        print(f\"Ended tool: {event['name']}\")\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chainlit as cl\n",
    "\n",
    "async for chunk in chain.astream(\n",
    "    \"je veux les chiffres du chomage\",\n",
    "    config=RunnableConfig(callbacks=[cl.AsyncLangchainCallbackHandler(stream_final_answer=True)]),\n",
    "):\n",
    "    if \"answer\" in chunk:\n",
    "        await answer_msg.stream_token(chunk[\"answer\"])\n",
    "        generated_answer = chunk[\"answer\"]\n",
    "\n",
    "    if \"context\" in chunk:\n",
    "        docs = chunk[\"context\"]\n",
    "        for doc in docs:\n",
    "            sources.append(doc.metadata.get(\"url\"))\n",
    "            titles.append(doc.metadata.get(\"Header 1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\"Je veux les chiffres du chômage\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
