{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete RAG with answer storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel, RunnableLambda\n",
    "from langchain.vectorstores import Chroma\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "import torch\n",
    "\n",
    "# vector database\n",
    "DB_DIR = \"llm-open-data-insee/src/data/chroma_db\"\n",
    "# embedding model\n",
    "EMB_DEVICE = \"cuda\"\n",
    "\n",
    "EMB_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# LLM\n",
    "MODEL_DEVICE = {\"\": 0}\n",
    "# MODEL_NAME = \"tiiuae/falcon-7b\"  #use flash attention (faster Attention computation) and Quantization (smaller model memory usage)\n",
    "\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "# MODEL_NAME = \"EleutherAI/gpt-neo-1.3B\"\n",
    "\n",
    "\n",
    "def build_llm_model():\n",
    "    \"\"\"\n",
    "    Create the llm model\n",
    "    \"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # load LLM config\n",
    "    config = AutoConfig.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "    # config.max_position_embeddings = 8096\n",
    "    # load quantization config\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=\"float16\",\n",
    "        bnb_4bit_use_double_quant=False,\n",
    "    )\n",
    "    # load llm tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True, device_map=\"auto\")\n",
    "\n",
    "    # Check if tokenizer has a pad_token; if not, set it to eos_token\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # load llm\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME, config=config, quantization_config=quantization_config\n",
    "    )\n",
    "    # Create a pipeline with  tokenizer and model\n",
    "    pipeline_HF = pipeline(\n",
    "        task=\"text-generation\",  # TextGenerationPipeline HF pipeline\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=2000,\n",
    "        temperature=0.2,\n",
    "        return_full_text=False,\n",
    "        device_map=\"auto\",\n",
    "        do_sample=True,\n",
    "    )\n",
    "    # Create a LangChain Runnable pipeline\n",
    "\n",
    "    langchain_llm = HuggingFacePipeline(pipeline=pipeline_HF)\n",
    "\n",
    "    return langchain_llm\n",
    "\n",
    "\n",
    "def format_docs(docs) -> str:\n",
    "    \"\"\"\n",
    "    Format the retrieved document before giving their content to complete the prompt\n",
    "    \"\"\"\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "def build_chain(hf_embeddings, vectorstore, retriever, prompt, llm):\n",
    "    \"\"\"\n",
    "    Build a LLM chain based on Langchain package and INSEE data\n",
    "    \"\"\"\n",
    "    # Create a Langchain LLM Chain\n",
    "    rag_chain_from_docs = (\n",
    "        RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    # Create a Langchain LLM Chain which return sources and store them into a log file\n",
    "    rag_chain_with_source = RunnableParallel(\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    ).assign(answer=rag_chain_from_docs) | RunnableLambda(log_interaction)\n",
    "\n",
    "    return rag_chain_with_source  # rag_chain_from_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create prompt for chat template\n",
    "RAG_PROMPT_TEMPLATE = \"\"\"\n",
    "<s>[INST] \n",
    "Tu es un assistant spécialisé dans la statistique publique répondant aux questions d'agent de l'INSEE. \n",
    "Réponds en Français seulement.\n",
    "Utilise les informations obtenues dans le contexte, réponds de manière argumentée à la question posée.\n",
    "La réponse doit être développée et citer ses sources.\n",
    "\n",
    "Si tu ne peux pas induire ta réponse du contexte, ne réponds pas. \n",
    "Voici le contexte sur lequel tu dois baser ta réponse : \n",
    "Contexte: {context}\n",
    "        ---\n",
    "Voici la question à laquelle tu dois répondre : \n",
    "Question: {question}\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "# load Embedding model\n",
    "hf_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=EMB_MODEL_NAME, model_kwargs={\"device\": EMB_DEVICE}\n",
    ")\n",
    "# load vector database\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"insee_data\", embedding_function=hf_embeddings, persist_directory=str(DB_DIR)\n",
    ")\n",
    "# set up a retriever\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\", search_kwargs={\"score_threshold\": 0.5, \"k\": 10}\n",
    ")\n",
    "# generate prompt template\n",
    "prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=RAG_PROMPT_TEMPLATE)\n",
    "\n",
    "# create a pipeline with tokenizer and LLM\n",
    "llm = build_llm_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save RAG outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List\n",
    "from langchain.docstore.document import Document\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "\n",
    "def save_logs(\n",
    "    user_query: str = None,\n",
    "    retrieved_documents: List[Document] = None,\n",
    "    prompt_template: str = None,\n",
    "    generated_answer: str = None,\n",
    "    embedding_model_name: str = None,\n",
    "    LLM_name: str = None,\n",
    "    filename=\"./logs/conversation_logs.json\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Save details of a RAG conversation to a json file.\n",
    "\n",
    "    Args:\n",
    "    user_query (str): The user's input query.\n",
    "    retrieved_documents (list[Document]): List of documents retrieved based on the user query.\n",
    "    prompt_template (str): The template used to generate the prompt for the language model.\n",
    "    generated_answer (str): The answer generated by the language model.\n",
    "    RAG_pipeline : (HF pipeline)\n",
    "    filename (str): The filename where the log will be saved.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Ensure the path for the log file exists\n",
    "    if not os.path.exists(os.path.dirname(filename)):\n",
    "        os.makedirs(os.path.dirname(filename))\n",
    "\n",
    "    retrieved_documents_text = [d.page_content for d in retrieved_documents]\n",
    "    retrieved_documents_metadata = [d.metadata for d in retrieved_documents]\n",
    "\n",
    "    # Prepare the content to be logged as a dictionary\n",
    "    log_entry = {\n",
    "        \"user_query\": user_query,\n",
    "        \"retrieved_docs_text\": retrieved_documents_text,\n",
    "        \"prompt\": prompt_template,\n",
    "        \"generated_answer\": generated_answer,\n",
    "        \"embedding_model\": embedding_model_name,\n",
    "        \"llm\": LLM_name,\n",
    "        \"retrieved_doc_metadata\": retrieved_documents_metadata,\n",
    "        \"timestamp\": datetime.datetime.now().isoformat(),\n",
    "    }\n",
    "\n",
    "    # Open the file in append mode and write the dictionary as a JSON object\n",
    "    with open(filename, \"a\", encoding=\"utf-8\") as file:\n",
    "        json.dump(log_entry, file, ensure_ascii=False, indent=4)\n",
    "        file.write(\"\\n\")\n",
    "\n",
    "\n",
    "def log_interaction(result):\n",
    "    \"\"\"\n",
    "    Logs interaction details into a JSON file and returns the original result.\n",
    "    \"\"\"\n",
    "    log_file_path = \"llm-open-data-insee/src/logs/conversation_logs.json\"\n",
    "\n",
    "    # Extracting necessary details from the result\n",
    "    user_query = result[\"question\"]\n",
    "    generated_answer = result[\"answer\"]\n",
    "    retrieved_documents = result[\"context\"]\n",
    "    prompt_template = prompt.template  # Ensure 'prompt' is accessible here\n",
    "    embedding_model_name = EMB_MODEL_NAME\n",
    "    LLM_name = MODEL_NAME\n",
    "\n",
    "    # Call to save the logs\n",
    "    print(f\"saving outputs in {log_file_path}\")\n",
    "    save_logs(\n",
    "        user_query,\n",
    "        retrieved_documents,\n",
    "        prompt_template,\n",
    "        generated_answer,\n",
    "        embedding_model_name,\n",
    "        LLM_name,\n",
    "        filename=log_file_path,\n",
    "    )\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = build_chain(hf_embeddings, vectorstore, retriever, prompt, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain.invoke(\"Quels sont les chiffres du tourisme en France métropolitaine?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
