{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Evaluation : Eval Dataset Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a synthetic dataset for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we do not have access to a real Q&A datset, we can use LLMs to generate synthetic Question and Answer couples based on a given context. This method is a very common approach, the well-known RAGAS package is based on synthetic Q&A. Furthermore, the zero-shot or few-short learning LLM abilities allow to filter out these generated proposals by asking (with a specific prompt) to evaluate (give a grade between 1 and 5) human understandable criteria (like groundedness, relevence,...) in a specific format like JSON. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\n",
    "    \"/home/onyxia/work/llm-open-data-insee/data_complete.csv\", low_memory=False\n",
    ")  # we assume the textual information have already been extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from db_building import extract_paragraphs\n",
    "\n",
    "results = extract_paragraphs(data)  # gather textual information from the same page together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pd.DataFrame.from_dict(results)\n",
    "ds.to_csv(\"insee_documents.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "from tqdm import tqdm\n",
    "\n",
    "ds = pd.read_csv(\"insee_documents.csv\")\n",
    "langchain_docs = [\n",
    "    LangchainDocument(\n",
    "        page_content=doc[\"document\"],\n",
    "        metadata={\"source\": doc[\"url_source\"], \"title\": doc[\"title\"], \"insee_id\": doc[\"id_origin\"]},\n",
    "    )\n",
    "    for _, doc in tqdm(ds.iterrows())\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000,  # define the size of the chunck\n",
    "    chunk_overlap=200,  # define the overlapping between following chunks\n",
    "    add_start_index=True,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    ")\n",
    "\n",
    "docs_processed = []\n",
    "for doc in tqdm(langchain_docs):\n",
    "    docs_processed += text_splitter.split_documents([doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : Phi3 microsoft SLM is not yet in HF official package version. Loading a Development version is required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip uninstall -y transformers && pip install git+https://github.com/huggingface/transformers\n",
    "import torch\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# model_name = \"microsoft/Phi-3-mini-128k-instruct\" #(to download)\n",
    "# path_model = \"microsoft-Phi-3-mini-128k-instruct\" #(to load)\n",
    "# model.save_pretrained(\"microsoft-Phi-3-mini-128k-instruct\")\n",
    "\n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"  # smaller model\n",
    "\n",
    "# load LLM config\n",
    "config = AutoConfig.from_pretrained(model_name, trust_remote_code=True)\n",
    "# config.max_position_embeddings = 8096\n",
    "# load quantization config\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    config=config,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim is to generate **questions** and **answers** based on large **context** (part of INSEE documents). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_generation_prompt = \"\"\"\n",
    "<|user|>\n",
    "Ta tâche consiste à écrire une question factuelle et sa réponse en te basant sur un contexte donné.\n",
    "Ta question factuelle doit pouvoir être répondue par une information factuelle spécifique et concise tirée du contexte.\n",
    "Ta question factuelle doit être formulée dans le même style que les questions que les utilisateurs pourraient poser dans un moteur de recherche.\n",
    "Cela signifie que ta question factuelle NE DOIT PAS mentionner des phrases comme \"selon le passage\" ou \"le contexte\".\n",
    "<|user|>\n",
    "<|assistant|>\n",
    "Voici maintenant le contexte.\n",
    "\n",
    "Contexte : {context}\n",
    "\n",
    "Question factuelle : (ta question factuelle)\n",
    "Réponse : (ta réponse à la question factuelle)\n",
    "<|assistant|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "cont_sam = docs_processed[int(np.random.choice(range(len(docs_processed)), 1)[0])]\n",
    "print(QA_generation_prompt.format(context=cont_sam.page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_output(s):\n",
    "    \"\"\"remove unwanted characters\"\"\"\n",
    "    return s.replace(\"\\n\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 300,\n",
    "    \"return_full_text\": False,\n",
    "    \"do_sample\": False,\n",
    "}\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def generate_test_dataset(pipeline, docs_processed, N_GENERATIONS, BATCH_SIZE):\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"Generating {N_GENERATIONS} QA couples...\")\n",
    "    t0 = time.time()\n",
    "    outputs = []\n",
    "    docs_sampled = random.sample(docs_processed, N_GENERATIONS)\n",
    "    # Process documents in batches\n",
    "    batch_contexts = [doc.page_content for doc in docs_sampled]\n",
    "    # Generate QA couples for the batch\n",
    "    generation_args[\"batch_size\"] = BATCH_SIZE\n",
    "    batch_prompts = [QA_generation_prompt.format(context=ctx) for ctx in batch_contexts]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated_out = pipeline(batch_prompts, **generation_args)\n",
    "\n",
    "    for output, sampled_context in zip(generated_out, docs_sampled):\n",
    "        output_QA_couple = output[0][\"generated_text\"]\n",
    "        question = output_QA_couple.split(\"Question factuelle : \")[-1].split(\"Réponse : \")[0]\n",
    "        answer = output_QA_couple.split(\"Réponse : \")[-1]\n",
    "\n",
    "        outputs.append(\n",
    "            {\n",
    "                \"context\": sampled_context.page_content,\n",
    "                \"question\": clean_output(question),\n",
    "                \"answer\": clean_output(answer),\n",
    "                \"source_doc\": sampled_context.metadata[\"source\"],\n",
    "            }\n",
    "        )\n",
    "    print(\"run in \", time.time() - t0, \"s\")\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = generate_test_dataset(pipe, docs_processed, N_GENERATIONS=200, BATCH_SIZE=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pd.DataFrame(outputs).head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SSP Cloud access : mc cp s3/projet-llm-insee-open-data/data/eval_data/q_and_a_insee_200.csv /home/onyxia/work/llm-open-data-insee/data/test\n",
    "q_and_a_data = pd.DataFrame(outputs)\n",
    "q_and_a_data.to_csv(\"q_and_a_insee_200.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup critique agents \n",
    "Need to evaluate if the generated questions are relevant\n",
    "- Groundedness \n",
    "- Relevance\n",
    "- Stand-alone question : Can the question be answerable without the linked document. (Is it a general knowledge question)\n",
    "\n",
    "These 3 tests are done using prompting and asking the model to share its explanations and then its score. \n",
    "The scale is between 1 and 5.\n",
    "At the end, the remaining questions received 4+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_and_a_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_groundedness_critique_prompt = \"\"\"\n",
    "<|user|>\n",
    "Vous allez recevoir un contexte et une question.\n",
    "Votre tâche est de fournir une évaluation globale notant dans quelle mesure on peut répondre de manière non ambiguë à la question donnée avec le contexte donné.\n",
    "Donnez votre réponse sur une échelle de 1 à 5, où 1 signifie que la question n'est pas du tout répondable compte tenu du contexte, et 5 signifie que la question est clairement et sans ambiguïté répondable avec le contexte.\n",
    "\n",
    "Fournissez votre réponse comme suit :\n",
    "\n",
    "Évaluation : (votre justification de la note, sous forme de texte)\n",
    "Note totale : (votre note, entre 1 et 5)\n",
    "\n",
    "Vous DEVEZ fournir des valeurs pour 'Évaluation :' et 'Note totale :' dans votre réponse et rien d'autre. \n",
    "\n",
    "Maintenant, voici la question et le contexte.\n",
    "Question : {question}\n",
    "Contexte : {context}\n",
    "<|end|>\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "question_relevance_critique_prompt = \"\"\"\n",
    "<|user|>\n",
    "Vous allez recevoir une question.\n",
    "Votre tâche est de fournir une 'notation globale' représentant à quel point cette question peut être utile pour les agents de l'institut de statistique public français.\n",
    "Donnez votre réponse sur une échelle de 1 à 5, où 1 signifie que la question n'est pas du tout utile, et 5 signifie que la question est extrêmement utile.\n",
    "\n",
    "Fournissez votre réponse comme suit :\n",
    "\n",
    "Évaluation : (votre justification de la note, sous forme de texte)\n",
    "Note totale : (votre note, sous forme d'un nombre entre 1 et 5)\n",
    "\n",
    "Vous DEVEZ fournir des valeurs pour 'Évaluation :' et 'Note totale :' dans votre réponse.\n",
    "\n",
    "Maintenant, voici la question.\n",
    "\n",
    "Question : {question}\n",
    "<|end|>\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "\n",
    "question_standalone_critique_prompt = \"\"\" \n",
    "<|user|>\n",
    "Vous allez recevoir une question.\n",
    "Votre tâche est de fournir une 'notation globale' représentant à quel point cette question est indépendante du contexte.\n",
    "Donnez votre réponse sur une échelle de 1 à 5, où 1 signifie que la question dépend d'informations supplémentaires pour être comprise, et 5 signifie que la question a du sens par elle-même.\n",
    "Par exemple, si la question fait référence à un cadre particulier, comme 'dans le contexte' ou 'dans le document', la note doit être de 1.\n",
    "Les questions peuvent contenir des noms techniques obscurs ou des acronymes comme INSEE ou du vocabulaire propre au statistiques et obtenir tout de même une note de 5 : il suffit simplement qu'un agent d'un institut de statistique public ayant accès à la documentation comprenne de quoi parle la question.\n",
    "\n",
    "Fournissez votre réponse comme suit :\n",
    "\n",
    "Évaluation : (votre justification de la note, sous forme de texte)\n",
    "Note totale : (votre note, sous forme d'un nombre entre 1 et 5)\n",
    "\n",
    "Vous DEVEZ fournir des valeurs pour 'Évaluation :' et 'Note totale :' dans votre réponse.\n",
    "\n",
    "Maintenant, voici la question.\n",
    "\n",
    "Question : {question}\\n\n",
    "<|end|>\n",
    "<|assistant|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def extract_btw_tag(text, tag_1, tag_2):\n",
    "    pattern = f\"{tag_1}(.*?){tag_2}\"\n",
    "    return re.findall(pattern, text)[0] if re.findall(pattern, text) else None\n",
    "\n",
    "\n",
    "def extract_score_eval(text):\n",
    "    eval_text = extract_btw_tag(text, tag_1=\"Évaluation : \", tag_2=\"\\n\")\n",
    "    score_text = extract_btw_tag(text, tag_1=\"Note totale : \", tag_2=\"\\n\")\n",
    "    score = int(score_text) if score_text else None\n",
    "    return score, eval_text\n",
    "\n",
    "\n",
    "def critique_Q_and_A(pipeline, dataset, args, k=2):\n",
    "    data = dataset.copy()\n",
    "\n",
    "    num_rows = len(data)\n",
    "\n",
    "    for batch_start in tqdm(range(0, num_rows, k)):\n",
    "        batch_end = min(\n",
    "            batch_start + k, num_rows\n",
    "        )  # Adjust batch end to handle the last incomplete batch\n",
    "        batch_prompts = []\n",
    "\n",
    "        for idx in range(batch_start, batch_end):\n",
    "            row = data.iloc[idx]\n",
    "            batch_prompts.extend(\n",
    "                [\n",
    "                    question_groundedness_critique_prompt.format(\n",
    "                        context=row.context, question=row.question\n",
    "                    ),\n",
    "                    question_relevance_critique_prompt.format(question=row.question),\n",
    "                    question_standalone_critique_prompt.format(question=row.question),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        outputs = pipeline(batch_prompts, **args)\n",
    "\n",
    "        try:\n",
    "            for j, idx in enumerate(range(batch_start, batch_end)):\n",
    "                metrics = {}\n",
    "                metrics[\"groundedness_score\"], metrics[\"groundedness_eval\"] = extract_score_eval(\n",
    "                    outputs[3 * j][0][\"generated_text\"]\n",
    "                )\n",
    "                metrics[\"relevance_score\"], metrics[\"relevance_eval\"] = extract_score_eval(\n",
    "                    outputs[3 * j + 1][0][\"generated_text\"]\n",
    "                )\n",
    "                metrics[\"standalone_score\"], metrics[\"standalone_eval\"] = extract_score_eval(\n",
    "                    outputs[3 * j + 2][0][\"generated_text\"]\n",
    "                )\n",
    "\n",
    "                data.loc[idx, list(metrics.keys())] = list(metrics.values())\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Error:\", e)\n",
    "            continue\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = q_and_a_data.iloc[0]\n",
    "prompt_final = question_groundedness_critique_prompt.format(\n",
    "    context=row.context, question=row.question\n",
    ")\n",
    "prompt_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_questions = critique_Q_and_A(pipe, q_and_a_data, generation_args, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_questions.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out bad questions based on our critique agent scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "print(\"Evaluation dataset before filtering:\")\n",
    "\n",
    "display(\n",
    "    generated_questions[\n",
    "        [\n",
    "            \"question\",  # question generated based on given context\n",
    "            \"answer\",  # answer generated based on given context (and generated question)\n",
    "            \"groundedness_score\",  # score\n",
    "            \"relevance_score\",  # score\n",
    "            \"standalone_score\",  # score\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Select only the most relevant questions\n",
    "generated_questions = generated_questions.loc[\n",
    "    (generated_questions[\"groundedness_score\"] >= 4)\n",
    "    & (generated_questions[\"relevance_score\"] >= 4)\n",
    "    & (generated_questions[\"standalone_score\"] >= 4)\n",
    "]\n",
    "print(\"number of selected Q&A couple : \", len(generated_questions))\n",
    "\n",
    "print(\"============================================\")\n",
    "print(\"Final evaluation dataset:\")\n",
    "display(\n",
    "    generated_questions[\n",
    "        [\n",
    "            \"question\",\n",
    "            \"answer\",\n",
    "            \"groundedness_score\",\n",
    "            \"relevance_score\",\n",
    "            \"standalone_score\",\n",
    "        ]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_questions.to_csv(\"eval_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mc cp /home/onyxia/work/eval_dataset.csv s3/projet-llm-insee-open-data/data/eval_data/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
