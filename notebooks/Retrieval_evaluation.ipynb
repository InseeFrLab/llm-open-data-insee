{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Information Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal : Evaluate Retrieval part of the RAG pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- use appropriate non Eval LLM metrics like Recall from Information Retrieval (IR) (LLM based metrics will be used later on)\n",
    "- Allow hyperparameter tuning (chunck size, embedding model, number of retrieved documents, use a RERANKER or not, add metadata to the chuncks)\n",
    "- Create clear visuals\n",
    "- Focus on Reproductibility with a small dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract data sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From **data_complete.csv** (raw data) OR **insee_documents.csv** (Extracted metadata and paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python insee_data_processing.py\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"data_complete.csv\", low_memory=False)\n",
    "data_sample = data.sample(1000)\n",
    "data_sample.head()\n",
    "\n",
    "from utils import extract_paragraphs\n",
    "\n",
    "data_sample = extract_paragraphs(data_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "from tqdm import tqdm\n",
    "\n",
    "# pd.DataFrame.from_dict(data_sample)\n",
    "ds = pd.read_csv(\"insee_documents.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain_docs = [\n",
    "    LangchainDocument(\n",
    "        page_content=doc[\"paragraphs\"],\n",
    "        metadata={\n",
    "            \"source\": doc[\"url_source\"],\n",
    "            \"title\": doc[\"titles_para\"],\n",
    "            \"insee_id\": doc[\"id_origin\"],\n",
    "            \"categories\": doc[\"categories\"],\n",
    "            \"date_diffusion\": doc[\"dateDiffusion\"],\n",
    "            \"themes\": doc[\"themes\"],\n",
    "            \"collections\": doc[\"collections\"],\n",
    "            \"libelleAffichageGeo\": doc[\"libelleAffichageGeo\"],\n",
    "            \"intertitres\": doc[\"intertitres\"],\n",
    "            \"authors\": doc[\"authors\"],\n",
    "            \"subtitle\": doc[\"subtitle\"],\n",
    "        },\n",
    "    )\n",
    "    for _, doc in tqdm(ds.iterrows())\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(langchain_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the top N documents with largest contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "def get_top_n_documents_with_largest_content(documents: List[LangchainDocument], n=1000):\n",
    "    # Create a list of tuples (content_size, document)\n",
    "    document_sizes = [(len(doc.page_content.split()), doc) for doc in documents]\n",
    "\n",
    "    # Sort the list by content size in descending order\n",
    "    sorted_documents = sorted(document_sizes, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    # Get the top N documents\n",
    "    top_n_documents = [doc for (_, doc) in sorted_documents[:n]]\n",
    "\n",
    "    return top_n_documents\n",
    "\n",
    "\n",
    "sample_langchain_docs = get_top_n_documents_with_largest_content(langchain_docs, n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def stats(documents: List[LangchainDocument], type=\"categories\"):\n",
    "    if not documents:\n",
    "        return None, None, None\n",
    "\n",
    "    res = {}\n",
    "    for doc in documents:\n",
    "        if doc.metadata[type] in res:\n",
    "            res[doc.metadata[type]].append(len(doc.page_content.split()))\n",
    "        else:\n",
    "            res[doc.metadata[type]] = [len(doc.page_content.split())]\n",
    "\n",
    "    for k in res:\n",
    "        lengths = res[k]\n",
    "        n = len(lengths)\n",
    "        max_length = max(lengths)\n",
    "        min_length = min(lengths)\n",
    "        avg_length = sum(lengths) / len(lengths)\n",
    "        res[k] = (n, max_length, np.round(avg_length, 2), min_length)\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "stats(sample_langchain_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_langchain_document(documents: List[LangchainDocument]):\n",
    "    data = []\n",
    "\n",
    "    for document in documents:\n",
    "        # Create a dictionary for each document\n",
    "        doc_data = {\n",
    "            \"content\": document.page_content,\n",
    "        }\n",
    "        # Add metadata fields\n",
    "        doc_data.update(document.metadata)\n",
    "\n",
    "        data.append(doc_data)\n",
    "\n",
    "    # Create a DataFrame from the list of dictionaries\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df = store_langchain_document(sample_langchain_docs)\n",
    "# Optionally, save the DataFrame to a CSV file\n",
    "output_csv_path = \"insee_documents_sample_ref_retrieval_evaluation.csv\"\n",
    "df.to_csv(output_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "counters_para = Counter()\n",
    "\n",
    "for doc in sample_langchain_docs:\n",
    "    counters_para[len(doc.page_content.split())] += 1\n",
    "\n",
    "lengths_sorted = sorted(counters_para.items())\n",
    "lengths, counts = zip(*lengths_sorted)\n",
    "# Create bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(lengths, counts, color=\"skyblue\")\n",
    "plt.xlabel(\"Paragraph Length (number of words)\")\n",
    "plt.ylabel(\"Number of Paragraphs\")\n",
    "plt.title(f\"Paragraph Length Distribution ({len(sample_langchain_docs)} docs)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "\n",
    "# load reference dataset :\n",
    "\n",
    "\n",
    "def load_sample_data(path_data, content_column=\"content\"):\n",
    "    df = pd.read_csv(path_data)\n",
    "    loader = DataFrameLoader(df, page_content_column=content_column)\n",
    "    return loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional\n",
    "\n",
    "import pandas as pd\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "def split_documents(\n",
    "    chunk_size: int,\n",
    "    knowledge_base: List[LangchainDocument],\n",
    "    tokenizer_name: Optional[str],\n",
    "    params: Dict,\n",
    ") -> List[LangchainDocument]:\n",
    "    \"\"\"\n",
    "    Split documents into chunks of maximum size `chunk_size` tokens and return a list of documents.\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "        AutoTokenizer.from_pretrained(tokenizer_name),\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=int(chunk_size / 10),\n",
    "        add_start_index=True,\n",
    "        strip_whitespace=True,\n",
    "        separators=params[\"markdown_separator\"],\n",
    "    )\n",
    "\n",
    "    docs_processed = []\n",
    "\n",
    "    for doc in tqdm(knowledge_base):\n",
    "        docs_processed += text_splitter.split_documents([doc])\n",
    "\n",
    "    # Remove duplicates\n",
    "    unique_texts = {}\n",
    "    docs_processed_unique = []\n",
    "    for doc in docs_processed:\n",
    "        if doc.page_content not in unique_texts:\n",
    "            unique_texts[doc.page_content] = True\n",
    "            docs_processed_unique.append(doc)\n",
    "\n",
    "    return docs_processed_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "REF_EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "MARKDOWN_SEPARATORS = [\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    "\n",
    "print(f\"----------Loading Embedding model {REF_EMBEDDING_MODEL_NAME}------------\")\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(  # load sentence transformers\n",
    "    model_name=REF_EMBEDDING_MODEL_NAME,\n",
    "    multi_process=True,\n",
    "    model_kwargs={\"device\": \"cuda\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True},  # set True for cosine similarity\n",
    "    show_progress=False,\n",
    ")\n",
    "\n",
    "print(\"---------Chunking LangChain documents-----------\")\n",
    "\n",
    "embedder_max_token_length = SentenceTransformer(REF_EMBEDDING_MODEL_NAME).max_seq_length\n",
    "print(f\"max sequence of token for {REF_EMBEDDING_MODEL_NAME} : {embedder_max_token_length}\")\n",
    "\n",
    "docs_processed_sample = split_documents(\n",
    "    knowledge_base=raw_ref_database,\n",
    "    chunk_size=embedder_max_token_length,\n",
    "    tokenizer_name=REF_EMBEDDING_MODEL_NAME,\n",
    ")\n",
    "\n",
    "print(\"----------Building Chroma DB------------\")\n",
    "collection_name = \"insee_sample_\" + str(REF_EMBEDDING_MODEL_NAME.split(\"/\")[-1])\n",
    "db_chroma = Chroma.from_documents(\n",
    "    collection_name=collection_name,\n",
    "    persist_directory=\"./chroma_db\",\n",
    "    documents=docs_processed_sample,\n",
    "    embedding=embedding_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "def remove_extra_spaces(text):\n",
    "    # Split the text by whitespaces and join them with a single whitespace\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "\n",
    "def extract_question(text):\n",
    "    return text.split(\"?\")[0] + \"?\"\n",
    "\n",
    "\n",
    "Q_generation_prompt = \"\"\"\n",
    "<|user|>\n",
    "Ta tâche consiste à écrire une question factuelle en te basant sur un contexte donné.\n",
    "Ta question factuelle doit pouvoir être répondue par une information factuelle spécifique et concise tirée du contexte.\n",
    "Ta question factuelle doit être formulée dans le même style que les questions que les utilisateurs pourraient poser dans un moteur de recherche.\n",
    "Cela signifie que ta question factuelle NE DOIT PAS mentionner des phrases comme \"selon le passage\" ou \"le contexte\".\n",
    "La question doit avoir pour sujet une thématiques d'un institut de statistique public. \n",
    "Tu DOIS respecter faire apparaitre \"Question factuelle : \" avant ta réponse. \n",
    "<|user|>\n",
    "<|assistant|>\n",
    "Voici maintenant le contexte.\n",
    "\n",
    "Contexte : {context}\n",
    "\n",
    "Question factuelle : (ta question factuelle)\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def generate_test_question(\n",
    "    vector_database: Chroma, pipeline: pipeline, nb_documents=50, batch_size=5, generation_args=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate Question based on a given vector_database (based on the smallest \"max_length_token\").\n",
    "    \"\"\"\n",
    "    test_data = {\"question\": [], \"content\": [], \"source\": []}\n",
    "\n",
    "    # sample indices\n",
    "    indices = np.random.choice(len(vector_database.get()[\"ids\"]), nb_documents)\n",
    "\n",
    "    for x in indices:\n",
    "        doc = vector_database.get()[\"metadatas\"][x]\n",
    "        source = doc[\"source\"]\n",
    "        content = vector_database.get()[\"documents\"][x]\n",
    "\n",
    "        test_data[\"content\"].append(content)\n",
    "        test_data[\"source\"].append(source)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"Generating {nb_documents} Questions ...\")\n",
    "    # generate final prompts\n",
    "    batch_prompts = [Q_generation_prompt.format(context=ctx) for ctx in test_data[\"content\"]]\n",
    "    # add batch size params\n",
    "    if \"batch_size\" not in generation_args.keys():\n",
    "        generation_args[\"batch_size\"] = batch_size\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated_out = pipeline(batch_prompts, **generation_args)\n",
    "\n",
    "    for out in generated_out:\n",
    "        output_Q = out[0][\"generated_text\"]\n",
    "        question = extract_question(\n",
    "            remove_extra_spaces(output_Q.split(\"Question factuelle : \")[-1])\n",
    "        )\n",
    "        test_data[\"question\"].append(question)\n",
    "\n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_name = \"microsoft/Phi-3-mini-128k-instruct\"\n",
    "\n",
    "# load LLM config\n",
    "config = AutoConfig.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# load quantization config\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    config=config,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 300,\n",
    "    \"return_full_text\": False,\n",
    "    \"do_sample\": False,\n",
    "}\n",
    "test_data = generate_test_question(\n",
    "    vector_database=db_chroma,\n",
    "    pipeline=pipe,\n",
    "    nb_documents=100,\n",
    "    batch_size=10,\n",
    "    generation_args=generation_args,\n",
    ")\n",
    "q_and_s_df = pd.DataFrame(test_data)\n",
    "q_and_s_df.to_csv(f\"Q&S_ref_retrieval_evaluation_{model_name.split(\"/\")[-1]}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluates Embedding models on Retrieval Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embedding model to test : \n",
    "- sentence-transformers/all-MiniLM-L6-v2 (multi)\n",
    "- manu/sentence_croissant_alpha_v0.4\n",
    "- OrdalieTech/Solon-embeddings-large-0.1\n",
    "- sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
    "- intfloat/multilingual-e5-large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IR Hyperparameters : \n",
    "- chunck size : integer\n",
    "- embedding model : str\n",
    "- overlap size : integer\n",
    "- Reranker : str (name + hyperparams if exists => BM25 ou ColBERT.)\n",
    "- quantization : Bool (hyperparams if exist\n",
    "- database format : str (ChromaDB,...)\n",
    "- max token length : integer (number of maximum token in the context window of the embedding model)\n",
    "- number of chunks generated by the embedding model (depends on chunck size)\n",
    "- Embedding fine tuning\n",
    "- Filter k : integer (number of retrieved documents (before any post-processing computation like reranking))\n",
    "- use Embedding metadata (creating another index based on metadata (like Title)): Bool (if True which metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will ask to retrieved 20 or 30 documents but only extract the top 15 (with or without reranking models.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Retrieval "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLFlow is used to keep tracking experiments "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mc cp s3/projet-llm-insee-open-data/data/eval_data/eval_retrieval . --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "import pandas as pd\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from ragatouille import RAGPretrainedModel\n",
    "from rank_bm25 import BM25L, BM25Okapi, BM25Plus\n",
    "from sklearn.metrics import f1_score, precision_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def recall(retrieved, relevant):\n",
    "    intersection = set(retrieved) & set(relevant)\n",
    "    return len(intersection) / len(relevant)\n",
    "\n",
    "\n",
    "def precision(retrieved, relevant):\n",
    "    intersection = set(retrieved) & set(relevant)\n",
    "    return len(intersection) / len(retrieved) if len(retrieved) > 0 else 0\n",
    "\n",
    "\n",
    "def compute_hit_rate(predictions, labels):\n",
    "    \"\"\"\n",
    "    Hit rate metric is equivalent to the accuracy\n",
    "    \"\"\"\n",
    "    correct_predictions = sum(1 for pred, label in zip(predictions, labels) if pred == label)\n",
    "    total_predictions = len(predictions)\n",
    "    hit_rate = correct_predictions / total_predictions\n",
    "    return hit_rate\n",
    "\n",
    "\n",
    "def compute_mmr(source, retrieved_sources):\n",
    "    # compute Mean Reciprocal Rank (Order Aware Metrics)\n",
    "    if source not in retrieved_sources:\n",
    "        mrr_score = 1 / np.inf\n",
    "    else:\n",
    "        rank_q = retrieved_sources.index(source)\n",
    "        mrr_score = 1 / (rank_q + 1)\n",
    "    return mrr_score\n",
    "\n",
    "\n",
    "def remove_duplicates(docs):\n",
    "    seen_content = set()\n",
    "    unique_docs = []\n",
    "    for doc in docs:\n",
    "        if doc.page_content not in seen_content:\n",
    "            unique_docs.append(doc)\n",
    "            seen_content.add(doc.page_content)\n",
    "    return unique_docs\n",
    "\n",
    "\n",
    "def rerank_with_ColBERT(reranker, query, retrieved_docs, filter_k):\n",
    "    relevant_docs = [doc.page_content for doc in retrieved_docs]  # keep only text\n",
    "    reranked_docs = reranker.rerank(query=query, documents=relevant_docs, k=filter_k)\n",
    "    content_to_doc = {\n",
    "        doc.page_content: doc for doc in retrieved_docs if isinstance(doc.page_content, str)\n",
    "    }\n",
    "    return [content_to_doc[doc[\"content\"]] for doc in reranked_docs]\n",
    "\n",
    "\n",
    "def rerank_with_BM25(model_class, query, retrieved_docs, filter_k):\n",
    "    relevant_docs = [doc.page_content for doc in retrieved_docs]\n",
    "    bm25 = model_class(relevant_docs)\n",
    "    tokenized_query = query.split()\n",
    "    return bm25.get_top_n(tokenized_query, retrieved_docs, n=filter_k)\n",
    "\n",
    "\n",
    "def rerank_with_metadata(reranker, query, retrieved_docs, filter_k, params):\n",
    "    \"\"\"\n",
    "    note if the metadata is missing we use a \"content\" information (always exists) as a fallback\n",
    "    \"\"\"\n",
    "    new_data = []\n",
    "    for doc in retrieved_docs:\n",
    "        metadata_field = params.get(\"use_metadata\")\n",
    "        if metadata_field in doc.metadata:\n",
    "            page_content = doc.metadata[metadata_field]\n",
    "        else:\n",
    "            page_content = doc.page_content\n",
    "\n",
    "        new_data.append(\n",
    "            LangchainDocument(\n",
    "                page_content=page_content,\n",
    "                metadata={\"source\": doc.metadata.get(\"source\", \"unknown\")},\n",
    "            )\n",
    "        )\n",
    "    # load reranker\n",
    "    new_retrieved_docs = rerank_with_ColBERT(\n",
    "        reranker=reranker, query=query, retrieved_docs=new_data, filter_k=filter_k\n",
    "    )\n",
    "\n",
    "    source_to_doc_map = {doc.metadata.get(\"source\", \"unknown\"): doc for doc in retrieved_docs}\n",
    "    # reorder the original retrieved documents based on thee new reranked docs\n",
    "    reordered_docs = [\n",
    "        source_to_doc_map[new_doc.metadata[\"source\"]]\n",
    "        for new_doc in new_retrieved_docs\n",
    "        if new_doc.metadata[\"source\"] in source_to_doc_map\n",
    "    ]\n",
    "\n",
    "    return reordered_docs\n",
    "\n",
    "\n",
    "def test_retriever(\n",
    "    knowledge_index: VectorStore,\n",
    "    eval_data: pd.DataFrame,\n",
    "    embedding_model: HuggingFaceEmbeddings = None,\n",
    "    re_ranker_config: Dict = None,\n",
    "    params={},\n",
    "):\n",
    "    # recall at K : look at the k first document retrieved\n",
    "    \"\"\"\n",
    "    knowledge_index : vector database for evaluation\n",
    "    eval_data : dataframe with at least tuple : question - source\n",
    "    embedding_model : embedding model to query the knowledge_index\n",
    "    reranker : reranking model\n",
    "    params : other hyperparameters\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        \"question\": [],\n",
    "        \"source\": [],\n",
    "        \"pred\": [],\n",
    "        \"recall_at_1\": [],\n",
    "        \"recall_at_2\": [],\n",
    "        \"recall_at_3\": [],\n",
    "        \"recall_at_5\": [],\n",
    "        \"recall_at_10\": [],\n",
    "        \"recall_at_15\": [],\n",
    "        \"mrr_at_5\": [],\n",
    "        \"mrr_at_10\": [],\n",
    "    }\n",
    "\n",
    "    # print(\"pre-computing query embeddings\")\n",
    "    queries = list(eval_data[\"question\"])\n",
    "    embeddings_queries = embedding_model.embed_documents(queries)\n",
    "\n",
    "    for i, row in tqdm(eval_data.iterrows(), total=len(eval_data)):\n",
    "        q = row[\"question\"]\n",
    "        source = row[\"source\"]\n",
    "\n",
    "        results[\"question\"].append(q)\n",
    "        results[\"source\"].append(source)\n",
    "\n",
    "        if params[\"nb_retrieved_doc\"] is None:\n",
    "            params[\"nb_retrieved_doc\"] = 20\n",
    "        if params[\"filter_k\"] is None:\n",
    "            params[\"filter_k\"] = 15\n",
    "\n",
    "        filter_k = int(params[\"filter_k\"])\n",
    "\n",
    "        # retrieved documents from the vector database\n",
    "        embedded_query = embeddings_queries[i]\n",
    "        retrieved_docs = knowledge_index.similarity_search_by_vector(\n",
    "            embedding=embedded_query, k=int(params[\"nb_retrieved_doc\"])\n",
    "        )\n",
    "        # remove duplicates if exist\n",
    "        # retrieved_docs = remove_duplicates(retrieved_docs)\n",
    "\n",
    "        if re_ranker_config is not None:\n",
    "            # relevant_docs = [doc.page_content for doc in retrieved_docs] # keep only text\n",
    "            params[\"reranker\"] = re_ranker_config[\"name\"]\n",
    "\n",
    "            if re_ranker_config[\"type\"] == \"ColBERT\":\n",
    "                reranker = RAGPretrainedModel.from_pretrained(re_ranker_config[\"name\"], verbose=0)\n",
    "                retrieved_docs = rerank_with_ColBERT(\n",
    "                    reranker=reranker, query=q, retrieved_docs=retrieved_docs, filter_k=filter_k\n",
    "                )\n",
    "            if re_ranker_config[\"type\"] == \"BM25\":\n",
    "                retrieved_docs = rerank_with_BM25(\n",
    "                    re_ranker_config[\"model_class\"],\n",
    "                    query=q,\n",
    "                    retrieved_docs=retrieved_docs,\n",
    "                    filter_k=filter_k,\n",
    "                )\n",
    "\n",
    "        if (\n",
    "            params[\"use_metadata\"] is not None\n",
    "        ):  # build a small index on retrieved documents metadata.\n",
    "            reranker = RAGPretrainedModel.from_pretrained(\n",
    "                \"antoinelouis/colbertv2-camembert-L4-mmarcoFR\", verbose=0\n",
    "            )  # use by default the best french ColBERT model\n",
    "            retrieved_docs = rerank_with_metadata(\n",
    "                reranker=reranker,\n",
    "                query=q,\n",
    "                retrieved_docs=retrieved_docs,\n",
    "                filter_k=filter_k,\n",
    "                params=params,\n",
    "            )\n",
    "\n",
    "        retrieved_docs = retrieved_docs[:filter_k]  # only keep the first num_docs_final documents\n",
    "        retrieved_sources = [doc.metadata.get(\"source\") for doc in retrieved_docs]\n",
    "\n",
    "        if len(retrieved_sources) > 0:\n",
    "            results[\"pred\"].append(\n",
    "                retrieved_sources[0]\n",
    "            )  # will be compare to source using precision and recall and average precision\n",
    "            # compute recall at\n",
    "            results[\"recall_at_1\"].append(recall(retrieved_sources[:1], [source]))\n",
    "            results[\"recall_at_2\"].append(recall(retrieved_sources[:2], [source]))\n",
    "            results[\"recall_at_3\"].append(recall(retrieved_sources[:3], [source]))\n",
    "            results[\"recall_at_5\"].append(recall(retrieved_sources[:5], [source]))\n",
    "            results[\"recall_at_10\"].append(recall(retrieved_sources[:10], [source]))\n",
    "            results[\"recall_at_15\"].append(recall(retrieved_sources[:15], [source]))\n",
    "\n",
    "            # compute Mean Reciprocal Rank (Order Aware Metrics)\n",
    "            results[\"mrr_at_5\"].append(compute_mmr(source, retrieved_sources[:5]))\n",
    "            results[\"mrr_at_10\"].append(compute_mmr(source, retrieved_sources[:10]))\n",
    "        else:\n",
    "            logging.warning(f\"No documents retrieved for query: {q}\")\n",
    "\n",
    "    metrics = {\n",
    "        \"recall_at_1\": np.mean(results[\"recall_at_1\"]),\n",
    "        \"recall_at_2\": np.mean(results[\"recall_at_2\"]),\n",
    "        \"recall_at_3\": np.mean(results[\"recall_at_3\"]),\n",
    "        \"recall_at_5\": np.mean(results[\"recall_at_5\"]),\n",
    "        \"recall_at_10\": np.mean(results[\"recall_at_10\"]),\n",
    "        \"recall_at_15\": np.mean(results[\"recall_at_15\"]),\n",
    "        \"precision_score\": precision_score(\n",
    "            y_true=results[\"source\"], y_pred=results[\"pred\"], average=\"micro\"\n",
    "        ),\n",
    "        \"f1_score\": f1_score(y_true=results[\"source\"], y_pred=results[\"pred\"], average=\"micro\"),\n",
    "        \"mrr_at_5\": np.mean(results[\"mrr_at_5\"]),\n",
    "        \"mrr_at_10\": np.mean(results[\"mrr_at_10\"]),\n",
    "        \"hit_rate\": compute_hit_rate(predictions=results[\"pred\"], labels=results[\"source\"]),\n",
    "    }\n",
    "    return pd.DataFrame.from_dict(results), metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLFlow experiment\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    category=FutureWarning,\n",
    "    message=\"`resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\",\n",
    ")\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import mlflow\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "params = {\n",
    "    \"chunck_size\": None,\n",
    "    \"embedding_model\": None,\n",
    "    \"overlap_size\": None,\n",
    "    \"reranker\": None,\n",
    "    \"quantization\": False,\n",
    "    \"database_format\": \"ChromaDB\",\n",
    "    \"max_token_length\": None,\n",
    "    \"nb_chunks\": None,\n",
    "    \"fine_tuned_embedding\": False,\n",
    "    \"nb_retrieved_doc\": 30,\n",
    "    \"filter_k\": 15,\n",
    "    \"use_metadata\": None,\n",
    "    \"markdown_separator\": None,\n",
    "    \"word_embedding_dim\": None,\n",
    "}\n",
    "\n",
    "EMBEDDING_MODEL_NAME = \"OrdalieTech/Solon-embeddings-large-0.1\"\n",
    "MARKDOWN_SEPARATORS = [\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    "# since the questions have been generated on \"sentence-transformers/all-MiniLM-L6-v2\" chunks, it seems that there is a bias in the results (see the 30/05 exp)\n",
    "embedding_model_test = [\n",
    "    \"sentence-transformers/all-mpnet-base-v2\",\n",
    "    \"manu/sentence_croissant_alpha_v0.4\",\n",
    "    \"OrdalieTech/Solon-embeddings-large-0.1\",\n",
    "    \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "    \"intfloat/multilingual-e5-large\",\n",
    "]\n",
    "re_ranker_model_test = [\n",
    "    {\"type\": \"BM25\", \"model_class\": BM25Okapi, \"name\": \"BM25Okapi\"},\n",
    "    {\"type\": \"BM25\", \"name\": \"BM25L\", \"model_class\": BM25L},\n",
    "    {\"type\": \"BM25\", \"name\": \"BM25Plus\", \"model_class\": BM25Plus},\n",
    "    {\"type\": None, \"name\": None, \"model_class\": None},\n",
    "    {\n",
    "        \"type\": \"ColBERT\",\n",
    "        \"name\": \"antoinelouis/colbertv2-camembert-L4-mmarcoFR\",\n",
    "        \"model_class\": None,\n",
    "    },\n",
    "    {\"type\": \"ColBERT\", \"name\": \"bclavie/FraColBERTv2\", \"model_class\": None},\n",
    "    {\"type\": \"ColBERT\", \"name\": \"colbert-ir/colbertv2.0\", \"model_class\": None},\n",
    "]\n",
    "\n",
    "default_reranker = {\"type\": None, \"name\": None, \"model_class\": None}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reranking Experiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define logging display level\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "# set MLFlow exp name\n",
    "EXPERIMENT_NAME = \"RETRIEVER_rerank\"\n",
    "# check if MLFlow URI and S3 endpoints are correctly set up.\n",
    "assert (\n",
    "    \"MLFLOW_TRACKING_URI\" in os.environ\n",
    "), \"Please set the MLFLOW_TRACKING_URI environment variable.\"\n",
    "\n",
    "# Set our tracking server uri for logging\n",
    "mlflow.set_tracking_uri(os.environ[\"MLFLOW_TRACKING_URI\"])\n",
    "# Create a new MLflow Experiment\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "for re_ranker_config in re_ranker_model_test:\n",
    "    with mlflow.start_run():\n",
    "        logging.info(f\"---------- Exp {EXPERIMENT_NAME} model : {re_ranker_config[\"name\"]} \")\n",
    "\n",
    "        params[\"embedding_model\"] = EMBEDDING_MODEL_NAME\n",
    "        params[\"reranker\"] = re_ranker_config[\"name\"]\n",
    "        params[\"markdown_separator\"] = MARKDOWN_SEPARATORS\n",
    "\n",
    "        # Load ref Corpus\n",
    "        logging.info(\"---------- Loading test Corpus data\")\n",
    "        path_data = (\n",
    "            \"/home/onyxia/work/eval_retrieval/insee_documents_sample_ref_retrieval_evaluation.csv\"\n",
    "        )\n",
    "        raw_ref_database = load_sample_data(path_data)\n",
    "\n",
    "        # Load the embedding model to get the max token length\n",
    "        logging.info(f\"---------- Loading Embedding model {EMBEDDING_MODEL_NAME}\")\n",
    "        embedding_model = HuggingFaceEmbeddings(\n",
    "            model_name=EMBEDDING_MODEL_NAME,\n",
    "            multi_process=True,\n",
    "            model_kwargs={\"device\": \"cuda\"},\n",
    "            encode_kwargs={\"normalize_embeddings\": True},  # Set True for cosine similarity\n",
    "            show_progress=False,\n",
    "        )\n",
    "\n",
    "        # get the max token length\n",
    "        embedder_max_token_length = embedding_model.client.get_max_seq_length()\n",
    "        params[\"max_token_length\"] = embedder_max_token_length\n",
    "        params[\"chunck_size\"] = embedder_max_token_length\n",
    "        params[\"overlap_size\"] = int(params[\"chunck_size\"] / 10)\n",
    "\n",
    "        logging.info(\"--------- Chunking LangChain documents\")\n",
    "        logging.info(f\"Max sequence length for {EMBEDDING_MODEL_NAME}: {embedder_max_token_length}\")\n",
    "\n",
    "        # build test corpus\n",
    "        documents = split_documents(\n",
    "            knowledge_base=raw_ref_database,\n",
    "            chunk_size=embedder_max_token_length,\n",
    "            tokenizer_name=EMBEDDING_MODEL_NAME,\n",
    "            params=params,\n",
    "        )\n",
    "        params[\"nb_chunks\"] = len(documents)\n",
    "\n",
    "        logging.info(\"---------- Building Chroma DB\")\n",
    "        # build vector database\n",
    "        collection_name = \"insee_sample_\" + EMBEDDING_MODEL_NAME.split(\"/\")[-1]\n",
    "\n",
    "        db_chroma = Chroma.from_documents(  # no persistance\n",
    "            documents=documents, embedding=embedding_model\n",
    "        )\n",
    "\n",
    "        # load eval data\n",
    "        eval_data_path = \"/home/onyxia/work/eval_retrieval/q_and_s_ref_retrieval_evaluation_Phi-3-mini-128k-instruct.csv\"\n",
    "        eval_data = pd.read_csv(eval_data_path)\n",
    "\n",
    "        # run experiement\n",
    "        logging.info(f\"---------- Evaluating Retrieval Performances for {EMBEDDING_MODEL_NAME}\")\n",
    "        results, metrics = test_retriever(\n",
    "            knowledge_index=db_chroma,\n",
    "            eval_data=eval_data,\n",
    "            embedding_model=embedding_model,\n",
    "            re_ranker_config=re_ranker_config,\n",
    "            params=params,\n",
    "        )\n",
    "\n",
    "        # log hyperparameters\n",
    "        mlflow.log_params(params)\n",
    "\n",
    "        # log metrics\n",
    "        mlflow.log_metrics(metrics)\n",
    "\n",
    "        # log prediction\n",
    "        preds = mlflow.data.from_pandas(results, targets=\"source\", predictions=\"pred\")\n",
    "        mlflow.log_input(preds, context=\"testing\")\n",
    "        logging.info(\"---------- End of Evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Experiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define logging display level\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "# set MLFlow exp name\n",
    "EXPERIMENT_NAME = \"RETRIEVER_embedding_model\"\n",
    "# check if MLFlow URI and S3 endpoints are correctly set up.\n",
    "assert (\n",
    "    \"MLFLOW_TRACKING_URI\" in os.environ\n",
    "), \"Please set the MLFLOW_TRACKING_URI environment variable.\"\n",
    "\n",
    "# Set our tracking server uri for logging\n",
    "mlflow.set_tracking_uri(os.environ[\"MLFLOW_TRACKING_URI\"])\n",
    "# Create a new MLflow Experiment\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "\n",
    "for EMBEDDING_MODEL_NAME in embedding_model_test:\n",
    "    with mlflow.start_run():\n",
    "        logging.info(f\"---------- Exp {EXPERIMENT_NAME} model : {EMBEDDING_MODEL_NAME}\")\n",
    "\n",
    "        params[\"embedding_model\"] = EMBEDDING_MODEL_NAME\n",
    "        params[\"reranker\"] = default_reranker[\"name\"]\n",
    "        params[\"markdown_separator\"] = MARKDOWN_SEPARATORS\n",
    "\n",
    "        # Load ref Corpus\n",
    "        logging.info(\"---------- Loading test Corpus data\")\n",
    "        path_data = (\n",
    "            \"/home/onyxia/work/eval_retrieval/insee_documents_sample_ref_retrieval_evaluation.csv\"\n",
    "        )\n",
    "        raw_ref_database = load_sample_data(path_data)\n",
    "\n",
    "        # Load the embedding model to get the max token length\n",
    "        logging.info(f\"---------- Loading Embedding model {EMBEDDING_MODEL_NAME}\")\n",
    "        embedding_model = HuggingFaceEmbeddings(\n",
    "            model_name=EMBEDDING_MODEL_NAME,\n",
    "            multi_process=True,\n",
    "            model_kwargs={\"device\": \"cuda\"},\n",
    "            encode_kwargs={\"normalize_embeddings\": True},  # Set True for cosine similarity\n",
    "            show_progress=False,\n",
    "        )\n",
    "\n",
    "        # get the max token length\n",
    "        embedder_max_token_length = embedding_model.client.get_max_seq_length()\n",
    "        params[\"max_token_length\"] = embedder_max_token_length\n",
    "        params[\"chunck_size\"] = embedder_max_token_length\n",
    "        params[\"overlap_size\"] = int(params[\"chunck_size\"] / 10)\n",
    "\n",
    "        logging.info(\"--------- Chunking LangChain documents\")\n",
    "        logging.info(f\"Max sequence length for {EMBEDDING_MODEL_NAME}: {embedder_max_token_length}\")\n",
    "\n",
    "        # build test corpus\n",
    "        documents = split_documents(\n",
    "            knowledge_base=raw_ref_database,\n",
    "            chunk_size=embedder_max_token_length,\n",
    "            tokenizer_name=EMBEDDING_MODEL_NAME,\n",
    "            params=params,\n",
    "        )\n",
    "        params[\"nb_chunks\"] = len(documents)\n",
    "\n",
    "        logging.info(\"---------- Building Chroma DB\")\n",
    "        # build vector database\n",
    "        collection_name = \"insee_sample_\" + EMBEDDING_MODEL_NAME.split(\"/\")[-1]\n",
    "\n",
    "        db_chroma = Chroma.from_documents(  # no persistance\n",
    "            documents=documents, embedding=embedding_model, collection_name=collection_name\n",
    "        )\n",
    "\n",
    "        # load eval data\n",
    "        # eval_data_path = \"/home/onyxia/work/eval_retrieval/q_and_s_ref_retrieval_evaluation_Phi-3-mini-128k-instruct.csv\"\n",
    "        eval_data_path = (\n",
    "            \"/home/onyxia/work/eval_retrieval/q_and_a_scored_filtered_Phi-3-mini-128k-instruct.csv\"\n",
    "        )\n",
    "        eval_data = pd.read_csv(eval_data_path)\n",
    "\n",
    "        # run experiement\n",
    "        logging.info(f\"---------- Evaluating Retrieval Performances for {EMBEDDING_MODEL_NAME}\")\n",
    "        results, metrics = test_retriever(\n",
    "            knowledge_index=db_chroma,\n",
    "            eval_data=eval_data,\n",
    "            embedding_model=embedding_model,\n",
    "            re_ranker_config=default_reranker,\n",
    "            params=params,\n",
    "        )\n",
    "\n",
    "        # log hyperparameters\n",
    "        mlflow.log_params(params)\n",
    "\n",
    "        # log metrics\n",
    "        mlflow.log_metrics(metrics)\n",
    "\n",
    "        # log prediction\n",
    "        preds = mlflow.data.from_pandas(results, targets=\"source\", predictions=\"pred\")\n",
    "        mlflow.log_input(preds, context=\"testing\")\n",
    "        logging.info(\"---------- End of Evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the embedding model\n",
    "# define logging display level\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "# set MLFlow exp name\n",
    "EXPERIMENT_NAME = \"RETRIEVER_quantization\"\n",
    "# check if MLFlow URI and S3 endpoints are correctly set up.\n",
    "assert (\n",
    "    \"MLFLOW_TRACKING_URI\" in os.environ\n",
    "), \"Please set the MLFLOW_TRACKING_URI environment variable.\"\n",
    "\n",
    "# Set our tracking server uri for logging\n",
    "mlflow.set_tracking_uri(os.environ[\"MLFLOW_TRACKING_URI\"])\n",
    "# Create a new MLflow Experiment\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "for precision in [\"float32\", \"int8\", \"uint8\", \"binary\", \"ubinary\"]:\n",
    "    with mlflow.start_run():\n",
    "        logging.info(f\"---------- Exp {EXPERIMENT_NAME} quantization : {precision}\")\n",
    "\n",
    "        params[\"embedding_model\"] = EMBEDDING_MODEL_NAME\n",
    "        params[\"reranker\"] = None\n",
    "        params[\"markdown_separator\"] = MARKDOWN_SEPARATORS\n",
    "        params[\"quantization\"] = precision\n",
    "\n",
    "        # Load ref Corpus\n",
    "        logging.info(\"---------- Loading test Corpus data\")\n",
    "        path_data = (\n",
    "            \"/home/onyxia/work/eval_retrieval/insee_documents_sample_ref_retrieval_evaluation.csv\"\n",
    "        )\n",
    "        raw_ref_database = load_sample_data(path_data)\n",
    "\n",
    "        # Load the embedding model to get the max token length\n",
    "        logging.info(f\"---------- Loading Embedding model {EMBEDDING_MODEL_NAME}\")\n",
    "        embedding_model = HuggingFaceEmbeddings(\n",
    "            model_name=EMBEDDING_MODEL_NAME,\n",
    "            multi_process=True,\n",
    "            model_kwargs={\"device\": \"cuda\"},\n",
    "            encode_kwargs={\n",
    "                \"normalize_embeddings\": True,\n",
    "                \"precision\": precision,\n",
    "            },  # Set True for cosine similarity\n",
    "            show_progress=False,\n",
    "        )\n",
    "\n",
    "        # get the max token length\n",
    "        embedder_max_token_length = embedding_model.client.get_max_seq_length()\n",
    "        params[\"max_token_length\"] = embedder_max_token_length\n",
    "        params[\"chunck_size\"] = embedder_max_token_length\n",
    "        params[\"overlap_size\"] = int(params[\"chunck_size\"] / 10)\n",
    "\n",
    "        logging.info(\"--------- Chunking LangChain documents\")\n",
    "        logging.info(f\"Max sequence length for {EMBEDDING_MODEL_NAME}: {embedder_max_token_length}\")\n",
    "\n",
    "        # build test corpus\n",
    "        documents = split_documents(\n",
    "            knowledge_base=raw_ref_database,\n",
    "            chunk_size=embedder_max_token_length,\n",
    "            tokenizer_name=EMBEDDING_MODEL_NAME,\n",
    "            params=params,\n",
    "        )\n",
    "        params[\"nb_chunks\"] = len(documents)\n",
    "\n",
    "        logging.info(\"---------- Building Chroma DB\")\n",
    "        # build vector database\n",
    "        collection_name = \"insee_sample_\" + EMBEDDING_MODEL_NAME.split(\"/\")[-1]\n",
    "\n",
    "        db_chroma = Chroma.from_documents(  # no persistance\n",
    "            documents=documents, embedding=embedding_model\n",
    "        )\n",
    "\n",
    "        # load eval data\n",
    "        eval_data_path = \"/home/onyxia/work/eval_retrieval/q_and_s_ref_retrieval_evaluation_Phi-3-mini-128k-instruct.csv\"\n",
    "        eval_data = pd.read_csv(eval_data_path)\n",
    "\n",
    "        # run experiement\n",
    "        logging.info(f\"---------- Evaluating Retrieval Performances for {EMBEDDING_MODEL_NAME}\")\n",
    "        results, metrics = test_retriever(\n",
    "            knowledge_index=db_chroma,\n",
    "            eval_data=eval_data,\n",
    "            embedding_model=embedding_model,\n",
    "            re_ranker_config=None,\n",
    "            params=params,\n",
    "        )\n",
    "\n",
    "        # log hyperparameters\n",
    "        mlflow.log_params(params)\n",
    "\n",
    "        # log metrics\n",
    "        mlflow.log_metrics(metrics)\n",
    "\n",
    "        # log prediction\n",
    "        preds = mlflow.data.from_pandas(results, targets=\"source\", predictions=\"pred\")\n",
    "        mlflow.log_input(preds, context=\"testing\")\n",
    "        logging.info(\"---------- End of Evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documents' metadata are would have potential a positive impact on retrieval performance. \\\n",
    "Expected results : helps retriever to refine its choices, use a reranker (BM25 or ColBERT model) OR an embedding model to query the retrieved documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_ref_database[0].metadata.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the embedding model\n",
    "# define logging display level\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "# set MLFlow exp name\n",
    "EXPERIMENT_NAME = \"RETRIEVER_metadata\"\n",
    "# check if MLFlow URI and S3 endpoints are correctly set up.\n",
    "assert (\n",
    "    \"MLFLOW_TRACKING_URI\" in os.environ\n",
    "), \"Please set the MLFLOW_TRACKING_URI environment variable.\"\n",
    "\n",
    "# Set our tracking server uri for logging\n",
    "mlflow.set_tracking_uri(os.environ[\"MLFLOW_TRACKING_URI\"])\n",
    "# Create a new MLflow Experiment\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "for metadata in [None, \"title\", \"intertitre\"]:\n",
    "    with mlflow.start_run():\n",
    "        logging.info(f\"---------- Exp {EXPERIMENT_NAME} Metadata : {metadata}\")\n",
    "        params[\"embedding_model\"] = EMBEDDING_MODEL_NAME\n",
    "        params[\"reranker\"] = None\n",
    "        params[\"markdown_separator\"] = MARKDOWN_SEPARATORS\n",
    "        params[\"use_metadata\"] = metadata\n",
    "\n",
    "        # Load ref Corpus\n",
    "        logging.info(\"---------- Loading test Corpus data\")\n",
    "        path_data = (\n",
    "            \"/home/onyxia/work/eval_retrieval/insee_documents_sample_ref_retrieval_evaluation.csv\"\n",
    "        )\n",
    "        raw_ref_database = load_sample_data(path_data=path_data, content_column=\"content\")\n",
    "\n",
    "        # Load the embedding model to get the max token length\n",
    "        logging.info(f\"---------- Loading Embedding model {EMBEDDING_MODEL_NAME}\")\n",
    "        embedding_model = HuggingFaceEmbeddings(\n",
    "            model_name=EMBEDDING_MODEL_NAME,\n",
    "            multi_process=True,\n",
    "            model_kwargs={\"device\": \"cuda\"},\n",
    "            encode_kwargs={\"normalize_embeddings\": True},  # Set True for cosine similarity\n",
    "            show_progress=False,\n",
    "        )\n",
    "\n",
    "        # get the max token length\n",
    "        embedder_max_token_length = embedding_model.client.get_max_seq_length()\n",
    "        params[\"max_token_length\"] = embedder_max_token_length\n",
    "        params[\"chunck_size\"] = embedder_max_token_length\n",
    "        params[\"overlap_size\"] = int(params[\"chunck_size\"] / 10)\n",
    "\n",
    "        logging.info(\"--------- Chunking LangChain documents\")\n",
    "        logging.info(f\"Max sequence length for {EMBEDDING_MODEL_NAME}: {embedder_max_token_length}\")\n",
    "\n",
    "        # build test corpus\n",
    "        documents = split_documents(\n",
    "            knowledge_base=raw_ref_database,\n",
    "            chunk_size=embedder_max_token_length,\n",
    "            tokenizer_name=EMBEDDING_MODEL_NAME,\n",
    "            params=params,\n",
    "        )\n",
    "        params[\"nb_chunks\"] = len(documents)\n",
    "\n",
    "        logging.info(\"---------- Building Chroma DB\")\n",
    "        # build vector database\n",
    "        collection_name = \"insee_sample_\" + EMBEDDING_MODEL_NAME.split(\"/\")[-1]\n",
    "\n",
    "        # since the vector database is already stored\n",
    "        db_chroma = Chroma.from_documents(  # no persistance\n",
    "            documents=documents, embedding=embedding_model\n",
    "        )\n",
    "\n",
    "        # load eval data\n",
    "        eval_data_path = \"/home/onyxia/work/eval_retrieval/q_and_s_ref_retrieval_evaluation_Phi-3-mini-128k-instruct.csv\"\n",
    "        eval_data = pd.read_csv(eval_data_path)\n",
    "\n",
    "        # run experiement\n",
    "        logging.info(f\"---------- Evaluating Retrieval Performances for {EMBEDDING_MODEL_NAME}\")\n",
    "        results, metrics = test_retriever(\n",
    "            knowledge_index=db_chroma,\n",
    "            eval_data=eval_data,\n",
    "            embedding_model=embedding_model,\n",
    "            re_ranker_config=None,\n",
    "            params=params,\n",
    "        )\n",
    "\n",
    "        # log hyperparameters\n",
    "        mlflow.log_params(params)\n",
    "\n",
    "        # log metrics\n",
    "        mlflow.log_metrics(metrics)\n",
    "\n",
    "        # log prediction\n",
    "        preds = mlflow.data.from_pandas(results, targets=\"source\", predictions=\"pred\")\n",
    "        mlflow.log_input(preds, context=\"testing\")\n",
    "        logging.info(\"---------- End of Evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Recall and Context precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Context Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"eval_retrieval/q_and_a_scored_filtered_Phi-3-mini-128k-instruct.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to evaluate Context Recall ? \n",
    "1. Break the ground truth answer into individual statements. \n",
    "2. Verify if each statements are related to the given contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "model_name = \"microsoft/Phi-3-mini-128k-instruct\"  # (to download)\n",
    "\n",
    "# load LLM config\n",
    "config = AutoConfig.from_pretrained(model_name, trust_remote_code=True)\n",
    "# load quantization config\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    config=config,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_statement_generation = \"\"\" \n",
    "<|user|>\n",
    "Pour CHAQUE phrase de la réponse attendue ci-dessous, déterminez si la phrase peut être attribuée aux contextes. Veuillez générer une liste de JSON avec deux clés : « verdict » et « raison ».\n",
    "La clé « verdict » doit STRICTEMENT être un « oui » ou un « non ». Répondez « oui » si la phrase peut être attribuée à n'importe quelle partie du contexte de récupération, sinon répondez « non ».\n",
    "La clé « raison » doit fournir une raison pour le verdict. Dans la raison, vous devez viser à inclure le nombre de nœuds dans le contexte de récupération (par exemple, le 1er nœud et le 2ème nœud dans le contexte de récupération) qui sont attribués à ladite phrase. Vous devez également viser à citer la partie spécifique du contexte de récupération pour justifier votre verdict, mais restez extrêmement concis et raccourcissez la citation avec des points de suspension si possible.\n",
    "**\n",
    "IMPORTANT : assurez-vous de renvoyer uniquement au format JSON, avec la clé \"verdicts\" sous forme de liste d'objets JSON, chacun avec deux clés : \"verdict\" et \"raison\".\n",
    "\n",
    "{{\n",
    " \"verdicts\": [\n",
    " {{\n",
    " \"raison\": \"...\",\n",
    " \"verdict\": \"oui\"\n",
    " \n",
    " }},\n",
    " ...\n",
    " ]\n",
    "}}\n",
    "\n",
    "Puisque vous allez générer un verdict pour chaque phrase, le nombre de « verdicts » DEVRAIT ÊTRE STRICTEMENT ÉGAL au nombre de phrases du « résultat attendu ».\n",
    "**\n",
    "<|end|>\n",
    "<|assistant|>\n",
    "\n",
    "Réponse attendue:\n",
    "{expected_output}\n",
    "\n",
    "Contexte :\n",
    "{retrieval_context}\n",
    "\n",
    "JSON :\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_score(verdicts):\n",
    "    number_of_verdicts = len(verdicts)\n",
    "    if number_of_verdicts == 0:\n",
    "        return 0\n",
    "\n",
    "    justified_sentences = 0\n",
    "    for verdict in verdicts:\n",
    "        if verdict[\"verdict\"].lower() == \"oui\":\n",
    "            justified_sentences += 1\n",
    "\n",
    "    score = justified_sentences / number_of_verdicts\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 2000,\n",
    "    \"return_full_text\": False,\n",
    "    \"do_sample\": False,\n",
    "    \"batch_size\": 1,\n",
    "}\n",
    "\n",
    "verdicts = []\n",
    "results = []\n",
    "for i in range(len(data[:5])):\n",
    "    row = data.iloc[i]\n",
    "    print(f\"context : {row.context} \\n question : {row.question} \\n answer : {row.answer}\")\n",
    "    final_prompt = prompt_statement_generation.format(\n",
    "        expected_output=row.answer, retrieval_context=row.context\n",
    "    )\n",
    "    generated_answer = pipe(final_prompt, **generation_args)\n",
    "    answer = generated_answer[0][\"generated_text\"]\n",
    "    d = json.loads(answer)\n",
    "    try:\n",
    "        verdicts += d\n",
    "        results.append(calculate_score(d))\n",
    "    except Exception as e:\n",
    "        print(\"error :\", e)\n",
    "        print(\"issue at parsing : \", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions generated by GPT4o "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    {\n",
    "        \"source\": \"https://www.insee.fr/fr/statistiques/6047789\",\n",
    "        \"questions\": [\n",
    "            \"Comment les nouvelles mesures sociales et fiscales de 2018 ont-elles globalement affecté le niveau de vie des différentes tranches de population ?\",\n",
    "            \"Quels sont les impacts spécifiques de la bascule des cotisations sociales vers la CSG pour les travailleurs par rapport aux retraités ?\",\n",
    "            \"Pourquoi l'indice de Gini et d'autres indicateurs d'inégalité varient-ils faiblement malgré les réformes de 2018 ?\",\n",
    "            \"En quoi la transformation de l’ISF en IFI et la mise en place du prélèvement forfaitaire unique sur les revenus du patrimoine ont-elles influencé les inégalités économiques en France ?\",\n",
    "            \"Comment les nouvelles mesures fiscales et sociales de 2018 ont-elles globalement affecté le niveau de vie des ménages en France ?\",\n",
    "            \"Quelles méthodes de microsimulation ont été utilisées pour évaluer les effets des réformes socio-fiscales de 2018, et quels sont leurs avantages et limites ?\",\n",
    "            \"Comment les réformes fiscales et sociales de 2018 ont-elles contribué à la réduction (ou à l’augmentation) des inégalités en France ?\",\n",
    "        ],\n",
    "    }\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
