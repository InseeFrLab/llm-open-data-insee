{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Load Q&A test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mc cp s3/projet-llm-insee-open-data/data/eval_data/eval_retrieval/q_and_a_scored_filtered_Phi-3-mini-128k-instruct.csv ../data/q_and_a_ref_retrieval_evaluation_Phi-3-mini-128k-instruct.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Load Knowledge Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mc cp s3/projet-llm-insee-open-data/data/eval_data/eval_retrieval/insee_documents_sample_ref_retrieval_evaluation.csv ../data/insee_documents_sample_ref_retrieval_evaluation.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "#question and answer\n",
    "path_qa = \"../data/q_and_a_ref_retrieval_evaluation_Phi-3-mini-128k-instruct.csv\"\n",
    "test = pd.read_csv(path_qa)\n",
    "\n",
    "df_dict = {}\n",
    "df_dict[\"the_df_dataset\"] = test\n",
    "\n",
    "#knowledge data \n",
    "path_knowledge = \"../data/insee_documents_sample_ref_retrieval_evaluation.csv\"\n",
    "data = pd.read_csv(path_knowledge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(test.head())\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### About test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the evaluator\n",
    "results = RetrievalEvaluator.run(\n",
    "    eval_configurations=eval_configs,\n",
    "    eval_dict=df_dict,  # Ensure 'df_dict' is a dictionary containing pandas DataFrames with the required structure\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_configs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Test 03/07/2024 - Update Retrieval Evaluation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "from typing import List, Dict \n",
    "import math\n",
    "from evaluation import RetrievalConfiguration\n",
    "\n",
    "def plot_results(eval_configs: List[RetrievalConfiguration], results: Dict[str, Dict[str, Dict[int, float]]], \n",
    "                    ir_metrics: List[str] = ['recall', 'precision', 'mrr', 'ndcg'], focus: str = None, \n",
    "                    title: str = \"\", k: int = 15, cmap_name: str = \"tab10\"):\n",
    "    \"\"\"\n",
    "    Plots IR metrics for different retrieval configurations.\n",
    "    \n",
    "    Parameters:\n",
    "    - eval_configs: List of RetrievalConfiguration objects.\n",
    "    - results: Dictionary of results where the keys are configuration names and values are dictionaries \n",
    "               of metrics.\n",
    "    - ir_metrics: List of metrics to plot (default is ['recall', 'precision', 'mrr', 'ndcg']).\n",
    "    - focus: The name of the config parameter to highlight in the legend.\n",
    "    - title: The title of the plot.\n",
    "    - k: The maximum value of k to plot.\n",
    "    \"\"\"\n",
    "\n",
    "    # dynamic plotting  \n",
    "    num_metrics = len(ir_metrics)\n",
    "    num_cols = math.ceil(math.sqrt(num_metrics))\n",
    "    num_rows = math.ceil(num_metrics / num_cols)\n",
    "\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(14, 10))\n",
    "    axes = axes.flatten() if num_metrics > 1 else [axes]\n",
    "\n",
    "    cmap = plt.get_cmap(cmap_name)\n",
    "    colors = cmap(range(len(eval_configs)))\n",
    "    \n",
    "    for i, metric in enumerate(ir_metrics):\n",
    "        ax = axes[i]\n",
    "\n",
    "        for j, config in enumerate(eval_configs):\n",
    "            config_results = results.get(config.name, {})\n",
    "            metric_values = config_results.get(metric, {})\n",
    "            \n",
    "            k_values = [key for key in config.k_values if key <= k]\n",
    "            values = [metric_values.get(ki, None) for ki in k_values]\n",
    "\n",
    "            label = config.get(focus) \n",
    "\n",
    "            if label is None:\n",
    "                label = config.name\n",
    "            ax.plot(k_values, values, marker='o', label=label.split(\"/\")[-1], color=colors[j])\n",
    "        \n",
    "        ax.set_xlabel('k')\n",
    "        ax.set_ylabel(metric.capitalize())\n",
    "        ax.set_title(f'{metric.upper()} vs k')\n",
    "        ax.set_xticks(k_values)\n",
    "        ax.grid(True)\n",
    "        ax.legend()\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.suptitle(title, fontsize=16, y=1.02)\n",
    "    plt.show()\n",
    "    \n",
    "def hist_results(eval_configs: List[RetrievalConfiguration], results: Dict[str, Dict[str, Dict[int, float]]], \n",
    "                 ir_metrics: List[str] = ['recall', 'precision', 'mrr', 'ndcg'], focus: str = None, \n",
    "                 title: str = \"\", k: int = 15, cmap_name : str = 'tab10', x_min=0.6):\n",
    "    \"\"\"\n",
    "    Plots histograms of IR metrics for different retrieval configurations at a given k.\n",
    "    \n",
    "    Parameters:\n",
    "    - eval_configs: List of RetrievalConfiguration objects.\n",
    "    - results: Dictionary of results where the keys are configuration names and values are dictionaries \n",
    "               of metrics.\n",
    "    - ir_metrics: List of metrics to plot (default is ['recall', 'precision', 'mrr', 'ndcg']).\n",
    "    - focus: The name of the config parameter to highlight in the legend.\n",
    "    - title: The title of the plot.\n",
    "    - k: The value of k to plot the histograms for.\n",
    "    \"\"\"\n",
    "    num_metrics = len(ir_metrics)\n",
    "    num_cols = math.ceil(math.sqrt(num_metrics))\n",
    "    num_rows = math.ceil(num_metrics / num_cols)\n",
    "\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(14, 10))\n",
    "    axes = axes.flatten() if num_metrics > 1 else [axes]\n",
    "\n",
    "    cmap = plt.get_cmap(cmap_name)\n",
    "    colors = cmap(range(len(eval_configs)))\n",
    "\n",
    "    for i, metric in enumerate(ir_metrics):\n",
    "        ax = axes[i]\n",
    "        values = []\n",
    "        labels = []\n",
    "        \n",
    "        for j, config in enumerate(eval_configs):\n",
    "            config_results = results.get(config.name, {})\n",
    "            metric_values = config_results.get(metric, {})\n",
    "            if isinstance(metric_values, dict):\n",
    "                value = metric_values.get(k, None)\n",
    "            else:\n",
    "                value = metric_values\n",
    "            \n",
    "            if value is not None:\n",
    "                values.append(value)\n",
    "                label = config.get(focus) \n",
    "                if label is None:\n",
    "                    label = config.name\n",
    "                labels.append(label.split(\"/\")[-1])\n",
    "                ax.barh(labels[-1], values[-1], color=colors[j])\n",
    "        \n",
    "        ax.set_xlabel('Value')\n",
    "        ax.set_ylabel('Configuration')\n",
    "        ax.set_xlim(left=x_min)\n",
    "        ax.set_title(f'{metric.upper()} at k={k}')\n",
    "        ax.grid(True)\n",
    "    \n",
    "    # Hide any unused subplots\n",
    "    for j in range(i + 1, num_rows * num_cols):\n",
    "        fig.delaxes(axes[j])\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.suptitle(title, fontsize=16, y=1.02)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### Test Cross-encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import RetrievalConfiguration, RetrievalEvaluator\n",
    "\n",
    "cross_encoders = [\"BAAI/bge-reranker-v2-m3\", \"antoinelouis/crossencoder-camembert-large-mmarcoFR\", \"dangvantuan/CrossEncoder-camembert-large\",\"antoinelouis/crossencoder-electra-base-french-mmarcoFR\", \"BAAI/bge-reranker-base\"]\n",
    "\n",
    "eval_configs = [RetrievalConfiguration(\n",
    "        name=f'test_cross_encoder_{i}',\n",
    "        database=\"chromadb\",\n",
    "        collection=\"Solon-embeddings-large-0.1_512_51\",\n",
    "        database_path=\"../data/insee_documents_sample_ref_retrieval_evaluation.csv\",\n",
    "        embedding_model_name=\"OrdalieTech/Solon-embeddings-large-0.1\",\n",
    "        reranker_type=\"Cross-encoder\",\n",
    "        reranker_name=cross_encoder,\n",
    "        rerank_k=50,\n",
    "        k_values=[1, 2, 3, 5, 10, 20, 30, 50]\n",
    "    ) for i,cross_encoder in enumerate(cross_encoders)] + [RetrievalConfiguration(\n",
    "        name=f'test_baseline',\n",
    "        database=\"chromadb\",\n",
    "        collection=\"Solon-embeddings-large-0.1_512_51\",\n",
    "        database_path=\"../data/insee_documents_sample_ref_retrieval_evaluation.csv\",\n",
    "        embedding_model_name=\"OrdalieTech/Solon-embeddings-large-0.1\",\n",
    "        reranker_type=None,\n",
    "        reranker_name=None,\n",
    "        rerank_k=50,\n",
    "        k_values=[1, 2, 3, 5, 10, 20, 30, 50]\n",
    "    )]\n",
    "\n",
    "# Run the evaluator\n",
    "results = RetrievalEvaluator.run(\n",
    "    eval_configurations=eval_configs,\n",
    "    eval_dict=df_dict,  # Ensure 'df_dict' is a dictionary containing pandas DataFrames with the required structure\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(\n",
    "    eval_configs, \n",
    "    results[\"the_df_dataset\"], \n",
    "    ir_metrics=['recall', 'precision', 'mrr', 'ndcg'], \n",
    "    focus=\"reranker_name\", \n",
    "    title = \"Rerankers\",\n",
    "    k = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_results(\n",
    "    eval_configs, \n",
    "    results[\"the_df_dataset\"], \n",
    "    ir_metrics=['recall', 'mrr', 'ndcg','runtime'], \n",
    "    focus=\"reranker_name\", \n",
    "    title = \"Rerankers\",\n",
    "    k = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Test ColBERTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "colberts = [\"colbert-ir/colbertv2.0\", \"bclavie/FraColBERTv2\", \"antoinelouis/colbertv2-camembert-L4-mmarcoFR\",\"antoinelouis/colbertv1-camembert-base-mmarcoFR\"]\n",
    "\n",
    "eval_configs_colbert = [RetrievalConfiguration(\n",
    "        name=f'test_colBERT_{i}',\n",
    "        database=\"chromadb\",\n",
    "        collection=\"Solon-embeddings-large-0.1_512_51\",\n",
    "        database_path=\"../data/insee_documents_sample_ref_retrieval_evaluation.csv\",\n",
    "        embedding_model_name=\"OrdalieTech/Solon-embeddings-large-0.1\",\n",
    "        reranker_type=\"ColBERT\",\n",
    "        reranker_name=colbert,\n",
    "        rerank_k=50,\n",
    "        k_values=[1, 2, 3, 5, 10, 15, 20, 25, 30, 35, 45, 50]\n",
    "    ) for i, colbert in enumerate(colberts)] + [RetrievalConfiguration(\n",
    "        name=f'test_baseline',\n",
    "        database=\"chromadb\",\n",
    "        collection=\"Solon-embeddings-large-0.1_512_51\",\n",
    "        database_path=\"../data/insee_documents_sample_ref_retrieval_evaluation.csv\",\n",
    "        embedding_model_name=\"OrdalieTech/Solon-embeddings-large-0.1\",\n",
    "        reranker_type=None,\n",
    "        reranker_name=None,\n",
    "        rerank_k=50,\n",
    "        k_values=[1, 2, 3, 5, 10, 15, 20, 25, 30, 35, 45, 50]\n",
    "    )]\n",
    "\n",
    "# Run the evaluator\n",
    "results_colbert = RetrievalEvaluator.run(\n",
    "    eval_configurations=eval_configs_colbert,\n",
    "    eval_dict=df_dict,  # Ensure 'df_dict' is a dictionary containing pandas DataFrames with the required structure\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(\n",
    "    eval_configs_colbert, \n",
    "    results_colbert[\"the_df_dataset\"], \n",
    "    ir_metrics=['recall', 'precision', 'mrr', 'ndcg'], \n",
    "    focus=\"reranker_name\", \n",
    "    title=\"Rerankers (Solon-embeddings-large-0.1)\",\n",
    "    k = 10\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_configs_colbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_colbert[\"the_df_dataset\"][\"test_colBERT_2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_results(\n",
    "    eval_configs_colbert, \n",
    "    results_colbert[\"the_df_dataset\"], \n",
    "    ir_metrics=['recall', 'mrr', 'ndcg','runtime'], \n",
    "    focus=\"reranker_name\", \n",
    "    title = \"Rerankers (Solon-embeddings-large-0.1)\",\n",
    "    k = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### Test BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_configs_bm25_metadata = [\n",
    "    RetrievalConfiguration(\n",
    "        name=f'test_BM25',\n",
    "        database=\"chromadb\",\n",
    "        collection=\"Solon-embeddings-large-0.1_512_51\",\n",
    "        database_path=\"../data/insee_documents_sample_ref_retrieval_evaluation.csv\",\n",
    "        embedding_model_name=\"OrdalieTech/Solon-embeddings-large-0.1\",\n",
    "        reranker_type=\"BM25\",\n",
    "        reranker_name=None,\n",
    "        rerank_k=50,\n",
    "        k_values=[1, 2, 3, 5, 10, 15, 20, 25, 30, 35, 45, 50]\n",
    "    ),\n",
    "    RetrievalConfiguration(\n",
    "        name=f'test_baseline',\n",
    "        database=\"chromadb\",\n",
    "        collection=\"Solon-embeddings-large-0.1_512_51\",\n",
    "        database_path=\"../data/insee_documents_sample_ref_retrieval_evaluation.csv\",\n",
    "        embedding_model_name=\"OrdalieTech/Solon-embeddings-large-0.1\",\n",
    "        reranker_type=None,\n",
    "        reranker_name=None,\n",
    "        rerank_k=50,\n",
    "        k_values=[1, 2, 3, 5, 10, 15, 20, 25, 30, 35, 45, 50]\n",
    "    )\n",
    "    ] + [\n",
    "        RetrievalConfiguration(\n",
    "            name=f'test_metadata',\n",
    "            database=\"chromadb\",\n",
    "            collection=\"Solon-embeddings-large-0.1_512_51\",\n",
    "            database_path=\"../data/insee_documents_sample_ref_retrieval_evaluation.csv\",\n",
    "            embedding_model_name=\"OrdalieTech/Solon-embeddings-large-0.1\",\n",
    "            reranker_type=\"Metadata\",\n",
    "            use_metadata=meta,\n",
    "            reranker_name=None,\n",
    "            rerank_k=50,\n",
    "            k_values=[1, 2, 3, 5, 10, 15, 20, 25, 30, 35, 45, 50])\n",
    "         for meta in ['title', \"intertitres\", \"themes\",\"subtitle\", \"libelleAffichageGeo\"]\n",
    "    ]\n",
    "    \n",
    "# Run the evaluator\n",
    "results_bm25_metadata = RetrievalEvaluator.run(\n",
    "    eval_configurations=eval_configs_bm25_metadata,\n",
    "    eval_dict=df_dict,  # Ensure 'df_dict' is a dictionary containing pandas DataFrames with the required structure\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(\n",
    "    eval_configs_bm25_metadata, \n",
    "    results_bm25_metadata[\"the_df_dataset\"], \n",
    "    ir_metrics=['recall', 'precision', 'mrr', 'ndcg'], \n",
    "    focus=\"use_metadata\", \n",
    "    title=\"Rerankers (Solon-embeddings-large-0.1)\",\n",
    "    k = 10\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_results(\n",
    "    eval_configs_bm25_metadata, \n",
    "    results_bm25_metadata[\"the_df_dataset\"], \n",
    "    ir_metrics=['recall', 'precision', 'mrr', 'ndcg', 'runtime'], \n",
    "    focus=\"use_metadata\", \n",
    "    title=\"Rerankers (Solon-embeddings-large-0.1)\",\n",
    "    k = 5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "### Test Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import RetrievalConfiguration, RetrievalEvaluator\n",
    "model_embeddings = [\n",
    "    \"sentence-transformers/all-MiniLM-L6-v2\", \n",
    "    \"manu/sentence_croissant_alpha_v0.4\",\n",
    "    \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "    \"OrdalieTech/Solon-embeddings-large-0.1\",\n",
    "    \"intfloat/multilingual-e5-large\",\n",
    "    \"Lajavaness/bilingual-embedding-large\",\n",
    "    ]\n",
    "\n",
    "eval_configs_embeddings= [\n",
    "    RetrievalConfiguration(\n",
    "        name=f'test_embeddings_{i}',\n",
    "        database=\"chromadb\",\n",
    "        collection=model.split(\"/\")[-1],\n",
    "        database_path=\"../data/insee_documents_sample_ref_retrieval_evaluation.csv\",\n",
    "        embedding_model_name=model,\n",
    "        reranker_type=None,\n",
    "        reranker_name=None,\n",
    "        rerank_k=50,\n",
    "        k_values=[1, 2, 3, 5, 10, 15, 20, 25, 30, 35, 45, 50]\n",
    "    ) for i, model in enumerate(model_embeddings)\n",
    "]\n",
    "\n",
    "# Run the evaluator\n",
    "results_embeddings = RetrievalEvaluator.run(\n",
    "    eval_configurations=eval_configs_embeddings,\n",
    "    eval_dict=df_dict,  # Ensure 'df_dict' is a dictionary containing pandas DataFrames with the required structure\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(\n",
    "    eval_configs_embeddings, \n",
    "    results_embeddings[\"the_df_dataset\"], \n",
    "    ir_metrics=['recall', 'precision', 'mrr', 'ndcg'], \n",
    "    focus=\"embedding_model_name\", \n",
    "    title=\"Embedding models\",\n",
    "    k = 50\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_results(\n",
    "    eval_configs_embeddings, \n",
    "    results_embeddings[\"the_df_dataset\"], \n",
    "    ir_metrics=['recall', 'precision', 'mrr', 'ndcg', 'runtime'], \n",
    "    focus=\"embedding_model_name\", \n",
    "    title=\"Embedding models\",\n",
    "    k = 5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "### Ensemble methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequence(start, step, count):\n",
    "    \"\"\"\n",
    "    Generate a sequence of tuples (x, y) such that x + y = 1.\n",
    "    \n",
    "    Parameters:\n",
    "    start (float): Starting value for the sequence\n",
    "    step (float): Increment value for each step in the sequence\n",
    "    count (int): Number of tuples to generate\n",
    "    \n",
    "    Returns:\n",
    "    list: A list of tuples (x, y)\n",
    "    \"\"\"\n",
    "    sequence = []\n",
    "    for i in range(count):\n",
    "        x = start + i * step\n",
    "        y = 1 - x\n",
    "        sequence.append((x, y))\n",
    "    return sequence\n",
    "\n",
    "# Example usage\n",
    "start = 0.0\n",
    "step = 0.1\n",
    "count = 10\n",
    "list_tuples = generate_sequence(start, step, count)\n",
    "print(list_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ensemble_eval_config = [\n",
    "    RetrievalConfiguration(\n",
    "        name='baseline',\n",
    "        database=\"chromadb\",\n",
    "        collection=\"all-MiniLM-L6-v2\",\n",
    "        database_path=\"../data/insee_documents_sample_ref_retrieval_evaluation.csv\",\n",
    "        embedding_model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        reranker_type=None,\n",
    "        reranker_name=None,\n",
    "        k_values=[1, 2, 3, 5, 10, 15, 20,25, 30, 35, 40, 45, 50],\n",
    "        rerank_k= 50\n",
    "    )\n",
    "    ] + [\n",
    "        RetrievalConfiguration(\n",
    "            name=f'ensemble_{i}_{j}',\n",
    "            database=\"chromadb\",\n",
    "            collection=\"all-MiniLM-L6-v2\",\n",
    "            database_path=\"../data/insee_documents_sample_ref_retrieval_evaluation.csv\",\n",
    "            embedding_model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "            reranker_type=\"Ensemble\",\n",
    "            param_ensemble = [\n",
    "                {\"reranker_type\":\"Cross-encoder\",\n",
    "                \"reranker_name\":\"BAAI/bge-reranker-base\",\n",
    "                \"reranker_weight\": i\n",
    "                },\n",
    "                {\"reranker_type\":\"BM25\",\n",
    "                \"reranker_name\": None,\n",
    "                \"reranker_weight\": j\n",
    "                },\n",
    "            ],\n",
    "            k_values=[1, 2, 3, 5, 10, 15, 20,25, 30, 35, 40, 45, 50],\n",
    "            rerank_k= 50\n",
    "        ) for i,j in list_tuples \n",
    "    ]\n",
    "\n",
    "# Run the evaluator\n",
    "results_ensemble = RetrievalEvaluator.run(\n",
    "    eval_configurations=ensemble_eval_config,\n",
    "    eval_dict=df_dict,  # Ensure 'df_dict' is a dictionary containing pandas DataFrames with the required structure\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the evaluator\n",
    "plot_results(\n",
    "    ensemble_eval_config, \n",
    "    results_ensemble[\"the_df_dataset\"], \n",
    "    ir_metrics=['recall', 'precision', 'mrr', 'ndcg'], \n",
    "    focus=\"reranker_name\", \n",
    "    title = \"Embedding (all-MiniLM-L6-v2) Ensemble (bge-reranker-base + BM25)\",\n",
    "    k = 15,\n",
    "    cmap_name = \"tab20\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the evaluator\n",
    "hist_results(\n",
    "    ensemble_eval_config, \n",
    "    results_ensemble[\"the_df_dataset\"], \n",
    "    ir_metrics=['recall', 'mrr', 'ndcg'], \n",
    "    focus=\"reranker_name\", \n",
    "    title = \"Ensemble\",\n",
    "    k = 5,\n",
    "    cmap_name = \"tab20\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "### Multiple dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#question and answer\n",
    "path_qa = \"../data/q_and_s_ref_retrieval_evaluation_Phi-3-mini-128k-instruct.csv\"\n",
    "test = pd.read_csv(path_qa)\n",
    "\n",
    "df_dict[\"the_df_dataset_big\"] = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_encoders = [\"BAAI/bge-reranker-v2-m3\", \"antoinelouis/crossencoder-camembert-large-mmarcoFR\", \"BAAI/bge-reranker-base\"]\n",
    "colberts = [\"bclavie/FraColBERTv2\", \"antoinelouis/colbertv2-camembert-L4-mmarcoFR\"]\n",
    "\n",
    "colbert_vs_cross_encoder_eval_config = [\n",
    "    RetrievalConfiguration(\n",
    "        name=f'cross_encoder_{i}',\n",
    "        database=\"chromadb\",\n",
    "        collection=\"Solon-embeddings-large-0.1\",\n",
    "        database_path=\"../data/insee_documents_sample_ref_retrieval_evaluation.csv\",\n",
    "        embedding_model_name=\"OrdalieTech/Solon-embeddings-large-0.1\",\n",
    "        reranker_type=\"Cross-encoder\",\n",
    "        reranker_name=cross_encoder,\n",
    "        k_values=[1, 2, 3, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50],\n",
    "        rerank_k= 50\n",
    "    ) for i, cross_encoder in enumerate(cross_encoders)] + [\n",
    "    RetrievalConfiguration(\n",
    "        name=f'colbert_{i}',\n",
    "        database=\"chromadb\",\n",
    "        collection=\"Solon-embeddings-large-0.1\",\n",
    "        database_path=\"../data/insee_documents_sample_ref_retrieval_evaluation.csv\",\n",
    "        embedding_model_name=\"OrdalieTech/Solon-embeddings-large-0.1\",\n",
    "        reranker_type=\"ColBERT\",\n",
    "        reranker_name=colbert,\n",
    "        k_values=[1, 2, 3, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50],\n",
    "        rerank_k= 50\n",
    "    ) for i, colbert in enumerate(colberts)]\n",
    "\n",
    "# Run the evaluator\n",
    "results_colbert_vs_cross_encoder = RetrievalEvaluator.run(\n",
    "    eval_configurations = colbert_vs_cross_encoder_eval_config,\n",
    "    eval_dict=df_dict,  # Ensure 'df_dict' is a dictionary containing pandas DataFrames with the required structure\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the evaluator\n",
    "plot_results(\n",
    "    colbert_vs_cross_encoder_eval_config, \n",
    "    results_colbert_vs_cross_encoder[\"the_df_dataset\"], \n",
    "    ir_metrics=['recall', 'mrr', 'ndcg'], \n",
    "    focus=\"reranker_name\", \n",
    "    title = \"Reranker Dataset 77\",\n",
    "    k = 5,\n",
    "    cmap_name = \"tab10\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(\n",
    "    colbert_vs_cross_encoder_eval_config, \n",
    "    results_colbert_vs_cross_encoder[\"the_df_dataset_big\"], \n",
    "    ir_metrics=['recall', 'mrr', 'ndcg'], \n",
    "    focus=\"reranker_name\", \n",
    "    title = \"Embedding (Solon-embeddings-large-0.1) Reranker\",\n",
    "    k = 5,\n",
    "    cmap_name = \"tab10\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the evaluator\n",
    "hist_results(\n",
    "    colbert_vs_cross_encoder_eval_config, \n",
    "    results_colbert_vs_cross_encoder[\"the_df_dataset\"], \n",
    "    ir_metrics=['recall', 'mrr', 'ndcg',\"runtime\"], \n",
    "    focus = \"reranker_name\", \n",
    "    title = \"Reranker Dataset 77\",\n",
    "    k = 5,\n",
    "    cmap_name = \"tab10\",\n",
    "    x_min=0.8\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_results(\n",
    "    colbert_vs_cross_encoder_eval_config, \n",
    "    results_colbert_vs_cross_encoder[\"the_df_dataset_big\"], \n",
    "    ir_metrics=['recall', 'mrr', 'ndcg',\"runtime\"], \n",
    "    focus = \"reranker_name\", \n",
    "    title = \"Reranker Dataset 100\",\n",
    "    k = 5,\n",
    "    cmap_name = \"tab10\",\n",
    "    x_min=0.8\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "## Test 12/07/2024 - Update Retrieval Evaluation process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "Goal:\n",
    "The previous tests were conducted using a dataset comprising the 100 largest available documents from the raw database. As a result, this high-quality content may not accurately reflect the distribution of data in the entire vector database. While we can observe differences between the configurations, it is challenging to determine which combination is the best choice for our use case among the top configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "Several experiments will be done : \n",
    "- add random documents to the base corpus (uniformly distributed)\n",
    "- add other big documents to the base corpus (keeping the same extraction procedure of the top N largest content documents)\n",
    "\n",
    "what to observe : \n",
    "- evolution of the retrieval metrics facing this added noise. \n",
    "- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
