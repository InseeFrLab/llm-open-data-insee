{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exp√©rimentation pour pr√©parer la m√©trique LLM as a judge\n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "<h3 class=\"alert-heading\">Warning</h3>\n",
    "Ce _notebook_ est un brouillon pour l'√©valuation de la qualit√© du retrieval. Pas eu le temps de finir car c'est les vacances üèñÔ∏è\n",
    "</div>\n",
    "\n",
    "D'abord un paquet de code extrait de `run_pipeline` pour avoir un environnement avec les √©l√©ments suivants:\n",
    "\n",
    "- La base de donn√©es vectorielle (√† venir: on r√©cup√®rera direct la BDD compl√®te de S3)\n",
    "- Le retriever brut\n",
    "- Le retriever avec reranker \n",
    "\n",
    "Le bloc de code est long mais y a des choses exploratoires en dessous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-26 13:18:31,728 - INFO - Loading faiss with AVX512 support.\n",
      "2024-07-26 13:18:31,752 - INFO - Successfully loaded faiss with AVX512 support.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import ast\n",
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "from src.chain_building import build_chain_validator\n",
    "from src.chain_building.build_chain import build_chain\n",
    "from src.config import CHATBOT_TEMPLATE, CHROMA_DB_LOCAL_DIRECTORY, RAG_PROMPT_TEMPLATE, S3_BUCKET\n",
    "from src.db_building import build_vector_database, chroma_topk_to_df, load_retriever\n",
    "from src.evaluation import answer_faq_by_bot, compare_performance_reranking, evaluate_question_validator, transform_answers_bot\n",
    "from src.model_building import build_llm_model\n",
    "\n",
    "fs = s3fs.S3FileSystem(client_kwargs={\"endpoint_url\": f\"\"\"https://{os.environ[\"AWS_S3_ENDPOINT\"]}\"\"\"})\n",
    "\n",
    "# INPUT: FAQ THAT WILL BE USED FOR EVALUATION -----------------\n",
    "bucket = \"projet-llm-insee-open-data\"\n",
    "path = \"data/FAQ_site/faq.parquet\"\n",
    "faq = pd.read_parquet(f\"{bucket}/{path}\", filesystem=fs)\n",
    "# Extract all URLs from the 'sources' column\n",
    "faq[\"urls\"] = faq[\"sources\"].str.findall(r\"https?://www\\.insee\\.fr[^\\s]*\").apply(lambda s: \", \".join(s))\n",
    "\n",
    "\n",
    "data_raw_s3_path= \"data/raw_data/applishare_solr_joined.parquet\"\n",
    "collection_name = \"insee_data\"\n",
    "embedding_model = \"OrdalieTech/Solon-embeddings-large-0.1\"\n",
    "db, df_raw = build_vector_database(\n",
    "            data_path=data_raw_s3_path,\n",
    "            persist_directory=CHROMA_DB_LOCAL_DIRECTORY,\n",
    "            collection_name=collection_name,\n",
    "            filesystem=fs,\n",
    "            chunk_size = 512,\n",
    "            chunk_overlap = 100,\n",
    "            max_pages = 20,\n",
    "            embedding_model = embedding_model\n",
    "        )\n",
    "\n",
    "llm, tokenizer = build_llm_model(\n",
    "            model_name=os.getenv(\"LLM_MODEL_NAME\", \"mistralai/Mistral-7B-Instruct-v0.2\"),\n",
    "            quantization_config=True,\n",
    "            config=True,\n",
    "            token=os.getenv(\"HF_TOKEN\"),\n",
    "            streaming=False,\n",
    "            generation_args=None,\n",
    "        )\n",
    "\n",
    "embedding_model = \"OrdalieTech/Solon-embeddings-large-0.1\"\n",
    "retriever, vectorstore = load_retriever(\n",
    "            emb_model_name=\"OrdalieTech/Solon-embeddings-large-0.1\",\n",
    "            vectorstore=db,\n",
    "            persist_directory=CHROMA_DB_LOCAL_DIRECTORY,\n",
    "            retriever_params={\"search_type\": \"similarity\", \"search_kwargs\": {\"k\": 30}},\n",
    "        )\n",
    "\n",
    "validator = build_chain_validator(evaluator_llm=llm, tokenizer=tokenizer)\n",
    "validator_answers = evaluate_question_validator(validator=validator)\n",
    "true_positive_validator = validator_answers.loc[validator_answers[\"real\"], \"real\"].mean()\n",
    "true_negative_validator = 1 - (validator_answers.loc[~validator_answers[\"real\"], \"real\"].mean())\n",
    "\n",
    "\n",
    "# Define a langchain prompt template\n",
    "RAG_PROMPT_TEMPLATE_RERANKER = tokenizer.apply_chat_template(\n",
    "    CHATBOT_TEMPLATE, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"], template=RAG_PROMPT_TEMPLATE_RERANKER\n",
    ")\n",
    "\n",
    "reranking_method = \"BM25\"\n",
    "chain = build_chain(\n",
    "                    retriever=retriever,\n",
    "                    prompt=prompt,\n",
    "                    llm=llm,\n",
    "                    bool_log=False,\n",
    "                    reranker=reranking_method,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "context               Pr√®s de 200 000 habitants r√©sident sur le Terr...\n",
       "question              Quelle √©tait la population du Territoire de la...\n",
       "answer                La population du Territoire de la C√¥te Ouest √©...\n",
       "source_doc                 https://www.insee.fr/fr/statistiques/1293858\n",
       "groundedness_score                                                    4\n",
       "groundedness_eval     Le contexte fournit des informations sur la po...\n",
       "relevance_score                                                     5.0\n",
       "relevance_eval        Cette question est tr√®s utile pour les agents ...\n",
       "standalone_score                                                    5.0\n",
       "standalone_eval       La question demande des informations sp√©cifiqu...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "with fs.open(\"projet-llm-insee-open-data/data/eval_data/eval_dataset.csv\", 'rb') as f:\n",
    "    eval_dataset = pd.read_csv(f)\n",
    "\n",
    "eval_dataset.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "web4g = pd.read_parquet(f\"projet-llm-insee-open-data/{data_raw_s3_path}\", filesystem=fs)\n",
    "valid_urls = web4g['url'].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation du potentiel hallucinatoire sans RAG\n",
    "\n",
    "Id√©e: c'est le baseline : si on fait pas de RAG, est-ce qu'on est mauvais ? Pour √ßa, on peut poser des questions √† un LLM non entra√Æn√© et v√©rifier les sources qu'il nous donne: existence (check si URL existe) voire qualit√© (?) (m√™me cat√©gorie du site que ce qu'on attend ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_no_context = f\"\"\"\n",
    "<s>[INST]\n",
    "Tu es un assistant sp√©cialis√© dans la statistique publique r√©pondant aux questions d'agents de l'INSEE.\n",
    "R√©ponds en Fran√ßais exclusivement. \n",
    "Donne une URL sur le site insee.fr. Si tu proposes des √©tapes de navigation sur le site, donne l'URL final. V√©rifie que les URL que tu proposes existent r√©ellement.\n",
    "\n",
    "---\n",
    "Voici la question √† laquelle tu dois r√©pondre :\n",
    "Question: {question}\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "answer_no_context = llm.invoke(prompt_no_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<s>[INST]\n",
      "Tu es un assistant sp√©cialis√© dans la statistique publique r√©pondant aux questions d'agent de l'INSEE.\n",
      "R√©ponds en Fran√ßais seulement.\n",
      "Utilise les informations obtenues dans le contexte, r√©ponds de mani√®re argument√©e √† la question pos√©e.\n",
      "La r√©ponse doit √™tre d√©velopp√©e et citer ses sources.\n",
      "\n",
      "Si tu ne peux pas induire ta r√©ponse du contexte, ne r√©ponds pas.\n",
      "Voici le contexte sur lequel tu dois baser ta r√©ponse :\n",
      "Contexte: \n",
      "Extracted documents:\n",
      "Document 0:::\n",
      "Pr√®s de 200 000 habitants r√©sident sur le Territoire de la C√¥te Ouest au 1er janvier 2006. La population a continu√© sa croissance au rythme de 1,5 % par an depuis 1999 sous le seul effet d'un exc√©dent de naissances sur les d√©c√®s. Pour la premi√®re fois depuis longtemps les migrations se soldent par un r√©sultat nul. L'habitat s'est l√©g√®rement modifi√© : plus de collectif et moins d'habitat traditionnel, une taille de logement recentr√©e autour des trois et quatre pi√®ces, une offre locative priv√©e qui s'est √©toff√©e. Toutefois les quartiers qui composent les cinq communes de l'Ouest pr√©sentent des caract√©ristiques tr√®s contrast√©es : des zones denses d'habitat social quasiment satur√©es, des zones baln√©aires en plein essor et attractives pour les nouveaux arrivants, des territoires dans les Hauts √† la croissance plus incertaine.\n",
      "        ---\n",
      "Voici la question √† laquelle tu dois r√©pondre :\n",
      "Question: ['Quelle √©tait la population du Territoire de la C√¥te Ouest au 1er janvier 2006 et quel √©tait le taux de croissance annuel de la population depuis 1999?']\n",
      "[/INST]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "relevant_docs = eval_dataset['context'].tolist()[:1]\n",
    "question = eval_dataset['question'].tolist()[:1]\n",
    "\n",
    "# Forcer le contexte\n",
    "context = \"\\nExtracted documents:\\n\"\n",
    "context += \"\".join([f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)])\n",
    "\n",
    "prompt_context_only_good_document = RAG_PROMPT_TEMPLATE.format(question=question, context=context)\n",
    "print(prompt_context_only_good_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.insee.fr/fr/statistiques exists and is reachable.',\n",
       " 'https://www.insee.fr/fr/statistiques/fichier/1110003/tableau/T1_POP_1110003.xls returned status code 500.']"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import requests\n",
    "# Extract all URLs using regular expressions\n",
    "urls = re.findall(r'<(https://www\\.insee\\.fr[^>]+)>', answer_no_context)\n",
    "\n",
    "# Function to check if URLs exist\n",
    "def check_url(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            return f\"{url} exists and is reachable.\"\n",
    "        else:\n",
    "            return f\"{url} returned status code {response.status_code}.\"\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"{url} could not be reached. Error: {e}\"\n",
    "\n",
    "# Check each URL and print the result\n",
    "results = [check_url(url) for url in urls]\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour le moment, je n'utilise pas `prompt_context_only_good_document` car je pense qu'il s'agit d'une m√©trique pour √©valuer la qualit√© de la g√©n√©ration, pas du retrieval. Je pense que √ßa vaut le coup de distinguer ces deux niveaux de contr√¥le qualit√©."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_chain2(hf_embeddings, vectorstore, retriever, prompt, llm):\n",
    "    \"\"\" \n",
    "    Build a LLM chain based on Langchain package and INSEE data \n",
    "    \"\"\"\n",
    "    #Create a Langchain LLM Chain \n",
    "    rag_chain_from_docs = (\n",
    "        RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
    "        | prompt\n",
    "        | llm \n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    return rag_chain_from_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation sans/avec reranking\n",
    "\n",
    "Le bout de code ci-dessous vise √† fournir un premier canevas pour √©valuer la capacit√© du mod√®le √† faire du bon _retrieval_ sans ou avec reranking.  \n",
    "\n",
    "C'est pas fini mais y a les vacances qui arrivent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = llm\n",
    "knowledge_index = vectorstore\n",
    "output_file = \"test_generated_ans.json\"\n",
    "reranker = None\n",
    "verbose=True\n",
    "test_settings = os.getenv(\"LLM_MODEL_NAME\", \"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "batch_size = 5\n",
    "\n",
    "batch_questions = eval_dataset['question'].iloc[:5].tolist()\n",
    "\n",
    "answer_no_reranking_context_complete = []\n",
    "answer_reranking_context_complete = []\n",
    "for questions in batch_questions:\n",
    "    answer_no_reranking_context_complete.append(\n",
    "        retriever.invoke(questions)\n",
    "    )\n",
    "    answer_reranking_context_complete.append(\n",
    "        chain.invoke(questions)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'page_content'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[88], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retrieved_docs_df\u001b[38;5;241m.\u001b[39mhead(k)\n\u001b[1;32m     17\u001b[0m answer_no_reranking_context_complete \u001b[38;5;241m=\u001b[39m check_answer_is_expected(answer_no_reranking_context_complete)\n\u001b[0;32m---> 18\u001b[0m answer_reranking_context_complete \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_answer_is_expected\u001b[49m\u001b[43m(\u001b[49m\u001b[43manswer_reranking_context_complete\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[88], line 9\u001b[0m, in \u001b[0;36mcheck_answer_is_expected\u001b[0;34m(answer_no_reranking_context_complete, idx, k)\u001b[0m\n\u001b[1;32m      7\u001b[0m result_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m retrieved_docs:\n\u001b[0;32m----> 9\u001b[0m     row \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpage_content\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpage_content\u001b[49m}\n\u001b[1;32m     10\u001b[0m     row\u001b[38;5;241m.\u001b[39mupdate(doc\u001b[38;5;241m.\u001b[39mmetadata)\n\u001b[1;32m     11\u001b[0m     result_list\u001b[38;5;241m.\u001b[39mappend(row)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'page_content'"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "def check_answer_is_expected(answer_no_reranking_context_complete, idx = 0, k = 5):\n",
    "    if isinstance(answer_no_reranking_context_complete, dict):\n",
    "        answer_no_reranking_context_complete = answer_no_reranking_context_complete['context']\n",
    "    retrieved_docs = answer_no_reranking_context_complete[idx]\n",
    "    for docs in retrieved_docs:\n",
    "        result_list = []\n",
    "        for doc in retrieved_docs:\n",
    "            row = {\"page_content\": doc.page_content}\n",
    "            row.update(doc.metadata)\n",
    "            result_list.append(row)\n",
    "    retrieved_docs_df = pd.DataFrame(result_list)\n",
    "    retrieved_docs_df['expected_url'] = (retrieved_docs_df['url'] == eval_dataset['source_doc'][idx])\n",
    "    retrieved_docs_df['question'] = questions[idx]\n",
    "    return retrieved_docs_df.head(k)\n",
    "\n",
    "answer_no_reranking_context_complete = check_answer_is_expected(answer_no_reranking_context_complete)\n",
    "answer_reranking_context_complete = check_answer_is_expected(answer_reranking_context_complete)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
