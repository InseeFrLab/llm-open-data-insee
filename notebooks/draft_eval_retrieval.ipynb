{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exp√©rimentation pour pr√©parer la m√©trique LLM as a judge\n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "<h3 class=\"alert-heading\">Warning</h3>\n",
    "Ce _notebook_ est un brouillon pour l'√©valuation de la qualit√© du retrieval. Pas eu le temps de finir car c'est les vacances üèñÔ∏è\n",
    "</div>\n",
    "\n",
    "D'abord un paquet de code extrait de `run_pipeline` pour avoir un environnement avec les √©l√©ments suivants:\n",
    "\n",
    "- La base de donn√©es vectorielle (√† venir: on r√©cup√®rera direct la BDD compl√®te de S3)\n",
    "- Le retriever brut\n",
    "- Le retriever avec reranker \n",
    "\n",
    "Le bloc de code est long mais y a des choses exploratoires en dessous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import ast\n",
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "from src.chain_building import build_chain_validator\n",
    "from src.chain_building.build_chain import build_chain\n",
    "from src.config import CHATBOT_TEMPLATE, CHROMA_DB_LOCAL_DIRECTORY, RAG_PROMPT_TEMPLATE, S3_BUCKET\n",
    "from src.db_building import build_vector_database, chroma_topk_to_df, load_retriever\n",
    "from src.evaluation import answer_faq_by_bot, compare_performance_reranking, evaluate_question_validator, transform_answers_bot\n",
    "from src.model_building import build_llm_model\n",
    "\n",
    "fs = s3fs.S3FileSystem(client_kwargs={\"endpoint_url\": f\"\"\"https://{os.environ[\"AWS_S3_ENDPOINT\"]}\"\"\"})\n",
    "\n",
    "# INPUT: FAQ THAT WILL BE USED FOR EVALUATION -----------------\n",
    "bucket = \"projet-llm-insee-open-data\"\n",
    "path = \"data/FAQ_site/faq.parquet\"\n",
    "faq = pd.read_parquet(f\"{bucket}/{path}\", filesystem=fs)\n",
    "# Extract all URLs from the 'sources' column\n",
    "faq[\"urls\"] = faq[\"sources\"].str.findall(r\"https?://www\\.insee\\.fr[^\\s]*\").apply(lambda s: \", \".join(s))\n",
    "\n",
    "\n",
    "data_raw_s3_path= \"data/raw_data/applishare_solr_joined.parquet\"\n",
    "collection_name = \"insee_data\"\n",
    "embedding_model = \"OrdalieTech/Solon-embeddings-large-0.1\"\n",
    "db, df_raw = build_vector_database(\n",
    "            data_path=data_raw_s3_path,\n",
    "            persist_directory=CHROMA_DB_LOCAL_DIRECTORY,\n",
    "            collection_name=collection_name,\n",
    "            filesystem=fs,\n",
    "            chunk_size = 512,\n",
    "            chunk_overlap = 100,\n",
    "            max_pages = 20,\n",
    "            embedding_model = embedding_model\n",
    "        )\n",
    "\n",
    "llm, tokenizer = build_llm_model(\n",
    "            model_name=os.getenv(\"LLM_MODEL_NAME\", \"mistralai/Mistral-7B-Instruct-v0.2\"),\n",
    "            quantization_config=True,\n",
    "            config=True,\n",
    "            token=os.getenv(\"HF_TOKEN\"),\n",
    "            streaming=False,\n",
    "            generation_args=None,\n",
    "        )\n",
    "\n",
    "embedding_model = \"OrdalieTech/Solon-embeddings-large-0.1\"\n",
    "retriever, vectorstore = load_retriever(\n",
    "            emb_model_name=\"OrdalieTech/Solon-embeddings-large-0.1\",\n",
    "            vectorstore=db,\n",
    "            persist_directory=CHROMA_DB_LOCAL_DIRECTORY,\n",
    "            retriever_params={\"search_type\": \"similarity\", \"search_kwargs\": {\"k\": 30}},\n",
    "        )\n",
    "\n",
    "validator = build_chain_validator(evaluator_llm=llm, tokenizer=tokenizer)\n",
    "validator_answers = evaluate_question_validator(validator=validator)\n",
    "true_positive_validator = validator_answers.loc[validator_answers[\"real\"], \"real\"].mean()\n",
    "true_negative_validator = 1 - (validator_answers.loc[~validator_answers[\"real\"], \"real\"].mean())\n",
    "\n",
    "\n",
    "# Define a langchain prompt template\n",
    "RAG_PROMPT_TEMPLATE_RERANKER = tokenizer.apply_chat_template(\n",
    "    CHATBOT_TEMPLATE, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"], template=RAG_PROMPT_TEMPLATE_RERANKER\n",
    ")\n",
    "\n",
    "reranking_method = \"BM25\"\n",
    "chain = build_chain(\n",
    "                    retriever=retriever,\n",
    "                    prompt=prompt,\n",
    "                    llm=llm,\n",
    "                    bool_log=False,\n",
    "                    reranker=reranking_method,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "with fs.open(\"projet-llm-insee-open-data/data/eval_data/eval_dataset.csv\", 'rb') as f:\n",
    "    eval_dataset = pd.read_csv(f)\n",
    "\n",
    "eval_dataset.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web4g = pd.read_parquet(f\"projet-llm-insee-open-data/{data_raw_s3_path}\", filesystem=fs)\n",
    "valid_urls = web4g['url'].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation du potentiel hallucinatoire sans RAG\n",
    "\n",
    "Id√©e: c'est le baseline : si on fait pas de RAG, est-ce qu'on est mauvais ? Pour √ßa, on peut poser des questions √† un LLM non entra√Æn√© et v√©rifier les sources qu'il nous donne: existence (check si URL existe) voire qualit√© (?) (m√™me cat√©gorie du site que ce qu'on attend ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_no_context = f\"\"\"\n",
    "<s>[INST]\n",
    "Tu es un assistant sp√©cialis√© dans la statistique publique r√©pondant aux questions d'agents de l'INSEE.\n",
    "R√©ponds en Fran√ßais exclusivement. \n",
    "Donne une URL sur le site insee.fr. Si tu proposes des √©tapes de navigation sur le site, donne l'URL final. V√©rifie que les URL que tu proposes existent r√©ellement.\n",
    "\n",
    "---\n",
    "Voici la question √† laquelle tu dois r√©pondre :\n",
    "Question: {question}\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "answer_no_context = llm.invoke(prompt_no_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_docs = eval_dataset['context'].tolist()[:1]\n",
    "question = eval_dataset['question'].tolist()[:1]\n",
    "\n",
    "# Forcer le contexte\n",
    "context = \"\\nExtracted documents:\\n\"\n",
    "context += \"\".join([f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)])\n",
    "\n",
    "prompt_context_only_good_document = RAG_PROMPT_TEMPLATE.format(question=question, context=context)\n",
    "print(prompt_context_only_good_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "# Extract all URLs using regular expressions\n",
    "urls = re.findall(r'<(https://www\\.insee\\.fr[^>]+)>', answer_no_context)\n",
    "\n",
    "# Function to check if URLs exist\n",
    "def check_url(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            return f\"{url} exists and is reachable.\"\n",
    "        else:\n",
    "            return f\"{url} returned status code {response.status_code}.\"\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"{url} could not be reached. Error: {e}\"\n",
    "\n",
    "# Check each URL and print the result\n",
    "results = [check_url(url) for url in urls]\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour le moment, je n'utilise pas `prompt_context_only_good_document` car je pense qu'il s'agit d'une m√©trique pour √©valuer la qualit√© de la g√©n√©ration, pas du retrieval. Je pense que √ßa vaut le coup de distinguer ces deux niveaux de contr√¥le qualit√©."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_chain2(hf_embeddings, vectorstore, retriever, prompt, llm):\n",
    "    \"\"\" \n",
    "    Build a LLM chain based on Langchain package and INSEE data \n",
    "    \"\"\"\n",
    "    #Create a Langchain LLM Chain \n",
    "    rag_chain_from_docs = (\n",
    "        RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
    "        | prompt\n",
    "        | llm \n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    return rag_chain_from_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation sans/avec reranking\n",
    "\n",
    "Le bout de code ci-dessous vise √† fournir un premier canevas pour √©valuer la capacit√© du mod√®le √† faire du bon _retrieval_ sans ou avec reranking.  \n",
    "\n",
    "C'est pas fini mais y a les vacances qui arrivent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = llm\n",
    "knowledge_index = vectorstore\n",
    "output_file = \"test_generated_ans.json\"\n",
    "reranker = None\n",
    "verbose=True\n",
    "test_settings = os.getenv(\"LLM_MODEL_NAME\", \"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "batch_size = 5\n",
    "\n",
    "batch_questions = eval_dataset['question'].iloc[:5].tolist()\n",
    "\n",
    "answer_no_reranking_context_complete = []\n",
    "answer_reranking_context_complete = []\n",
    "for questions in batch_questions:\n",
    "    answer_no_reranking_context_complete.append(\n",
    "        retriever.invoke(questions)\n",
    "    )\n",
    "    answer_reranking_context_complete.append(\n",
    "        chain.invoke(questions)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "def check_answer_is_expected(answer_no_reranking_context_complete, idx = 0, k = 5):\n",
    "    if isinstance(answer_no_reranking_context_complete, dict):\n",
    "        answer_no_reranking_context_complete = answer_no_reranking_context_complete['context']\n",
    "    retrieved_docs = answer_no_reranking_context_complete[idx]\n",
    "    for docs in retrieved_docs:\n",
    "        result_list = []\n",
    "        for doc in retrieved_docs:\n",
    "            row = {\"page_content\": doc.page_content}\n",
    "            row.update(doc.metadata)\n",
    "            result_list.append(row)\n",
    "    retrieved_docs_df = pd.DataFrame(result_list)\n",
    "    retrieved_docs_df['expected_url'] = (retrieved_docs_df['url'] == eval_dataset['source_doc'][idx])\n",
    "    retrieved_docs_df['question'] = questions[idx]\n",
    "    return retrieved_docs_df.head(k)\n",
    "\n",
    "answer_no_reranking_context_complete = check_answer_is_expected(answer_no_reranking_context_complete)\n",
    "answer_reranking_context_complete = check_answer_is_expected(answer_reranking_context_complete)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
