{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a Complete Database "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- date : 24/05/2024\n",
    "- New features : generate a complete database using \"OrdalieTech/Solon-embeddings-large-0.1\" embedding model (config file have been improved)\n",
    "- Expected improvement : improving the retrieval capabilities with a much strong embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building New Complete Dataset based on config files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from db_building import  build_database_from_csv\n",
    "#db = build_database_from_csv('/home/onyxia/work/llm-open-data-insee/data_complete.csv')\n",
    "#db.similarity_search(\"Quels sont les chiffres du chômages en 2023\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Dataset based on config files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from db_building import reload_database_from_local_dir\n",
    "db = reload_database_from_local_dir(persist_directory=\"/home/onyxia/work/llm-open-data-insee/data/chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if there are at least one encoded document in our vectorstore\n",
    "print(len(db.get()[\"ids\"])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = db.similarity_search(\"Quels résultats au BAC les étudiants de classes préparatoires ont ils généralement?\", k = 5 )\n",
    "print(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import RAG_PROMPT_TEMPLATE, EMB_MODEL_NAME, MODEL_NAME\n",
    "from model_building import build_llm_model\n",
    "from chain_building.build_chain import (\n",
    "    load_retriever,\n",
    "    build_chain\n",
    "    )\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import chainlit as cl\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "def build_chain_test(retriever, prompt, llm):\n",
    "    \"\"\"\n",
    "    Build a LLM chain based on Langchain package and INSEE data\n",
    "    \"\"\"\n",
    "    # Create a Langchain LLM Chain\n",
    "    chain = (\n",
    "        RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    rag_chain_with_source = RunnableParallel(\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    ).assign(answer=chain)\n",
    "\n",
    "    return rag_chain_with_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=RAG_PROMPT_TEMPLATE)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever(search_type=\"mmr\", search_kwargs={\"score_threshold\": 0.5, \"k\": 5})\n",
    "\n",
    "#retriever = load_retriever(emb_model_name=EMB_MODEL_NAME,\n",
    "                        #persist_directory=\"/home/onyxia/work/llm-open-data-insee/data/chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = build_llm_model(model_name=MODEL_NAME,\n",
    "                        quantization_config=True,\n",
    "                        config=True, \n",
    "                        token = os.environ[\"HF_TOKEN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = build_chain_test(retriever, prompt, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Quel est le but initial derrière la création du système de retraites français après la Seconde Guerre mondiale?\" \n",
    "#question = \"Quelle est la cause principale de l'augmentation de l'indice des prix à la consommation (IPC)?\"\n",
    "results = retriever.invoke(question)\n",
    "\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"Doc {i} : {doc.metadata[\"source\"]}\")\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in chain.stream(question):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = chain.invoke(question) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answer[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a Reranker "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this part is to build a pipeline Langchain where we have added a reranker: a BM25, a ColBERT model, a french cross-encoder, a multilingual cross-encoder and several hyperparameters.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reranker model list : \n",
    "- multilingual cross encoder : BAAI/bge-reranker-large (multilingual),\n",
    "- french cross encoder : antoinelouis/crossencoder-electra-base-french-mmarcoFR  OR dangvantuan/CrossEncoder-camembert-large\n",
    "- BM25 : langchain_community.retrievers import BM25Retriever\n",
    "- ColBERT : antoinelouis/colbertv2-camembert-L4-mmarcoFR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mc cp s3/projet-llm-insee-open-data/data/chroma_database/chroma_db /home/onyxia/work/llm-open-data-insee/data --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/home/onyxia/work/llm-open-data-insee/src\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chain_building import (\n",
    "    load_retriever\n",
    "    )\n",
    "from config import EMB_MODEL_NAME, MODEL_NAME\n",
    "\n",
    "retriever = load_retriever(\n",
    "                emb_model_name = EMB_MODEL_NAME,\n",
    "                persist_directory=\"/home/onyxia/work/llm-open-data-insee/data/chroma_db\",\n",
    "                device=\"cuda\",\n",
    "                collection_name=\"insee_data\"\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_docs(docs):\n",
    "    print(\n",
    "        f\"\\n{'-' * 100}\\n\".join(\n",
    "            [f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test embedding retriever \n",
    "question = \"Comment est calculé le pouvoir d'achat ?\" \n",
    "#question = \"Quelle est la cause principale de l'augmentation de l'indice des prix à la consommation (IPC)?\"\n",
    "results = retriever.invoke(question)\n",
    "pretty_print_docs(results) #OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder #CrossEncoder \n",
    "from ragatouille import RAGPretrainedModel #ColBERT\n",
    "from langchain_community.retrievers import BM25Retriever #BM25\n",
    "\n",
    "colBERT = RAGPretrainedModel.from_pretrained(\"antoinelouis/colbertv2-camembert-L4-mmarcoFR\")\n",
    "colBERT_retriever  = ContextualCompressionRetriever(base_compressor=colBERT.as_langchain_document_compressor(k=5), base_retriever=retriever)\n",
    "\n",
    "compressed_docs = colBERT_retriever.invoke(question)\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HuggingFaceCrossEncoder(model_name=\"dangvantuan/CrossEncoder-camembert-large\") #\"antoinelouis/crossencoder-electra-base-french-mmarcoFR\")\n",
    "compressor_1 = CrossEncoderReranker(model=model, top_n=5)\n",
    "compression_retriever_1 = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor_1, base_retriever=retriever\n",
    ")\n",
    "\n",
    "compressed_docs = compression_retriever.invoke(question)\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HuggingFaceCrossEncoder(model_name= \"BAAI/bge-reranker-large\")\n",
    "compressor_2 = CrossEncoderReranker(model=model, top_n=5)\n",
    "compression_retriever_2 = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor_2, base_retriever=retriever\n",
    ")\n",
    "\n",
    "compressed_docs = compression_retriever.invoke(question)\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import EnsembleRetriever\n",
    "\n",
    "ensemble_retriever = EnsembleRetriever(retrievers = [compression_retriever_1, compression_retriever_2, colBERT_retriever], weigths = [1/3,1/3,1/3])\n",
    "\n",
    "compressed_docs = ensemble_retriever.invoke(question)\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List, Optional, Sequence, Dict\n",
    "from langchain.retrievers.document_compressors.base import BaseDocumentCompressor\n",
    "from langchain.schema import Document\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.runnables import RunnableLambda, RunnableParallel , RunnablePassthrough\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "from langchain_core.callbacks import Callbacks\n",
    "import math\n",
    "from collections import Counter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Define the compression function\n",
    "def compress_documents_lambda(documents: Sequence[Document], query: str, k: int = 5, **kwargs: Dict[str, Any]) -> Sequence[Document]:\n",
    "    \"\"\"Compress retrieved documents given the query context.\"\"\"\n",
    "\n",
    "    # Initialize the retriever with the documents\n",
    "    retriever = BM25Retriever.from_documents(documents, k=k, **kwargs)\n",
    "    relevant_docs = retriever.get_relevant_documents(query)\n",
    "    return relevant_docs\n",
    "\n",
    "# Define the complete chain\n",
    "bm25_retriever = (\n",
    "    RunnableParallel(\n",
    "    {\"documents\": retriever, \"query\": RunnablePassthrough()}\n",
    "    ) \n",
    "    | RunnableLambda(lambda r : compress_documents_lambda(documents= r[\"documents\"] , query = r[\"query\"]))\n",
    ")\n",
    "\n",
    "bm25_retriever.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder #CrossEncoder \n",
    "\n",
    "model = HuggingFaceCrossEncoder(model_name=\"dangvantuan/CrossEncoder-camembert-large\") #\"antoinelouis/crossencoder-electra-base-french-mmarcoFR\")\n",
    "compressor_1 = CrossEncoderReranker(model=model, top_n=5)\n",
    "\n",
    "compression_retriever_cross_encoder = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor_1, base_retriever=retriever\n",
    ")\n",
    "\n",
    "emsemble_reranking = EnsembleRetriever(retrievers = [compression_retriever_cross_encoder, bm25_retriever], weigths = [0.5, 0.5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emsemble_reranking.invoke(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a LLM Reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    pipeline,\n",
    "    BitsAndBytesConfig,\n",
    "    TextStreamer, \n",
    "    pipeline,\n",
    "    TextStreamer\n",
    ")\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "# quantization config \n",
    "quantization_config  = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=\"float16\",\n",
    "            bnb_4bit_use_double_quant=False,\n",
    "        )\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quantization_config,\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-grained label reranker "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reranking utils function \n",
    "\n",
    "def expected_relevance_values(logits, grades_token_ids, list_grades):\n",
    "    next_token_logits = logits[:, -1, :]\n",
    "    next_token_logits = next_token_logits.cpu()[0]\n",
    "    probabilities = F.softmax(next_token_logits[grades_token_ids], dim=-1).numpy()\n",
    "    return np.dot(np.array(list_grades), probabilities)\n",
    "\n",
    "def peak_relevance_likelihood(logits, grades_token_ids, list_grades):\n",
    "    index_max_grade = np.array(list_grades).argmax()\n",
    "    next_token_logits = logits[:, -1, :]\n",
    "    probabilities = F.softmax(next_token_logits, dim=-1).cpu().numpy()[0]\n",
    "    return probabilities[grades_token_ids[index_max_grade]]\n",
    "\n",
    "def find_sublist_indices(main_list, sublist):\n",
    "    sublist_length = len(sublist)\n",
    "    main_list_length = len(main_list)\n",
    "    \n",
    "    # Helper function to check if sublist matches\n",
    "    def is_sublist_at_index(index):\n",
    "        return main_list[index:index + sublist_length] == sublist\n",
    "    \n",
    "    # Finding the first index\n",
    "    first_index = -1\n",
    "    \n",
    "    for i in range(main_list_length - sublist_length + 1):\n",
    "        if is_sublist_at_index(i):\n",
    "            first_index = i\n",
    "            break\n",
    "\n",
    "    return first_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## assessing methods \n",
    "def RG_S(tokenizer, model,query, document, aggregating_method, k=5):\n",
    "\n",
    "    list_grades = list(range(k))\n",
    "    grades_token_ids = [tokenizer(str(grade))[\"input_ids\"][1] for grade in list_grades]\n",
    " \n",
    "    RG_S_template = \"\"\"\n",
    "    Sur une échelle de 0 à {k}, jugez la pertinence entre la requête et le document.\n",
    "    Requête : {query}\n",
    "    Document : {document}\n",
    "    Réponse : \"\"\" \n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"Tu es un assistant chatbot expert en Statistique Publique.\"},\n",
    "        {\"role\": \"user\", \"content\": RG_S_template.format(query=query, document=document, k=k)},\n",
    "    ]\n",
    "\n",
    "    input_text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=False\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(input_text, return_tensors='pt').to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    return aggregating_method(logits, grades_token_ids, list_grades)\n",
    "\n",
    "\n",
    "def RG_4L(tokenizer, model,query, document, args):\n",
    "    possible_judgements = [\" Parfaitement Pertinent\", \" Très Pertinent\", \" Assez Pertinent\", \" Non Pertinent\"]\n",
    "    list_grades = np.array([3, 2, 1, 0])\n",
    "    RG_4L_template = \"\"\"\n",
    "    Evaluez la pertinence du document donné par rapport à la question posée.\n",
    "    Répondez uniquement parmi : Parfaitement Pertinent, Très Pertinent, Assez Pertinent ou Non Pertinent.\n",
    "    Requête : {query}\n",
    "    Document : {document}\n",
    "    Réponse : {judgement}\"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"Tu es un assistant chatbot expert en Statistique Publique.\"},\n",
    "        {\"role\": \"user\", \"content\": RG_4L_template},\n",
    "    ]\n",
    "\n",
    "    log_probs = []\n",
    "    for judgement in possible_judgements:\n",
    "        input_text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=False,\n",
    "            tokenize=False\n",
    "        ).format(query=query, document=document, judgement=judgement)\n",
    "        log_probs.append(compute_sequence_log_probs(sequence=input_text))\n",
    "\n",
    "    probs = F.softmax(torch.tensor(log_probs), dim=-1).numpy()\n",
    "    return np.dot(probs, list_grades)\n",
    "\n",
    "def RG_3L(tokenizer, model,query, document, args):\n",
    "    possible_judgements = [\" Très Pertinent\", \" Assez Pertinent\", \" Non Pertinent\"]\n",
    "    list_grades = np.array([2, 1, 0])\n",
    "    RG_3L_template = \"\"\"\n",
    "    Evaluez la pertinence du document donné par rapport à la question posée.\n",
    "    Répondez uniquement parmi : Très Pertinent, Assez Pertinent ou Non Pertinent.\n",
    "    Requête : {query}\n",
    "    Document : {document}\n",
    "    Réponse : {judgement}\"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"Tu es un assistant chatbot expert en Statistique Publique.\"},\n",
    "        {\"role\": \"user\", \"content\": RG_3L_template},\n",
    "    ]\n",
    "\n",
    "    log_probs = []\n",
    "    for judgement in possible_judgements:\n",
    "        input_text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=False,\n",
    "            tokenize=False\n",
    "        ).format(query=query, document=document, judgement=judgement)\n",
    "        log_probs.append(compute_sequence_log_probs(sequence=input_text))\n",
    "\n",
    "    probs = F.softmax(torch.tensor(log_probs), dim=-1).numpy()\n",
    "    return np.dot(probs, list_grades)\n",
    "    \n",
    "def RG_YN(tokenizer, model,query, document, aggregating_method):\n",
    "    list_judgements = [\" Oui\", \" Non\"]\n",
    "    grades_token_ids = [tokenizer(j)[\"input_ids\"][1] for j in list_judgements]\n",
    "    list_grades = [1, 0]\n",
    "\n",
    "    RG_YN_template = \"\"\"\n",
    "    Pour la requête et le document suivants, jugez s'ils sont pertinents. Répondez UNIQUEMENT par Oui ou Non.\n",
    "    Requête : {query}\n",
    "    Document : {document}\n",
    "    Réponse : \"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"Tu es un assistant chatbot expert en Statistique Publique.\"},\n",
    "        {\"role\": \"user\", \"content\": RG_YN_template.format(query=query, document=document)},\n",
    "    ]\n",
    "\n",
    "    input_text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=False\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(input_text, return_tensors='pt').to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    return aggregating_method(logits, grades_token_ids, list_grades)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_reranking(tokenizer, model,  query, retrieved_documents, assessing_method, aggregating_method):\n",
    "    docs_content = retrieved_documents.copy() #[doc.page_content for doc in retrieved_documents]\n",
    "\n",
    "    scores = []\n",
    "    for document in docs_content:  \n",
    "        score = assessing_method(tokenizer, model, query, document, aggregating_method)\n",
    "        scores.append(score)\n",
    "\n",
    "    docs_with_scores = list(zip(retrieved_documents, scores))\n",
    "    docs_with_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    sorted_documents = [doc for doc, score in docs_with_scores] #docs_with_scores \n",
    "    return sorted_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_relevance_values(query=\"Combien y a t il d'habitant en France en 2024?\", document=\"Le nombre d'habitant en Birmanie est de 54 millions de personnes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Comment le taux de chômage en France a-t-il évolué au cours des dix dernières années ?\"\n",
    "\n",
    "documents = [\n",
    "    \"Le taux de chômage en France a connu des variations significatives au cours des dix dernières années. En 2014, le taux de chômage s'élevait à environ 10 %. Après un pic en 2015 à près de 10,5 %, il a progressivement diminué pour atteindre 8 % en 2019. La crise sanitaire de 2020 a provoqué une hausse temporaire du chômage, mais les réformes économiques ont permis de ramener le taux à 7,8 % en 2023.\",\n",
    "    \"Selon les données de l'INSEE, le taux de chômage en France a fluctué entre 10 % et 7,8 % au cours des dix dernières années. Après une hausse notable en 2015, des mesures gouvernementales ont contribué à une baisse progressive. Cependant, la pandémie de COVID-19 en 2020 a inversé cette tendance temporairement avant de redescendre en 2023.\",\n",
    "    \"Entre 2014 et 2023, le taux de chômage en France a varié considérablement. Après avoir atteint un sommet de 10,5 % en 2015, il a progressivement diminué, atteignant un minimum de 7,8 % en 2023. La pandémie de COVID-19 a temporairement perturbé cette tendance, augmentant le taux de chômage en 2020 et 2021.\",\n",
    "    \"Au cours des dix dernières années, le taux de chômage en France a montré une tendance à la baisse. Après avoir atteint un pic en 2015, il a progressivement diminué, bien que la crise sanitaire de 2020 ait causé une augmentation temporaire. En 2023, le taux de chômage était de 7,8 %.\",\n",
    "    \"L'évolution du taux de chômage en France de 2014 à 2023 révèle une baisse progressive après un pic en 2015. Bien que la pandémie ait provoqué une hausse temporaire, le taux de chômage a repris sa tendance à la baisse pour atteindre 7,8 % en 2023, selon l'INSEE.\",\n",
    "    \"La cuisine française est mondialement reconnue pour sa diversité et son raffinement. Des plats emblématiques comme le coq au vin, la bouillabaisse et le bœuf bourguignon illustrent la richesse gastronomique du pays. Les vins et fromages français sont également très appréciés à l'international.\",\n",
    "    \"Le système éducatif français est structuré en plusieurs niveaux, allant de l'école maternelle à l'université. L'éducation est obligatoire de 3 à 16 ans. Les élèves passent des examens nationaux comme le brevet des collèges et le baccalauréat, qui sont des étapes clés dans leur parcours scolaire.\",\n",
    "    \"La France est l'un des leaders mondiaux dans la production d'énergie nucléaire. Environ 70 % de l'électricité du pays provient de centrales nucléaires. Cette dépendance permet à la France d'avoir une empreinte carbone relativement faible par rapport à d'autres pays européens.\",\n",
    "    \"Le football est le sport le plus populaire en France, avec des millions de licenciés et de nombreux clubs répartis sur tout le territoire. L'équipe nationale, les Bleus, a remporté la Coupe du Monde de la FIFA en 1998 et 2018, ce qui a renforcé l'engouement pour ce sport parmi les Français.\",\n",
    "    \"La France est riche en patrimoine culturel, avec des monuments emblématiques tels que la Tour Eiffel, le Mont Saint-Michel et le Château de Versailles. Le pays abrite également de nombreux musées de renommée mondiale, dont le Louvre et le Musée d'Orsay, qui attirent des millions de visiteurs chaque année.\"\n",
    "]\n",
    "\n",
    "dict_doc_rank = {doc : r for r, doc in enumerate(documents)}\n",
    "\n",
    "for ass_func in [RG_YN, RG_S, RG_3L, RG_4L]:\n",
    "    print(\"------------------------\")\n",
    "    reranked_documents = llm_reranking(\n",
    "        query=question, \n",
    "        retrieved_documents=documents, \n",
    "        assessing_method=ass_func, \n",
    "        aggregating_method=expected_relevance_values\n",
    "        )\n",
    "    for i in range(5):\n",
    "        print(f\"Doc {i} th\", reranked_documents[i])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment LLM reranker "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
