{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a Complete Database "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- date : 24/05/2024\n",
    "- New features : generate a complete database using \"OrdalieTech/Solon-embeddings-large-0.1\" embedding model (config file have been improved)\n",
    "- Expected improvement : improving the retrieval capabilities with a much strong embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building New Complete Dataset based on config files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db = build_database_from_csv('/home/onyxia/work/llm-open-data-insee/data_complete.csv')\n",
    "# db.similarity_search(\"Quels sont les chiffres du chômages en 2023\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Dataset based on config files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from db_building import reload_database_from_local_dir\n",
    "\n",
    "db = reload_database_from_local_dir(\n",
    "    persist_directory=\"/home/onyxia/work/llm-open-data-insee/data/chroma_db\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there are at least one encoded document in our vectorstore\n",
    "print(len(db.get()[\"ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = db.similarity_search(\n",
    "    \"Quels résultats au BAC les étudiants de classes préparatoires ont ils généralement?\", k=5\n",
    ")\n",
    "print(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chain_building.build_chain import load_retriever\n",
    "from config import EMB_MODEL_NAME, MODEL_NAME, RAG_PROMPT_TEMPLATE\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from model_building import build_llm_model\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "def build_chain_test(retriever, prompt, llm):\n",
    "    \"\"\"\n",
    "    Build a LLM chain based on Langchain package and INSEE data\n",
    "    \"\"\"\n",
    "    # Create a Langchain LLM Chain\n",
    "    chain = (\n",
    "        RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    rag_chain_with_source = RunnableParallel(\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    ).assign(answer=chain)\n",
    "\n",
    "    return rag_chain_with_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=RAG_PROMPT_TEMPLATE)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever(search_type=\"mmr\", search_kwargs={\"score_threshold\": 0.5, \"k\": 5})\n",
    "\n",
    "# retriever = load_retriever(emb_model_name=EMB_MODEL_NAME,\n",
    "# persist_directory=\"/home/onyxia/work/llm-open-data-insee/data/chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HF_TOKEN\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = build_llm_model(\n",
    "    model_name=MODEL_NAME, quantization_config=True, config=True, token=os.environ[\"HF_TOKEN\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = build_chain_test(retriever, prompt, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Quel est le but initial derrière la création du système de retraites français après la Seconde Guerre mondiale?\"\n",
    "# question = \"Quelle est la cause principale de l'augmentation de l'indice des prix à la consommation (IPC)?\"\n",
    "results = retriever.invoke(question)\n",
    "\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"Doc {i} : {doc.metadata[\"source\"]}\")\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in chain.stream(question):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answer[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a Reranker "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this part is to build a pipeline Langchain where we have added a reranker: a BM25, a ColBERT model, a french cross-encoder, a multilingual cross-encoder and several hyperparameters.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reranker model list : \n",
    "- multilingual cross encoder : BAAI/bge-reranker-large (multilingual),\n",
    "- french cross encoder : antoinelouis/crossencoder-electra-base-french-mmarcoFR  OR dangvantuan/CrossEncoder-camembert-large\n",
    "- BM25 : langchain_community.retrievers import BM25Retriever\n",
    "- ColBERT : antoinelouis/colbertv2-camembert-L4-mmarcoFR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mc cp s3/projet-llm-insee-open-data/data/chroma_database/chroma_db /home/onyxia/work/llm-open-data-insee/data --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/home/onyxia/work/llm-open-data-insee/src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chain_building import load_retriever\n",
    "from config import MODEL_NAME\n",
    "\n",
    "retriever = load_retriever(\n",
    "    emb_model_name=EMB_MODEL_NAME,\n",
    "    persist_directory=\"/home/onyxia/work/llm-open-data-insee/data/chroma_db\",\n",
    "    device=\"cuda\",\n",
    "    collection_name=\"insee_data\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_docs(docs):\n",
    "    print(\n",
    "        f\"\\n{'-' * 100}\\n\".join(\n",
    "            [f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test embedding retriever\n",
    "question = \"Comment est calculé le pouvoir d'achat ?\"\n",
    "# question = \"Quelle est la cause principale de l'augmentation de l'indice des prix à la consommation (IPC)?\"\n",
    "results = retriever.invoke(question)\n",
    "pretty_print_docs(results)  # OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder  # CrossEncoder\n",
    "from langchain_community.retrievers import BM25Retriever  # BM25\n",
    "from ragatouille import RAGPretrainedModel  # ColBERT\n",
    "\n",
    "colBERT = RAGPretrainedModel.from_pretrained(\"antoinelouis/colbertv2-camembert-L4-mmarcoFR\")\n",
    "colBERT_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=colBERT.as_langchain_document_compressor(k=5), base_retriever=retriever\n",
    ")\n",
    "\n",
    "compressed_docs = colBERT_retriever.invoke(question)\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HuggingFaceCrossEncoder(\n",
    "    model_name=\"dangvantuan/CrossEncoder-camembert-large\"\n",
    ")  # \"antoinelouis/crossencoder-electra-base-french-mmarcoFR\")\n",
    "compressor_1 = CrossEncoderReranker(model=model, top_n=5)\n",
    "compression_retriever_1 = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor_1, base_retriever=retriever\n",
    ")\n",
    "\n",
    "compressed_docs = compression_retriever.invoke(question)\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HuggingFaceCrossEncoder(model_name=\"BAAI/bge-reranker-large\")\n",
    "compressor_2 = CrossEncoderReranker(model=model, top_n=5)\n",
    "compression_retriever_2 = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor_2, base_retriever=retriever\n",
    ")\n",
    "\n",
    "compressed_docs = compression_retriever.invoke(question)\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import EnsembleRetriever\n",
    "\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[compression_retriever_1, compression_retriever_2, colBERT_retriever],\n",
    "    weigths=[1 / 3, 1 / 3, 1 / 3],\n",
    ")\n",
    "\n",
    "compressed_docs = ensemble_retriever.invoke(question)\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Sequence\n",
    "\n",
    "from langchain.schema import Document\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "\n",
    "# Define the compression function\n",
    "def compress_documents_lambda(\n",
    "    documents: Sequence[Document], query: str, k: int = 5, **kwargs: Dict[str, Any]\n",
    ") -> Sequence[Document]:\n",
    "    \"\"\"Compress retrieved documents given the query context.\"\"\"\n",
    "\n",
    "    # Initialize the retriever with the documents\n",
    "    retriever = BM25Retriever.from_documents(documents, k=k, **kwargs)\n",
    "    relevant_docs = retriever.get_relevant_documents(query)\n",
    "    return relevant_docs\n",
    "\n",
    "\n",
    "# Define the complete chain\n",
    "bm25_retriever = RunnableParallel(\n",
    "    {\"documents\": retriever, \"query\": RunnablePassthrough()}\n",
    ") | RunnableLambda(lambda r: compress_documents_lambda(documents=r[\"documents\"], query=r[\"query\"]))\n",
    "\n",
    "bm25_retriever.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever, EnsembleRetriever\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder  # CrossEncoder\n",
    "\n",
    "model = HuggingFaceCrossEncoder(\n",
    "    model_name=\"dangvantuan/CrossEncoder-camembert-large\"\n",
    ")  # \"antoinelouis/crossencoder-electra-base-french-mmarcoFR\")\n",
    "compressor_1 = CrossEncoderReranker(model=model, top_n=5)\n",
    "\n",
    "compression_retriever_cross_encoder = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor_1, base_retriever=retriever\n",
    ")\n",
    "\n",
    "emsemble_reranking = EnsembleRetriever(\n",
    "    retrievers=[compression_retriever_cross_encoder, bm25_retriever], weigths=[0.5, 0.5]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emsemble_reranking.invoke(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test New LLM model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "# quantization config\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "hf_token = \"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\"\n",
    "config = AutoConfig.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "pipe = pipeline(model=model, tokenizer=tokenizer)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "terminators = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")]\n",
    "\n",
    "outputs = pipe(\n",
    "    input_ids,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "response = outputs[0][input_ids.shape[-1] :]\n",
    "print(tokenizer.decode(response, skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
