{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dit RAG ? ü§ñ\n",
    "\n",
    "## R√©cup√©ration de la base de donn√©es vectorielle\n",
    "\n",
    "On va d√©j√† r√©cup√©rer la base de donn√©es vectorielle des publis DIRAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from s3fs import S3FileSystem\n",
    "\n",
    "fs = S3FileSystem(\n",
    "    client_kwargs={\"endpoint_url\": f\"https://{os.environ['AWS_S3_ENDPOINT']}\"},\n",
    "    key=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "    secret=os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_path = \"projet-llm-insee-open-data/data/chroma_database/experiment/dirag/\"\n",
    "local_path = \"data/chroma_db_checkpoint/\"\n",
    "\n",
    "fs.download(\n",
    "    rpath=s3_path,\n",
    "    lpath=local_path,\n",
    "    recursive=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On _load_ la base dans `Python` gr√¢ce √† `langchain`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.db_building.build_database import load_vector_database_from_local\n",
    "\n",
    "db = load_vector_database_from_local(\n",
    "    persist_directory=\"data/chroma_db_checkpoint/\", embedding_model=\"OrdalieTech/Solon-embeddings-large-0.1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On v√©rifie juste que √ßa fonctionne sur une requ√™te simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.similarity_search(\"quel est le secteur dominant en guadeloupe?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On transforme en `retriever` langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.db_building import load_retriever\n",
    "\n",
    "retriever, vectorstore = load_retriever(\n",
    "    vectorstore=db,\n",
    "    retriever_params={\"search_type\": \"similarity\", \"search_kwargs\": {\"k\": 10}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.invoke(\"subventions aux entreprises\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain RAG vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "from src.utils import create_prompt_from_instructions, format_docs\n",
    "\n",
    "system_instructions = \"\"\"\n",
    "Tu es un assistant sp√©cialis√© dans la statistique publique.\n",
    "Tu r√©ponds √† des questions concernant les donn√©es de l'Insee,\n",
    "l'institut national statistique Fran√ßais.\n",
    "\n",
    "R√©ponds en FRANCAIS UNIQUEMENT. Utilise une mise en forme au format markdown.\n",
    "\n",
    "En utilisant UNIQUEMENT les informations pr√©sentes dans le contexte,\n",
    "r√©ponds de mani√®re argument√©e √† la question pos√©e.\n",
    "\n",
    "La r√©ponse doit √™tre d√©velopp√©e et citer l'ensemble de ses sources\n",
    "(titre et url de la publication) qui sont r√©f√©renc√©es √† la fin.\n",
    "Cite notamment l'url d'origine de la publication, dans un format markdown.\n",
    "Un lien hypertext vers l'url de la source serait appr√©ci√©.\n",
    "\n",
    "Cite 10 sources maximum.\n",
    "\n",
    "Tu n'es pas oblig√© d'utiliser les sources les moins pertinentes.\n",
    "\n",
    "Si tu ne peux pas induire ta r√©ponse du contexte, ne r√©ponds pas.\n",
    "\n",
    "Voici le contexte sur lequel tu dois baser ta r√©ponse :\n",
    "Contexte: {context}\n",
    "\"\"\"\n",
    "\n",
    "question_instructions = \"\"\"\n",
    "Voici la question √† laquelle tu dois r√©pondre :\n",
    "Question: {question}\n",
    "\n",
    "R√©ponse:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Set OpenAI's API key and API base to use vLLM's API server.\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"https://projet-llm-insee-open-data-vllm.user.lab.sspcloud.fr/v1/\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "\n",
    "\n",
    "def statbot_from_vllm_api(\n",
    "    question,\n",
    "    retriever,\n",
    "    client,\n",
    "    system_instructions=system_instructions,\n",
    "    question_instructions=question_instructions,\n",
    "    model=\"mistralai/Mistral-Small-24B-Instruct-2501\",\n",
    "):\n",
    "    prompt = create_prompt_from_instructions(system_instructions, question_instructions)\n",
    "    context = format_docs(retriever.invoke(question))\n",
    "\n",
    "    question_with_context = prompt.format(question=question, context=context)\n",
    "\n",
    "    chat_response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_instructions},\n",
    "            {\"role\": \"user\", \"content\": question_with_context},\n",
    "        ],\n",
    "    )\n",
    "    return chat_response, chat_response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "_, answer = statbot_from_vllm_api(\n",
    "    \"Quelles sont les subventions sp√©cifiques aux DOMS et leur effet sur le tissu productif ?\", retriever, client\n",
    ")\n",
    "display(Markdown(answer.replace(\"   \", \"\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, answer = statbot_from_vllm_api(\"La Guadeloupe est-elle d√©favoris√©e par rapport √† la M√©tropole ?\", retriever, client)\n",
    "display(Markdown(answer.replace(\"   \", \"\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, answer = statbot_from_vllm_api(\"y a quoi √† manger √† la cantine ce soir?\", retriever, client)\n",
    "display(Markdown(answer.replace(\"   \", \"\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain RAG locale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import VLLM\n",
    "\n",
    "from src.config import MODEL_TO_ARGS\n",
    "\n",
    "LLM_MODEL = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "llm = VLLM(model=LLM_MODEL, **MODEL_TO_ARGS.get(LLM_MODEL, {}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_instructions = \"\"\"\n",
    "Tu es un assistant sp√©cialis√© dans la statistique publique.\n",
    "Tu r√©ponds √† des questions concernant les donn√©es de l'Insee, l'institut national statistique Fran√ßais.\n",
    "\n",
    "R√©ponds en FRANCAIS UNIQUEMENT. Utilise une mise en forme au format markdown.\n",
    "\n",
    "En utilisant UNIQUEMENT les informations pr√©sentes dans le contexte, r√©ponds de mani√®re argument√©e √† la question pos√©e.\n",
    "\n",
    "La r√©ponse doit √™tre d√©velopp√©e et citer ses sources (titre et url de la publication) qui sont r√©f√©renc√©es √† la fin.\n",
    "Cite notamment l'url d'origine de la publication, dans un format markdown.\n",
    "\n",
    "Cite 5 sources maximum.\n",
    "\n",
    "Tu n'es pas oblig√© d'utiliser les sources les moins pertinentes.\n",
    "\n",
    "Si tu ne peux pas induire ta r√©ponse du contexte, ne r√©ponds pas.\n",
    "\n",
    "Voici le contexte sur lequel tu dois baser ta r√©ponse :\n",
    "Contexte: {context}\n",
    "\"\"\"\n",
    "\n",
    "question_instructions = \"\"\"\n",
    "Voici la question √† laquelle tu dois r√©pondre :\n",
    "Question: {question}\n",
    "\n",
    "R√©ponse:\n",
    "\"\"\"\n",
    "\n",
    "prompt = create_prompt_from_instructions(system_instructions, question_instructions)\n",
    "\n",
    "rag_chain = {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()} | prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "answer = rag_chain.invoke(\"Quel est le niveau de vie en Guadeloupe?\")\n",
    "\n",
    "display(Markdown(answer.replace(\"   \", \"\")))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
