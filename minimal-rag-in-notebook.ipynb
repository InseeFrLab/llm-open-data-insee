{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bac à sable RAG\n",
    "\n",
    "Ce notebook part du principe que la _vector database_ est déjà prête, c'est-à-dire que les étapes suivantes ont déjà été faites:\n",
    "\n",
    "<div>\n",
    "<img src=\"https://python.langchain.com/assets/images/rag_indexing-8160f90a90a33253d0154659cf7d453f.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "Nous nous intéressons à celles-ci:\n",
    "\n",
    "<div>\n",
    "<img src=\"https://python.langchain.com/assets/images/rag_retrieval_generation-1046a4668d6bb08786ef73c56d4f228a.png\" width=\"500\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import s3fs\n",
    "\n",
    "from langchain.schema.runnable.config import RunnableConfig\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "from src.chain_building.build_chain import build_chain\n",
    "from src.chain_building.build_chain_validator import build_chain_validator\n",
    "from src.config import CHATBOT_TEMPLATE, EMB_MODEL_NAME\n",
    "from src.db_building import (\n",
    "    load_retriever,\n",
    "    load_vector_database\n",
    ")\n",
    "from src.model_building import build_llm_model\n",
    "from src.utils.formatting_utilities import add_sources_to_messages, str_to_bool\n",
    "\n",
    "# Logging configuration\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %I:%M:%S %p\",\n",
    "    level=logging.DEBUG,\n",
    ")\n",
    "\n",
    "# Remote file configuration\n",
    "os.environ['MLFLOW_TRACKING_URI'] = \"https://projet-llm-insee-open-data-mlflow.user.lab.sspcloud.fr/\"\n",
    "fs = s3fs.S3FileSystem(client_kwargs={\"endpoint_url\": f\"\"\"https://{os.environ[\"AWS_S3_ENDPOINT\"]}\"\"\"})\n",
    "\n",
    "# PARAMETERS --------------------------------------\n",
    "\n",
    "os.environ['UVICORN_TIMEOUT_KEEP_ALIVE'] = \"0\"\n",
    "\n",
    "model = os.getenv(\"LLM_MODEL_NAME\")\n",
    "CHROMA_DB_LOCAL_DIRECTORY = \"./data/chroma_db\"\n",
    "CLI_MESSAGE_SEPARATOR = f\"{80*'-'} \\n\"\n",
    "quantization = True\n",
    "DEFAULT_MAX_NEW_TOKENS = 10\n",
    "DEFAULT_MODEL_TEMPERATURE = 1\n",
    "embedding = os.getenv(\"EMB_MODEL_NAME\", EMB_MODEL_NAME)\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "LLM_MODEL = os.getenv(\"LLM_MODEL_NAME\", \"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "LLM_MODEL = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "QUANTIZATION = os.getenv(\"QUANTIZATION\", True)\n",
    "MAX_NEW_TOKENS = int(os.getenv(\"MAX_NEW_TOKENS\", DEFAULT_MAX_NEW_TOKENS))\n",
    "MODEL_TEMPERATURE = int(os.getenv(\"MODEL_TEMPERATURE\", DEFAULT_MODEL_TEMPERATURE))\n",
    "RETURN_FULL_TEXT = os.getenv(\"RETURN_FULL_TEXT\", True)\n",
    "DO_SAMPLE = os.getenv(\"DO_SAMPLE\", True)\n",
    "DATABASE_RUN_ID = \"32d4150a14fa40d49b9512e1f3ff9e8c\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import de la database et du modèle génératif\n",
    "\n",
    "### Base de données vectorielle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 02:13:57 PM Destination path /tmp/mlflow/32d4150a14fa40d49b9512e1f3ff9e8c/chroma exists. Skipping download because force is set to False.\n",
      "2024-11-20 02:13:57 PM Load pretrained SentenceTransformer: OrdalieTech/Solon-embeddings-large-0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model OrdalieTech/Solon-embeddings-large-0.1 found in local cache. \n",
      "Model mistralai/Mistral-7B-Instruct-v0.3 found in local cache. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/onyxia/work/llm-open-data-insee/src/db_building/build_database.py:112: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  db = Chroma(\n",
      "2024-11-20 02:14:03 PM Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "2024-11-20 02:14:03 PM ⚠️ It looks like you upgraded from a version below 0.6 and could benefit from vacuuming your database. Run chromadb utils vacuum --help for more information.\n",
      "2024-11-20 02:14:03 PM The database (collection insee_data) has been reloaded from directory /tmp/mlflow/32d4150a14fa40d49b9512e1f3ff9e8c/chroma/chroma\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from src.model_building.fetch_llm_model import cache_model_from_hf_hub\n",
    "from utils import retrieve_db_from_cache\n",
    "\n",
    "EMB_MODEL_NAME = \"OrdalieTech/Solon-embeddings-large-0.1\"\n",
    "LLM_MODEL = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "hf_token = os.environ[\"HF_TOKEN\"]\n",
    "s3_token = os.environ[\"AWS_SESSION_TOKEN\"]\n",
    "\n",
    "cache_model_from_hf_hub(EMB_MODEL_NAME, hf_token=hf_token, s3_token=s3_token)\n",
    "cache_model_from_hf_hub(LLM_MODEL, hf_token=hf_token, s3_token=s3_token)\n",
    "\n",
    "db = retrieve_db_from_cache(filesystem=fs, run_id=DATABASE_RUN_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nombre de documents dans la vector db: 287086'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"Nombre de documents dans la vector db: {len(db.get()['documents'])}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modèle génératif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model OrdalieTech/Solon-embeddings-large-0.1 found in local cache. \n",
      "Model mistralai/Mistral-7B-Instruct-v0.3 found in local cache. \n",
      "INFO 11-20 14:39:08 config.py:350] This model supports multiple tasks: {'embedding', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 11-20 14:39:08 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post1) with config: model='mistralai/Mistral-7B-Instruct-v0.3', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.3', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=mistralai/Mistral-7B-Instruct-v0.3, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, chat_template_text_format=string, mm_processor_kwargs=None, pooler_config=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer_group/tokenizer_group.py:23: FutureWarning: It is strongly recommended to run mistral models with `--tokenizer_mode \"mistral\"` to ensure correct encoding and decoding.\n",
      "  self.tokenizer = get_tokenizer(self.tokenizer_id, **tokenizer_config)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-20 14:39:11 selector.py:135] Using Flash Attention backend.\n",
      "INFO 11-20 14:39:12 model_runner.py:1072] Starting to load model mistralai/Mistral-7B-Instruct-v0.3...\n",
      "INFO 11-20 14:39:12 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 1/3 [00:06<00:12,  6.27s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 2/3 [00:14<00:07,  7.44s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:25<00:00,  9.17s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:25<00:00,  8.59s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-20 14:45:34 model_runner.py:1077] Loading model weights took 13.5083 GB\n",
      "INFO 11-20 14:45:35 worker.py:232] Memory profiling results: total_gpu_memory=79.11GiB initial_memory_usage=16.22GiB peak_torch_memory=19.00GiB memory_usage_post_profile=16.22GiB non_torch_memory=0.59GiB kv_cache_size=51.60GiB gpu_memory_utilization=0.90\n",
      "INFO 11-20 14:45:36 gpu_executor.py:113] # GPU blocks: 26420, # CPU blocks: 2048\n",
      "INFO 11-20 14:45:36 gpu_executor.py:117] Maximum concurrency for 32768 tokens per request: 12.90x\n",
      "INFO 11-20 14:45:41 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 11-20 14:45:41 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 11-20 14:46:04 model_runner.py:1518] Graph capturing finished in 23 secs, took 0.89 GiB\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import VLLM\n",
    "\n",
    "MAX_NEW_TOKEN = 8192\n",
    "TEMPERATURE = 0.2\n",
    "REP_PENALTY = 1.1\n",
    "TOP_P = 0.8\n",
    "\n",
    "hf_token = os.environ[\"HF_TOKEN\"]\n",
    "s3_token = os.environ[\"AWS_SESSION_TOKEN\"]\n",
    "\n",
    "cache_model_from_hf_hub(EMB_MODEL_NAME, hf_token=hf_token, s3_token=s3_token)\n",
    "cache_model_from_hf_hub(LLM_MODEL, hf_token=hf_token, s3_token=s3_token)\n",
    "\n",
    "llm = VLLM(\n",
    "        model=LLM_MODEL,\n",
    "        max_new_tokens=MAX_NEW_TOKEN,\n",
    "        top_p=TOP_P,\n",
    "        temperature=TEMPERATURE,\n",
    "        rep_penalty=REP_PENALTY,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform vector database into retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 02:21:02 PM vectorstore being provided, skipping the reloading\n"
     ]
    }
   ],
   "source": [
    "retriever, vectorstore = load_retriever(\n",
    "                emb_model_name=embedding,\n",
    "                persist_directory=CHROMA_DB_LOCAL_DIRECTORY,\n",
    "                vectorstore=db,\n",
    "                retriever_params={\n",
    "                    \"search_type\": \"similarity\",\n",
    "                    \"search_kwargs\": {\"k\": 30}\n",
    "                },\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 103.74it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Header 1': 'Chômage et halo autour du chômage en 2019', 'Header 2': 'Résumé :', 'Header 3': \"Téléchargement des tableaux à l'unité\", 'Header 4': 'Chômage', 'categorie': 'Chiffres détaillés', 'collection': '', 'dateDiffusion': '2020-06-23 12:00', 'libelleAffichageGeo': 'France', 'theme': 'Emploi – Population active', 'titre': 'Chômage et halo autour du chômage en 2019', 'url': 'https://www.insee.fr/fr/statistiques/4498582'}, page_content='#### Chômage'),\n",
       " Document(metadata={'Header 1': 'Emploi-Chômage', 'categorie': 'Publications pour expert', 'collection': 'Note de conjoncture', 'dateDiffusion': '2020-12-15 17:00', 'libelleAffichageGeo': 'France', 'theme': 'Économie générale (inflation, PIB, dette,...)', 'titre': 'Emploi-Chômage', 'url': 'https://www.insee.fr/fr/statistiques/4653872'}, page_content='# Emploi-Chômage'),\n",
       " Document(metadata={'Header 1': 'Chômage', 'categorie': 'Publications pour expert', 'collection': 'Note de conjoncture', 'dateDiffusion': '2017-03-16 17:00', 'libelleAffichageGeo': 'France', 'theme': 'Chômage', 'titre': 'Chômage', 'url': 'https://www.insee.fr/fr/statistiques/2662536'}, page_content='# Chômage'),\n",
       " Document(metadata={'Header 1': 'Fiche Chômage', 'categorie': 'Publications pour expert', 'collection': 'Note de conjoncture', 'dateDiffusion': '2017-12-19 17:00', 'libelleAffichageGeo': 'France', 'theme': 'Chômage', 'titre': 'Fiche Chômage', 'url': 'https://www.insee.fr/fr/statistiques/3292347'}, page_content='# Fiche Chômage'),\n",
       " Document(metadata={'Header 1': 'Données détaillées sur le chômage', 'Header 2': 'Résumé :', 'Header 3': '« Halo » du chômage', 'categorie': 'Chiffres détaillés', 'collection': '', 'dateDiffusion': '2015-02-10 13:00', 'libelleAffichageGeo': 'France', 'theme': 'Chômage', 'titre': 'Données détaillées sur le chômage', 'url': 'https://www.insee.fr/fr/statistiques/1991750'}, page_content='### « Halo » du chômage')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"Chiffres du chômage\")[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG mode: on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import (\n",
    "    format_docs,\n",
    "    create_prompt_from_instructions,\n",
    "    retrieve_db_from_cache,\n",
    ")\n",
    "\n",
    "system_instructions = \"\"\"\n",
    "Tu es un assistant spécialisé dans la statistique publique. Tu réponds à des questions concernant les données de l'Insee, l'institut national statistique Français.\n",
    "\n",
    "Réponds en FRANCAIS UNIQUEMENT. Utilise une mise en forme au format markdown.\n",
    "\n",
    "En utilisant UNIQUEMENT les informations présentes dans le contexte, réponds de manière argumentée à la question posée.\n",
    "\n",
    "La réponse doit être développée et citer ses sources (titre et url de la publication) qui sont référencées à la fin. Cite notamment l'url d'origine de la publication, dans un format markdown.\n",
    "\n",
    "Cite 5 sources maximum.\n",
    "\n",
    "Tu n'es pas obligé d'utiliser les sources les moins pertinentes. \n",
    "\n",
    "Si tu ne peux pas induire ta réponse du contexte, ne réponds pas.\n",
    "\n",
    "Voici le contexte sur lequel tu dois baser ta réponse :\n",
    "Contexte: {context}\n",
    "\"\"\"\n",
    "\n",
    "question_instructions = \"\"\"\n",
    "Voici la question à laquelle tu dois répondre :\n",
    "Question: {question}\n",
    "\n",
    "Réponse:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt = create_prompt_from_instructions(system_instructions, question_instructions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "## Imbrication dans langchain\n",
    "rag_chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 86.58it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:13<00:00, 13.11s/it, est. speed input: 675.99 toks/s, output: 78.27 toks/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"1. Le chômage en France est mesuré par le taux de chômage, qui est le rapport entre le nombre de chômeurs au sens du BIT et le nombre de personnes en emploi ou au chômage. Le BIT (Bénéficiaire d'allocations chômage) est une personne qui reçoit des allocations chômage, c'est-à-dire des allocations de l'État pour couvrir ses frais de subsistance pendant qu'elle est au chômage.\\n\\n    2. Le taux de chômage en France a été stable depuis plusieurs années, mais il varie selon les régions et les catégories socioprofessionnelles. Par exemple, en 2019, le taux de chômage en France métropolitaine était de 7,7%, mais il était plus élevé en région Île-de-France (9,3%) et plus bas en région Centre-Val de Loire (6,1%).\\n\\n    3. Les chômeurs en France sont majoritairement des hommes (54,6% en 2019), mais leur proportion est en baisse depuis plusieurs années. Les chômeurs sont également majoritairement des personnes de 30 à 49 ans (68,1% en 2019), mais leur proportion est en hausse depuis plusieurs années.\\n\\n    4. Les chômeurs en France sont majoritairement des personnes qui n'ont jamais travaillé (15,4% en 2019), mais leur proportion est en baisse depuis plusieurs années. Les chômeurs ayant déjà travaillé sont majoritairement des personnes qui ont été licenciés économiquement (6,1% en 2019).\\n\\n    5. Les chômeurs en France sont majoritairement des personnes qui ont un diplôme supérieur au baccalauréat (71,8% en 2019). Les chômeurs ayant un diplôme inférieur au baccalauréat sont majoritairement des personnes qui ont un diplôme de CAP ou BEP (25,8% en 2019).\\n\\nSources:\\n\\n* <https://www.insee.fr/fr/statistiques/4498582>\\n* <https://www.insee.fr/fr/statistiques/4653872>\\n* <https://www.insee.fr/fr/statistiques/2662536>\\n* <https://www.insee.fr/fr/statistiques/3292347>\\n* <https://www.insee.fr/fr/statistiques/1991750>\\n* <https://www.insee.fr/fr/statistiques/1372609>\\n* <https://www.insee.fr/fr/statistiques/3594288>\\n* <https://www.insee.fr/fr/statistiques/1285564>\\n* <https://www.insee.fr/fr/statistiques/4501595>\\n* <https://www.insee.fr/fr/statistiques/6435502>\\n* <https://www.insee.fr/fr/statistiques/7671567>\\n* <https://www.insee.fr/fr/statistiques/7076726>\\n* <https://www.insee.fr/fr/metadonnees/source/serie/s2107>\\n* <https://www.insee.fr/fr/statistiques/6010063>\\n* <https://www.insee.fr/fr/statistiques/6661596>\\n* <https://www.insee.fr/fr/statistiques/3532084>\\n* <https://www.insee.fr/fr/statistiques/3532055>\\n* <https://www.insee.fr/fr/statistiques/3532194>\\n* <https://www.insee.fr/fr/statistiques/1300758>\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"Chiffres du chômage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.60it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.83s/it, est. speed input: 1743.61 toks/s, output: 66.77 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Le taux de chômage peut être trouvé à l'échelle de la région, du département ou de la commune. Il est calculé en divisant le nombre de personnes chômeures par le nombre de personnes actives de la population.\n",
      "\n",
      "     Pour trouver le taux de chômage à l'échelle de la région, il est nécessaire de consulter les données de l'Insee sur le taux de chômage régional. Par exemple, pour la région Grand Est, vous pouvez consulter la publication \"Taux de chômage régional\" sur le site de l'Insee : https://www.insee.fr/fr/statistiques/7931214\n",
      "\n",
      "     Pour trouver le taux de chômage à l'échelle du département, il est nécessaire de consulter les données de l'Insee sur le taux de chômage départemental. Par exemple, pour le département de la Nièvre, vous pouvez consulter la publication \"Taux de chômage départemental\" sur le site de l'Insee : https://www.insee.fr/fr/statistiques/5370760\n",
      "\n",
      "     Pour trouver le taux de chômage à l'échelle de la commune, il est nécessaire de consulter les données de l'Insee sur le taux de chômage communal. Par exemple, pour la commune de Toulouse, vous pouvez consulter la publication \"Taux de chômage communal\" sur le site de l'Insee : https://www.insee.fr/fr/statistiques/1304071\n",
      "\n",
      "     Les sources utilisées pour cette réponse sont :\n",
      "\n",
      "     - Insee, Taux de chômage régional : https://www.insee.fr/fr/statistiques/7931214\n",
      "     - Insee, Taux de chômage départemental : https://www.insee.fr/fr/statistiques/5370760\n",
      "     - Insee, Taux de chômage communal : https://www.insee.fr/fr/statistiques/1304071"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for chunk in rag_chain.stream(\"A quelle échelle puis-je trouver le taux de chîmage ? \"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
