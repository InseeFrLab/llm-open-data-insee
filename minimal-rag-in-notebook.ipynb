{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bac à sable RAG\n",
    "\n",
    "Ce notebook part du principe que la _vector database_ est déjà prête, c'est-à-dire que les étapes suivantes ont déjà été faites:\n",
    "\n",
    "<div>\n",
    "<img src=\"https://python.langchain.com/assets/images/rag_indexing-8160f90a90a33253d0154659cf7d453f.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "Nous nous intéressons à celles-ci:\n",
    "\n",
    "<div>\n",
    "<img src=\"https://python.langchain.com/assets/images/rag_retrieval_generation-1046a4668d6bb08786ef73c56d4f228a.png\" width=\"500\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import s3fs\n",
    "\n",
    "from src.db_building import load_retriever, load_vector_database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import de la database et du modèle génératif\n",
    "\n",
    "### Base de données vectorielle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.config import custom_config\n",
    "from src.model_building import cache_model_from_hf_hub\n",
    "\n",
    "EMB_MODEL_NAME = \"OrdalieTech/Solon-embeddings-large-0.1\"\n",
    "LLM_MODEL = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "hf_token = os.environ[\"HF_TOKEN\"]\n",
    "s3_token = os.environ[\"AWS_SESSION_TOKEN\"]\n",
    "\n",
    "cache_model_from_hf_hub(EMB_MODEL_NAME, hf_token=hf_token, s3_token=s3_token)\n",
    "cache_model_from_hf_hub(LLM_MODEL, hf_token=hf_token, s3_token=s3_token)\n",
    "\n",
    "DATABASE_RUN_ID = \"9c9c411829c947799e3acd3df1564c0b\"\n",
    "\n",
    "# Create a custom confz configuration\n",
    "config = custom_config(\n",
    "    defaults={  # These defaults can be overriden with env variables\n",
    "        \"MAX_NEW_TOKENS\": 2000,\n",
    "        \"MODEL_TEMPERATURE\": 1.0,\n",
    "        \"quantization\": True,\n",
    "        \"mlflow_run_id\": DATABASE_RUN_ID,\n",
    "    },\n",
    "    overrides={  # These values are going to be used no matter what\n",
    "        \"UVICORN_TIMEOUT_KEEP_ALIVE\": 0,\n",
    "        \"MAX_NEW_TOKENS\": 2000,\n",
    "        \"LLM_MODEL\": LLM_MODEL,\n",
    "        \"EMB_MODEL_NAME\": EMB_MODEL_NAME,\n",
    "        \"mlflow_run_id\": DATABASE_RUN_ID,\n",
    "    },\n",
    ")\n",
    "RETURN_FULL_TEXT = True\n",
    "DO_SAMPLE = True\n",
    "\n",
    "CLI_MESSAGE_SEPARATOR = (config.cli_message_separator_length * \"-\") + \" \\n\"\n",
    "\n",
    "# Remote file configuration\n",
    "fs = s3fs.S3FileSystem(endpoint_url=config.s3_endpoint_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = load_vector_database(filesystem=fs, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f\"Nombre de documents dans la vector db: {len(db.get()['documents'])}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La chaine tout en un (avec langchain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import VLLM\n",
    "\n",
    "from src.config import MODEL_TO_ARGS\n",
    "\n",
    "retriever, vectorstore = load_retriever(\n",
    "    vectorstore=db,\n",
    "    retriever_params={\"search_type\": \"similarity\", \"search_kwargs\": {\"k\": 10}},\n",
    ")\n",
    "\n",
    "\n",
    "llm = VLLM(model=LLM_MODEL, **MODEL_TO_ARGS.get(LLM_MODEL, {}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import create_prompt_from_instructions, format_docs\n",
    "\n",
    "system_instructions = \"\"\"\n",
    "Tu es un assistant spécialisé dans la statistique publique.\n",
    "Tu réponds à des questions concernant les données de l'Insee, l'institut national statistique Français.\n",
    "\n",
    "Réponds en FRANCAIS UNIQUEMENT. Utilise une mise en forme au format markdown.\n",
    "\n",
    "En utilisant UNIQUEMENT les informations présentes dans le contexte, réponds de manière argumentée à la question posée.\n",
    "\n",
    "La réponse doit être développée et citer ses sources (titre et url de la publication) qui sont référencées à la fin.\n",
    "Cite notamment l'url d'origine de la publication, dans un format markdown.\n",
    "\n",
    "Cite 5 sources maximum.\n",
    "\n",
    "Tu n'es pas obligé d'utiliser les sources les moins pertinentes.\n",
    "\n",
    "Si tu ne peux pas induire ta réponse du contexte, ne réponds pas.\n",
    "\n",
    "Voici le contexte sur lequel tu dois baser ta réponse :\n",
    "Contexte: {context}\n",
    "\"\"\"\n",
    "\n",
    "question_instructions = \"\"\"\n",
    "Voici la question à laquelle tu dois répondre :\n",
    "Question: {question}\n",
    "\n",
    "Réponse:\n",
    "\"\"\"\n",
    "\n",
    "prompt = create_prompt_from_instructions(system_instructions, question_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "rag_chain = {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()} | prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_pib = rag_chain.invoke(\"Quelle est la définition du PIB ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_pib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(answer_pib.replace(\"   \", \"\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain_from_docs = (\n",
    "    RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"]))) | prompt | llm | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain_with_source = RunnableParallel({\"context\": retriever, \"question\": RunnablePassthrough()}).assign(\n",
    "    answer=rag_chain_from_docs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in rag_chain_with_source.stream(\"Quelle est la définition du PIB ?\"):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.invoke(\"Quelle est la définition du PIB ?\")[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain.batch([\"Quelle est la définition du PIB ?\" \"Où trouver les nouveaux chiffres du chpimage ?\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La chaine décomposée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modèle génératif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain_community.llms import VLLM\n",
    "\n",
    "MAX_NEW_TOKEN = 8192\n",
    "TEMPERATURE = 0.2\n",
    "REP_PENALTY = 1.1\n",
    "TOP_P = 0.8\n",
    "\n",
    "hf_token = os.environ[\"HF_TOKEN\"]\n",
    "s3_token = os.environ[\"AWS_SESSION_TOKEN\"]\n",
    "\n",
    "# cache_model_from_hf_hub(EMB_MODEL_NAME, hf_token=hf_token, s3_token=s3_token)\n",
    "# cache_model_from_hf_hub(LLM_MODEL, hf_token=hf_token, s3_token=s3_token)\n",
    "LLM_MODEL = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "llm = VLLM(\n",
    "    model=LLM_MODEL,\n",
    "    max_new_tokens=MAX_NEW_TOKEN,\n",
    "    top_p=TOP_P,\n",
    "    temperature=TEMPERATURE,\n",
    "    rep_penalty=REP_PENALTY,\n",
    "    tokenizer_mode=\"mistral\",\n",
    "    config_format=\"mistral\",\n",
    "    load_format=\"mistral\",\n",
    "    enforce_eager=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.generate([\"La recette de la tarte tatin\", \"tu fais qupoi ?\", \"où va le monde\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
