{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "import s3fs\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "\n",
    "def cache_model_from_hf_hub(\n",
    "    model_name: str,\n",
    "    s3_bucket: str = \"models-hf\",\n",
    "    s3_cache_dir: str = \"hf_hub/diffusion\",\n",
    "    s3_token: str = None,\n",
    "    hf_token: str = None,\n",
    "):\n",
    "    \"\"\"Use S3 as proxy cache from HF hub if a model is not already cached locally.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Name of the model on the HF hub.\n",
    "        s3_bucket (str): Name of the S3 bucket to use.\n",
    "        s3_cache_dir (str): Path of the cache directory on S3.\n",
    "    \"\"\"\n",
    "    assert \"MC_HOST_s3\" in os.environ, \"Please set the MC_HOST_s3 environment variable.\"\n",
    "\n",
    "    # Local cache config\n",
    "    LOCAL_HF_CACHE_DIR = os.path.join(os.path.expanduser(\"~\"), \".cache\", \"huggingface\", \"hub\")\n",
    "    model_name_hf_cache = \"models--\" + \"--\".join(model_name.split(\"/\"))\n",
    "    dir_model_local = os.path.join(LOCAL_HF_CACHE_DIR, model_name_hf_cache)\n",
    "\n",
    "    # Remote cache config\n",
    "    fs = s3fs.S3FileSystem(endpoint_url=\"https://minio.lab.sspcloud.fr\")\n",
    "    available_models_s3 = [os.path.basename(path) for path in fs.ls(os.path.join(s3_bucket, s3_cache_dir))]\n",
    "    dir_model_s3 = os.path.join(s3_bucket, s3_cache_dir, model_name_hf_cache)\n",
    "    print(dir_model_s3)\n",
    "\n",
    "    if model_name_hf_cache not in os.listdir(LOCAL_HF_CACHE_DIR):\n",
    "        # Try fetching from S3 if available\n",
    "        if model_name_hf_cache in available_models_s3:\n",
    "            print(f\"Fetching model {model_name} from S3.\")\n",
    "            cmd = [\"mc\", \"cp\", \"-r\", f\"s3/{dir_model_s3}\", f\"{LOCAL_HF_CACHE_DIR}/\"]\n",
    "            with open(\"/dev/null\", \"w\") as devnull:\n",
    "                subprocess.run(cmd, check=True, stdout=devnull, stderr=devnull)\n",
    "        # Else, fetch from HF Hub and push to S3\n",
    "        else:\n",
    "            print(f\"Model {model_name} not found on S3, fetching from HF hub.\")\n",
    "            AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\", token=hf_token)\n",
    "            print(f\"Putting model {model_name} on S3.\")\n",
    "            cmd = [\n",
    "                \"mc\",\n",
    "                \"cp\",\n",
    "                \"-r\",\n",
    "                f\"{dir_model_local}/\",\n",
    "                f\"s3/{dir_model_s3}\",\n",
    "            ]\n",
    "            with open(\"/dev/null\", \"w\") as devnull:\n",
    "                subprocess.run(cmd, check=True, stdout=devnull, stderr=devnull)\n",
    "    else:\n",
    "        print(f\"Model {model_name} found in local cache. \")\n",
    "        if model_name_hf_cache not in available_models_s3:\n",
    "            # Push from local HF cache to S3\n",
    "            print(f\"Putting model {model_name} on S3.\")\n",
    "            cmd = [\n",
    "                \"mc\",\n",
    "                \"cp\",\n",
    "                \"-r\",\n",
    "                f\"{dir_model_local}/\",\n",
    "                f\"s3/{dir_model_s3}\",\n",
    "            ]\n",
    "            with open(\"/dev/null\", \"w\") as devnull:\n",
    "                subprocess.run(cmd, check=True, stdout=devnull, stderr=devnull)\n",
    "\n",
    "\n",
    "def create_prompt_from_instructions(system_instructions: str, question_instructions: str) -> PromptTemplate:\n",
    "    template = f\"\"\"\n",
    "    {system_instructions}\n",
    "\n",
    "    {question_instructions}\n",
    "    \"\"\"\n",
    "\n",
    "    custom_rag_prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "    return custom_rag_prompt\n",
    "\n",
    "\n",
    "def format_docs(docs: list):\n",
    "    return \"\\n\\n\".join(\n",
    "        [\n",
    "            f\"\"\"\n",
    "            Doc {i + 1}:\\n:\n",
    "            Content:\\n{doc.page_content}\n",
    "            \"\"\"\n",
    "            for i, doc in enumerate(docs)\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_model_from_hf_hub(\"OrdalieTech/Solon-embeddings-large-0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://projet-llm-insee-open-data-227689-0.user.lab.sspcloud.fr/proxy/3000/\",\n",
    "    api_key=\"EMPTY\",\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"mistralai/Mistral-Small-24B-Instruct-2501\", messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n",
    ")\n",
    "\n",
    "completion\n",
    "# print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Modify OpenAI's API key and API base to use vLLM's API server.\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:8000/v1\"\n",
    "\n",
    "client = OpenAI(\n",
    "    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "\n",
    "models = client.models.list()\n",
    "model = models.data[0].id\n",
    "\n",
    "responses = client.embeddings.create(\n",
    "    input=[\"Hello my name is\", \"The best thing about vLLM is that it supports many different models\"],\n",
    "    model=model,\n",
    ")\n",
    "\n",
    "for data in responses.data:\n",
    "    print(data.embedding)  # List of float of len 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm import embedding\n",
    "\n",
    "response = embedding(\n",
    "    model=\"hosted_vllm/intfloat/e5-mistral-7b-instruct\",\n",
    "    # add `openai/` prefix to model so litellm knows to route to OpenAI\n",
    "    api_base=\"http://localhost:8000/v1\",\n",
    "    # set API Base of your Custom OpenAI Endpoint\n",
    "    input=[\"good morning from litellm\"],\n",
    ")\n",
    "\n",
    "# response.data[0][\"embedding\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm import completion\n",
    "\n",
    "response = completion(\n",
    "    model=\"hosted_vllm/mistralai/Mistral-Small-24B-Instruct-2501\",\n",
    "    # add `openai/` prefix to model so litellm knows to route to OpenAI\n",
    "    api_base=\"http://localhost:3000/v1\",\n",
    "    # set API Base of your Custom OpenAI Endpoint\n",
    "    messages=[{\"role\": \"user\", \"content\": \"what llm are you\"}],\n",
    ")\n",
    "\n",
    "response.choices[0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import litellm\n",
    "from litellm import CustomLLM, completion\n",
    "\n",
    "\n",
    "class MyCustomLLM(CustomLLM):\n",
    "    def completion(self, *args, **kwargs) -> litellm.ModelResponse:\n",
    "        return litellm.completion(\n",
    "            model=\"hosted_vllm/mistralai/Mistral-Small-24B-Instruct-2501\",\n",
    "            api_base=\"http://localhost:3000/v1\",\n",
    "            messages=[{\"role\": \"user\", \"content\": \"Hello world\"}],\n",
    "        )  # type: ignore\n",
    "\n",
    "    async def acompletion(self, *args, **kwargs) -> litellm.ModelResponse:\n",
    "        return litellm.completion(\n",
    "            model=\"hosted_vllm/mistralai/Mistral-Small-24B-Instruct-2501\",\n",
    "            api_base=\"http://localhost:3000/v1\",\n",
    "            messages=[{\"role\": \"user\", \"content\": \"Hello world\"}],\n",
    "        )  # type: ignore\n",
    "\n",
    "    def embedding(self, *args, **kwargs) -> litellm.ModelResponse:\n",
    "        return litellm.embedding(\n",
    "            model=\"hosted_vllm/intfloat/e5-mistral-7b-instruct\",\n",
    "            # add `openai/` prefix to model so litellm knows to route to OpenAI\n",
    "            api_base=\"http://localhost:8000/v1\",\n",
    "            # set API Base of your Custom OpenAI Endpoint\n",
    "            input=[\"good morning from litellm\"],\n",
    "        )  # type: ignore\n",
    "\n",
    "    async def aembedding(self, *args, **kwargs) -> litellm.ModelResponse:\n",
    "        return litellm.aembedding(\n",
    "            model=\"hosted_vllm/intfloat/e5-mistral-7b-instruct\",\n",
    "            # add `openai/` prefix to model so litellm knows to route to OpenAI\n",
    "            api_base=\"http://localhost:8000/v1\",\n",
    "            # set API Base of your Custom OpenAI Endpoint\n",
    "            input=[\"good morning from litellm\"],\n",
    "        )  # type: ignore\n",
    "\n",
    "\n",
    "my_custom_llm = MyCustomLLM()\n",
    "\n",
    "litellm.custom_provider_map = [  # ðŸ‘ˆ KEY STEP - REGISTER HANDLER\n",
    "    {\"provider\": \"my-custom-llm\", \"custom_handler\": my_custom_llm}\n",
    "]\n",
    "\n",
    "resp = completion(\n",
    "    model=\"my-custom-llm/my-fake-model\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"fais le chien\"}],\n",
    ")\n",
    "\n",
    "emb = embedding(\n",
    "    model=\"my-custom-llm/my-fake-model\",\n",
    "    input=[\"good morning from litellm\"],\n",
    ")\n",
    "\n",
    "# resp.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "s3_path = \"s3://projet-llm-insee-open-data/data/raw_data/applishare_solr_joined.parquet\"\n",
    "\n",
    "filesystem = s3fs.S3FileSystem(endpoint_url=\"https://minio.lab.sspcloud.fr\")\n",
    "df = pd.read_parquet(s3_path, engine=\"pyarrow\", filesystem=filesystem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import VLLMOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"EMPTY\"\n",
    "\n",
    "llm_completion = VLLMOpenAI(\n",
    "    openai_api_key=\"EMPTY\",\n",
    "    openai_api_base=\"http://localhost:3000/v1\",\n",
    "    model_name=\"mistralai/Mistral-Small-24B-Instruct-2501\",\n",
    "    model_kwargs={\"stop\": [\".\"]},\n",
    ")\n",
    "\n",
    "emb_model = OpenAIEmbeddings(\n",
    "    model=\"intfloat/e5-mistral-7b-instruct\", openai_api_base=\"http://localhost:8000/v1\", openai_api_key=\"EMPTY\"\n",
    ")\n",
    "\n",
    "print(emb_model.embed_query(\"A sentence to encode.\")[:5])\n",
    "print(llm_completion.invoke(\"Rome is\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "\n",
    "generator_llm = LangchainLLMWrapper(llm_completion)\n",
    "generator_embeddings = LangchainEmbeddingsWrapper(emb_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "\n",
    "loader = DataFrameLoader(df.head(2), page_content_column=\"xml_content\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset import TestsetGenerator\n",
    "\n",
    "generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings)\n",
    "dataset = generator.generate_with_langchain_docs(docs[:3], testset_size=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
