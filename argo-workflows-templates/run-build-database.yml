apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: run-build-database-
spec:
  serviceAccountName: workflow
  volumes:
    - name: dshm
      emptyDir:
        medium: Memory
        sizeLimit: 16Gi
  entrypoint: main
  arguments:
    parameters:
      - name: run-build-database-conf-list
        value: '[
            { "EXPERIMENT": "BUILD_CHROMA_TEST",
              "COLLECTION_NAME": "insee_data",
              "EMBEDDING_MODEL": "Alibaba-NLP/gte-Qwen2-1.5B-instruct",
              "CHUNK_SIZE": 8192,
              "CHUNK_OVERLAP": 0.1,
              "BATCH_EMBEDDING": 50
            }
            ]'

  templates:
    # Entrypoint DAG template
    - name: main
      dag:
        tasks:
          - name: run-build-database-with-params
            template: run-build-database-wt
            arguments:
              parameters:
                - name: EXPERIMENT
                  value: "{{item.EXPERIMENT}}"
                - name: COLLECTION_NAME
                  value: "{{item.COLLECTION_NAME}}"
                - name: EMBEDDING_MODEL
                  value: "{{item.EMBEDDING_MODEL}}"
                - name: CHUNK_SIZE
                  value: "{{item.CHUNK_SIZE}}"
                - name: CHUNK_OVERLAP
                  value: "{{item.CHUNK_OVERLAP}}"
                - name: BATCH_EMBEDDING
                  value: "{{item.BATCH_EMBEDDING}}"
            # Pass the inputs to the task using "withParam"
            withParam: "{{workflow.parameters.run-build-database-conf-list}}"

    # Now task container templates are defined
    - name: run-build-database-wt
      inputs:
        parameters:
          - name: EXPERIMENT
          - name: BATCH_EMBEDDING
          - name: COLLECTION_NAME
          - name: EMBEDDING_MODEL
          - name: CHUNK_SIZE
          - name: CHUNK_OVERLAP

      nodeSelector:
        nvidia.com/gpu.product: "NVIDIA-H100-NVL"
      container:
        image: inseefrlab/onyxia-vscode-pytorch:py3.12.6-gpu
        imagePullPolicy: Always
        resources:
          requests:
            memory: 16Gi
            # cpu: 40
          limits:
            memory: 64Gi
            # cpu: 50
            nvidia.com/gpu: 1
        command: ["/bin/bash", -c]
        args:
          - |
            git clone https://github.com/InseeFrLab/llm-open-data-insee.git &&
            export MC_HOST_s3=https://${AWS_ACCESS_KEY_ID}:${AWS_SECRET_ACCESS_KEY}:@${AWS_S3_ENDPOINT} &&
            cd llm-open-data-insee/ &&
            git checkout qwen-embedding &&
            pip install -r requirements.txt &&
            python run_build_database.py --experiment_name {{inputs.parameters.EXPERIMENT}} \
              --collection_name {{inputs.parameters.COLLECTION_NAME}} \
              --emb_model {{inputs.parameters.EMBEDDING_MODEL}} \
              --chunk_size {{inputs.parameters.CHUNK_SIZE}} \
              --chunk_overlap {{inputs.parameters.CHUNK_OVERLAP}} \
              --batch_size_embedding {{inputs.parameters.BATCH_EMBEDDING}} \
              --max_pages 100

        volumeMounts:
          - mountPath: /dev/shm
            name: dshm
        env:
          # env var for s3 connexion
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                name: s3-creds
                key: accessKey
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                name: s3-creds
                key: secretKey
          - name: AWS_DEFAULT_REGION
            value: us-east-1
          - name: AWS_S3_ENDPOINT
            value: minio.lab.sspcloud.fr
          - name: MLFLOW_S3_ENDPOINT_URL
            value: https://minio.lab.sspcloud.fr
          - name: MLFLOW_TRACKING_URI
            value: https://projet-llm-insee-open-data-mlflow.user.lab.sspcloud.fr/
