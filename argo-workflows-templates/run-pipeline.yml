apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: run-pipeline-
spec:
  serviceAccountName: workflow
  volumes:
    - name: dshm
      emptyDir:
        medium: Memory
        sizeLimit: 16Gi
  entrypoint: main
  arguments:
    parameters:
      - name: run-pipeline-conf-list
        value: '[
            { "EXPERIMENT": "BUILD_PIPELINE",
              "DATA_PATH": "data/raw_data/applishare_solr_joined.parquet",
              "COLLECTION_NAME": "insee_data",
              "EMBEDDING_MODEL": "OrdalieTech/Solon-embeddings-large-0.1",
              "LLM_MODEL": "mistralai/Mistral-7B-Instruct-v0.2"
            }
            ]'

  templates:
    # Entrypoint DAG template
    - name: main
      dag:
        tasks:
          - name: run-pipeline-with-params
            template: run-pipeline-wt
            arguments:
              parameters:
                - name: EXPERIMENT
                  value: "{{item.EXPERIMENT}}"
                - name: DATA_PATH
                  value: "{{item.DATA_PATH}}"
                - name: COLLECTION_NAME
                  value: "{{item.COLLECTION_NAME}}"
                - name: EMBEDDING_MODEL
                  value: "{{item.EMBEDDING_MODEL}}"
                - name: LLM_MODEL
                  value: "{{item.LLM_MODEL}}"
            # Pass the inputs to the task using "withParam"
            withParam: "{{workflow.parameters.run-pipeline-conf-list}}"

    # Now task container templates are defined
    - name: run-pipeline-wt
      inputs:
        parameters:
          - name: EXPERIMENT
          - name: DATA_PATH
          - name: COLLECTION_NAME
          - name: EMBEDDING_MODEL
          - name: LLM_MODEL

      nodeSelector:
        nvidia.com/gpu.product: "NVIDIA-H100-PCIe"
      container:
        image: inseefrlab/onyxia-vscode-pytorch:py3.12.3-gpu
        imagePullPolicy: Always
        resources:
          requests:
            memory: 16Gi
            # cpu: 40
          limits:
            memory: 64Gi
            # cpu: 50
            nvidia.com/gpu: 1
        command: ["/bin/bash", -c]
        args:
          - |
            git clone https://github.com/InseeFrLab/llm-open-data-insee.git &&
            cd llm-open-data-insee/ &&
            git checkout db-build-pipeline &&
            pip install -r requirements-dev.txt &&
            python run_pipeline.py --experiment_name {{inputs.parameters.EXPERIMENT}} \
              --data_raw_s3_path {{inputs.parameters.DATA_PATH}} \
              --collection_name {{inputs.parameters.COLLECTION_NAME}} \
              --embedding_model {{inputs.parameters.EMBEDDING_MODEL}} \
              --llm_model {{inputs.parameters.LLM_MODEL}} \
              --max_pages 25

        volumeMounts:
          - mountPath: /dev/shm
            name: dshm
        env:
          # env var for s3 connexion
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                name: s3-creds
                key: accessKey
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                name: s3-creds
                key: secretKey
          - name: HF_TOKEN
            valueFrom:
              secretKeyRef:
                name: hf-token
                key: token
          - name: AWS_DEFAULT_REGION
            value: us-east-1
          - name: AWS_S3_ENDPOINT
            value: minio.lab.sspcloud.fr
          - name: MLFLOW_S3_ENDPOINT_URL
            value: https://minio.lab.sspcloud.fr
          - name: MLFLOW_TRACKING_URI
            value: https://projet-llm-insee-open-data-mlflow.user.lab.sspcloud.fr/
          - name: S3_BUCKET
            value: projet-llm-insee-open-data
