import json
from typing import List
from langchain.docstore.document import Document
from datetime import datetime
import logging
import os

from config import MODEL_NAME, EMB_MODEL_NAME, RELATIVE_DATA_DIR

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

def save_results_to_json(
    user_query: str = None,
    retrieved_documents: List[Document] = None,
    prompt_template: str = None,
    generated_answer: str = None,
    embedding_model_name: str = None,
    LLM_name: str = None,
    reranker: str = None,
    folder_path: str = RELATIVE_DATA_DIR,
):
    """
    Save details of a RAG conversation to a JSON file.

    Args:
    user_query (str): The user's input query.
    retrieved_documents (list[Document]): List of documents retrieved based on the user query.
    prompt_template (str): The template used to generate the prompt for the language model.
    generated_answer (str): The answer generated by the language model.
    embedding_model_name (str): The name of the embedding model used.
    LLM_name (str): The name of the language model used.
    reranker (str): The name of the reranker used.
    folder_path (str): The folder path where the log will be saved.

    Returns:
    None
    """
    retrieved_documents_text = [d.page_content for d in retrieved_documents]
    retrieved_documents_metadata = [d.metadata for d in retrieved_documents]

    # Define file name
    timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
    file_path = os.path.join(folder_path, f"conv_{timestamp}.json")
    
    # Ensure the path for the log file exists
    directory = os.path.dirname(file_path)
    if not os.path.exists(directory):
        os.makedirs(directory)
        logging.info(f"Directory {directory} has been created")

    # Prepare the content to be logged as a dictionary
    log_entry = {
        "user_query": user_query,
        "retrieved_docs_text": retrieved_documents_text,
        "prompt": prompt_template,
        "generated_answer": generated_answer,
        "embedding_model": embedding_model_name,
        "llm": LLM_name,
        "reranker": reranker,
        "retrieved_doc_metadata": retrieved_documents_metadata,
        "timestamp": timestamp,
    }

    # Open the file in write mode and write the dictionary as a JSON object
    with open(file_path, "w", encoding="utf-8") as file:
        json.dump(log_entry, file, ensure_ascii=False, indent=4)

def log_chain_results(result, prompt, reranker):
    """
    Logs interaction details into a JSON file and returns the original result.

    Args:
    result (dict): The result from the RAG pipeline containing question, answer, and context.
    prompt (object): The prompt template used.
    reranker (str): The reranker used in the pipeline.

    Returns:
    dict: The original result.
    """
    log_folder_path = os.path.join(RELATIVE_DATA_DIR, "logs")

    # Extracting necessary details from the result
    user_query = result["question"]
    generated_answer = result["answer"]
    retrieved_documents = result["context"]
    prompt_template = prompt.template
    embedding_model_name = EMB_MODEL_NAME
    LLM_name = MODEL_NAME

    # Call to save the logs
    logging.info(f"saving outputs in {log_folder_path} folder")
    save_results_to_json(
        user_query=user_query,
        retrieved_documents=retrieved_documents,
        prompt_template=prompt_template,
        generated_answer=generated_answer,
        embedding_model_name=embedding_model_name,
        LLM_name=LLM_name,
        reranker=reranker,
        folder_path=log_folder_path,
    )

    return result
