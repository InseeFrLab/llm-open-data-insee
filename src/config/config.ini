[DEFAULT]

# S3 CONFIG -------------------------------------------------------------------

experiment_name = default
aws_s3_endpoint = minio.lab.sspcloud.fr
s3_endpoint_url = https://${aws_s3_endpoint}
s3_bucket = projet-llm-insee-open-data

# LOCAL FILES -----------------------------------------------------------------

relative_data_dir = data
relative_log_dir = logs
log_file_path = ${relative_log_dir}/conversation_logs.json

# ML FLOW LOGGING -------------------------------------------------------------

mlflow_tracking_uri = https://projet-llm-insee-open-data-mlflow.user.lab.sspcloud.fr

# MLflow run ID to load parameters from
mlflow_run_id =
# Should artifacts (Chroma DB) also be loaded from MLflow
mlflow_load_artifacts = true

# RAW DATA PROCESSING  --------------------------------------------------------

data_raw_s3_path = data/raw_data/applishare_solr_joined.parquet
raw_dataset_uri = s3://${s3_bucket}/${data_raw_s3_path}
markdown_split = true
use_tokenizer_to_chunk = true
separators = ['\n\n', '\n', '.', ' ', '']

rawdata_web4g = data/raw_data/applishare_solr_joined.parquet
rawdata_rmes = data/raw_data/applishare_solr_joined.parquet

rawdata_web4g_uri = s3://${s3_bucket}/${rawdata_web4g}
rawdata_rmes_uri = s3://${s3_bucket}/${rawdata_rmes}


# CORPUS BUILDING  ------------------------------------------------------------

# PARSING, PROCESSING and CHUNKING  -------------------------------------------

# Limits the number of rows
max_pages =

chunk_size = 1000
chunk_overlap = 100

# VECTOR DATABASE -------------------------------------------------------------

chroma_db_local_dir = ${relative_data_dir}/chroma_db
chroma_db_s3_dir = s3/${s3_bucket}/data/chroma_database/

collection_name = insee_data
force_rebuild = true

# MODELS USED   ---------------------------------------------------------------

# Embedding model for IR
emb_device = cuda
emb_model = OrdalieTech/Solon-embeddings-large-0.1

# LLM Model
llm_model = mistralai/Mistral-7B-Instruct-v0.2
quantization = false

# LLM -------------------------------------------------------------------------

s3_model_cache_dir = models/hf_hub
max_new_tokens = 2000

# EVALUATION ------------------------------------------------------------------

faq_s3_path = data/FAQ_site/faq.parquet
faq_s3_uri = s3://${s3_bucket}/${faq_s3_path}

# INSTRUCTION PROMPT ----------------------------------------------------------

BASIC_RAG_PROMPT_TEMPLATE = <s>[INST]
    Instruction: Réponds à la question en te basant sur le contexte donné:

    {context}

    Question:
    {question}
    [/INST]

RAG_PROMPT_TEMPLATE = <s>[INST]
    Tu es un assistant spécialisé dans la statistique publique répondant aux questions d'agent de l'INSEE.
    Réponds en Français seulement.
    Utilise les informations obtenues dans le contexte, réponds de manière argumentée à la question posée.
    La réponse doit être développée et citer ses sources.

    Si tu ne peux pas induire ta réponse du contexte, ne réponds pas.
    Voici le contexte sur lequel tu dois baser ta réponse :
    Contexte: {context}
            ---
    Voici la question à laquelle tu dois répondre :
    Question: {question}
    [/INST]

# CHATBOT CONFIGURATION -------------------------------------------------------

CHATBOT_INSTRUCTION = En utilisant UNIQUEMENT les informations présentes dans le contexte, réponds de manière argumentée à la question posée.
    La réponse doit être développée et citer ses sources.

    Si tu ne peux pas induire ta réponse du contexte, ne réponds pas.

USER_INSTRUCTION = Voici le contexte sur lequel tu dois baser ta réponse :
    Contexte:
    {context}
    ---
    Voici la question à laquelle tu dois répondre :
    Question: {question}


[data]

RAW_DATA = "projet-llm-insee-open-data/data/insee_contact/data_2019.csv"
LS_DATA_PATH = "projet-llm-insee-open-data/data/insee_contact/tasks/"
LS_ANNOTATIONS_PATH = "projet-llm-insee-open-data/data/insee_contact/annotations/"


[chainlit.app]
# Chainlit specific parameters in this section

uvicorn_timeout_keep_alive = 0
cli_message_separator_length = 80
quantization = True
model_temperature = 1.0
return_full_text = true
do_sample = true
database_run_id =
max_new_tokens = 8192
temperature = 0.2
rep_penalty  = 1.1
top_p = 0.8
reranking_method =
