from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.runnables import RunnablePassthrough, RunnableParallel, RunnableLambda
from langchain_core.output_parsers import StrOutputParser

from config import EMB_MODEL_NAME, EMB_DEVICE, LOG_FILE_PATH, MODEL_NAME

import json
from typing import List
from langchain.docstore.document import Document
import datetime
import os


def format_docs(docs) -> str:
    """
    Format the retrieved document before giving their content to complete the prompt
    """
    return "\n\n".join(doc.page_content for doc in docs)


def build_chain(retriever, prompt, llm):
    """
    Build a LLM chain based on Langchain package and INSEE data
    """
    # Create a Langchain LLM Chain
    rag_chain_from_docs = (
        RunnablePassthrough.assign(context=(lambda x: format_docs(x["context"])))
        | prompt
        | llm
        | StrOutputParser()
    )

    # Create a Langchain LLM Chain which return sources and store them into a log file
    rag_chain_with_source = RunnableParallel(
        {"context": retriever, "question": RunnablePassthrough()}
    ).assign(answer=rag_chain_from_docs) | RunnableLambda(log_interaction)

    return rag_chain_with_source


def load_retriever(persist_directory):
    # Load Embedding model
    embeddings = HuggingFaceEmbeddings(
        model_name=EMB_MODEL_NAME, model_kwargs={"device": EMB_DEVICE}
    )
    # Load vector database
    vectorstore = Chroma(
        collection_name="insee_data",
        embedding_function=embeddings,
        persist_directory=persist_directory,
    )
    # Set up a retriever
    retriever = vectorstore.as_retriever(
        search_type="mmr", search_kwargs={"score_threshold": 0.5, "k": 10}
    )
    return retriever


def extract_context_as_dict(response):
    dict_context = {}
    for i, doc in enumerate(response['context']):
        dict_context[i] = doc.metadata
        dict_context[i]['content'] = doc.page_content
    return dict_context


def save_logs(
    user_query: str = None,
    retrieved_documents: List[Document] = None,
    prompt_template: str = None,
    generated_answer: str = None,
    embedding_model_name: str = None,
    LLM_name: str = None,
    filename="./logs/conversation_logs.json",
):
    """
    Save details of a RAG conversation to a json file.

    Args:
    user_query (str): The user's input query.
    retrieved_documents (list[Document]): List of documents retrieved based on the user query.
    prompt_template (str): The template used to generate the prompt for the language model.
    generated_answer (str): The answer generated by the language model.
    RAG_pipeline : (HF pipeline)
    filename (str): The filename where the log will be saved.

    Returns:
    None
    """
    # Ensure the path for the log file exists
    if not os.path.exists(os.path.dirname(filename)):
        os.makedirs(os.path.dirname(filename))

    retrieved_documents_text = [d.page_content for d in retrieved_documents]
    retrieved_documents_metadata = [d.metadata for d in retrieved_documents]

    # Prepare the content to be logged as a dictionary
    log_entry = {
        "user_query": user_query,
        "retrieved_docs_text": retrieved_documents_text,
        "prompt": prompt_template,
        "generated_answer": generated_answer,
        "embedding_model": embedding_model_name,
        "llm": LLM_name,
        "retrieved_doc_metadata": retrieved_documents_metadata,
        "timestamp": datetime.datetime.now().isoformat(),
    }

    # Open the file in append mode and write the dictionary as a JSON object
    with open(filename, "a", encoding="utf-8") as file:
        json.dump(log_entry, file, ensure_ascii=False, indent=4)
        file.write("\n")


def log_interaction(result):
    """
    Logs interaction details into a JSON file and returns the original result.
    """
    # Extracting necessary details from the result
    user_query = result["question"]
    generated_answer = result["answer"]
    retrieved_documents = result["context"]
    # prompt_template = prompt.template  # Ensure 'prompt' is accessible here

    # Call to save the logs
    print(f"saving outputs in {LOG_FILE_PATH}")
    save_logs(
        user_query,
        retrieved_documents,
        None,
        generated_answer,
        EMB_MODEL_NAME,
        MODEL_NAME,
        filename=LOG_FILE_PATH,
    )

    return result
