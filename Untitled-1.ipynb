{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import s3fs\n",
    "\n",
    "import chainlit as cl\n",
    "import chainlit.data as cl_data\n",
    "from langchain.schema.runnable.config import RunnableConfig\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "from src.chain_building.build_chain import build_chain\n",
    "from src.chain_building.build_chain_validator import build_chain_validator\n",
    "from src.config import CHATBOT_TEMPLATE, EMB_MODEL_NAME\n",
    "from src.db_building import (\n",
    "    load_retriever,\n",
    "    load_vector_database\n",
    ")\n",
    "from src.model_building import build_llm_model\n",
    "from src.results_logging.log_conversations import log_feedback_to_s3, log_qa_to_s3\n",
    "from src.utils.formatting_utilities import add_sources_to_messages, str_to_bool\n",
    "\n",
    "# Logging configuration\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %I:%M:%S %p\",\n",
    "    level=logging.DEBUG,\n",
    ")\n",
    "\n",
    "# Remote file configuration\n",
    "os.environ['MLFLOW_TRACKING_URI'] = \"https://projet-llm-insee-open-data-mlflow.user.lab.sspcloud.fr/\"\n",
    "fs = s3fs.S3FileSystem(client_kwargs={\"endpoint_url\": f\"\"\"https://{os.environ[\"AWS_S3_ENDPOINT\"]}\"\"\"})\n",
    "\n",
    "# PARAMETERS --------------------------------------\n",
    "\n",
    "os.environ['UVICORN_TIMEOUT_KEEP_ALIVE'] = \"0\"\n",
    "\n",
    "model = os.getenv(\"LLM_MODEL_NAME\")\n",
    "CHROMA_DB_LOCAL_DIRECTORY = \"./data/chroma_db\"\n",
    "CLI_MESSAGE_SEPARATOR = f\"{80*'-'} \\n\"\n",
    "quantization = True\n",
    "DEFAULT_MAX_NEW_TOKENS = 10\n",
    "DEFAULT_MODEL_TEMPERATURE = 1\n",
    "embedding = os.getenv(\"EMB_MODEL_NAME\", EMB_MODEL_NAME)\n",
    "\n",
    "LLM_MODEL = os.getenv(\"LLM_MODEL_NAME\", \"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "QUANTIZATION = os.getenv(\"QUANTIZATION\", True)\n",
    "MAX_NEW_TOKENS = int(os.getenv(\"MAX_NEW_TOKENS\", DEFAULT_MAX_NEW_TOKENS))\n",
    "MODEL_TEMPERATURE = int(os.getenv(\"MODEL_TEMPERATURE\", DEFAULT_MODEL_TEMPERATURE))\n",
    "RETURN_FULL_TEXT = os.getenv(\"RETURN_FULL_TEXT\", True)\n",
    "DO_SAMPLE = os.getenv(\"DO_SAMPLE\", True)\n",
    "DATABASE_RUN_ID = \"32d4150a14fa40d49b9512e1f3ff9e8c\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm, tokenizer = build_llm_model(\n",
    "            model_name=LLM_MODEL,\n",
    "            quantization_config=QUANTIZATION,\n",
    "            config=True,\n",
    "            token=os.getenv(\"HF_TOKEN\"),\n",
    "            streaming=False,\n",
    "            generation_args={\n",
    "                \"max_new_tokens\": 100000,\n",
    "                \"return_full_text\": RETURN_FULL_TEXT,\n",
    "                \"do_sample\": DO_SAMPLE,\n",
    "                \"temperature\": MODEL_TEMPERATURE\n",
    "            },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    llm.invoke(\"quels sont les chiffres du chômage\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = load_vector_database(\n",
    "            filesystem=fs,\n",
    "            database_run_id=DATABASE_RUN_ID\n",
    "            # hard coded pour le moment\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever, vectorstore = load_retriever(\n",
    "                emb_model_name=embedding,\n",
    "                persist_directory=CHROMA_DB_LOCAL_DIRECTORY,\n",
    "                vectorstore=db,\n",
    "                retriever_params={\n",
    "                    \"search_type\": \"similarity\",\n",
    "                    \"search_kwargs\": {\"k\": 30}\n",
    "                },\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.invoke(\"je veux les chiffres du chomage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_docs = db.get()[\"documents\"]\n",
    "ndocs = f\"Ma base de connaissance du site Insee comporte {len(db_docs)} documents\"\n",
    "ndocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_PROMPT_TEMPLATE = tokenizer.apply_chat_template(\n",
    "    CHATBOT_TEMPLATE, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"], template=RAG_PROMPT_TEMPLATE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator = build_chain_validator(\n",
    "        evaluator_llm=llm, tokenizer=tokenizer\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.chain_building.build_chain import build_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = build_chain(\n",
    "        retriever=retriever,\n",
    "        prompt=prompt,\n",
    "        llm=llm,\n",
    "        reranker=\"BM25\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il y a un problème en ce moment sur la chaine, au niveau de la génération après retrieval. Je vais tester sur un exemple plus minime, à partir de \n",
    "\n",
    "* https://python.langchain.com/docs/integrations/llms/huggingface_pipelines/#gpu-inference\n",
    "* https://python.langchain.com/v0.1/docs/expression_language/interface/#async-stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "retrieval_chain = (\n",
    "    {\n",
    "        \"context\": retriever.with_config(run_name=\"Docs\"),\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async for event in retrieval_chain.astream_events(\n",
    "    \"je veux les chiffres du chômage\", version=\"v1\", include_names=[\"Docs\", \"my_llm\"]\n",
    "):\n",
    "    kind = event[\"event\"]\n",
    "    if kind == \"on_chat_model_stream\":\n",
    "        print(event[\"data\"][\"chunk\"].content, end=\"|\")\n",
    "    elif kind in {\"on_chat_model_start\"}:\n",
    "        print()\n",
    "        print(\"Streaming LLM:\")\n",
    "    elif kind in {\"on_chat_model_end\"}:\n",
    "        print()\n",
    "        print(\"Done streaming LLM.\")\n",
    "    elif kind == \"on_retriever_end\":\n",
    "        print(\"--\")\n",
    "        print(\"Retrieved the following documents:\")\n",
    "        print(event[\"data\"][\"output\"][\"documents\"])\n",
    "    elif kind == \"on_tool_end\":\n",
    "        print(f\"Ended tool: {event['name']}\")\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        async for chunk in chain.astream(\n",
    "            \"je veux les chiffres du chomage\",\n",
    "            config=RunnableConfig(\n",
    "                callbacks=[cl.AsyncLangchainCallbackHandler(stream_final_answer=True)]\n",
    "            ),\n",
    "        ):\n",
    "            if \"answer\" in chunk:\n",
    "                await answer_msg.stream_token(chunk[\"answer\"])\n",
    "                generated_answer = chunk[\"answer\"]\n",
    "\n",
    "            if \"context\" in chunk:\n",
    "                docs = chunk[\"context\"]\n",
    "                for doc in docs:\n",
    "                    sources.append(doc.metadata.get(\"url\"))\n",
    "                    titles.append(doc.metadata.get(\"Header 1\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\"Je veux les chiffres du chômage\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
